
本章将使用MNIST数据集，这是一组由美国高中生和人口调查局员工手写的70 000个数字的图片。每张图片都用其代表的数字标记。这个数据集被广为使用，因此也被称作是机器学习领域的“Hello
World”：但凡有人想到了一个新的分类算法，都会想看看在MNIST上的执行结果。因此只要是学习机器学习的人，早晚都要面对MNIST。
## MNIST


**获取数据集**
```python
from sklearn.datasets import fetch_openml
mnist = fetch_openml('mnist_784',version = 1)
mnist.keys() #字典
```
让我们分解这个调用：

- `fetch_openml`：这是`sklearn.datasets`模块中的一个函数，用于从OpenML平台获取数据集。它允许用户指定数据集的名称、版本等参数，以检索所需的数据集。
- `'mnist_784'`：这是传递给`fetch_openml`函数的第一个参数，指定了要下载的数据集的名称。在这个例子中，它指的是MNIST数据集的一个版本，其中数字图像被展平为784维的向量（因为MNIST图像是28x28像素的，所以28 \* 28=784）。
- `version=1`：这是传递给`fetch_openml`函数的一个关键字参数，用于指定要下载的数据集的版本。在OpenML上，数据集可能有多个版本，每个版本可能包含不同的数据或具有不同的预处理方式。在这个例子中，`version=1`指定了MNIST数据集的一个特定版本。

Scikit-Learn加载的数据集通常具有类似的字典结构，包括：

- DESCR键，描述数据集。

- data键，包含一个数组，每个实例为一行，每个特征为一列。

- target键，包含一个带有标记的数组。


结果如下：
```python
dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])
```
接下来引入变量：
```python
X =mnist['data']
y = mnist['target']

```
y表示X数据的数字标签。
```python
import matplotlib as mlp
import matplotlib.pyplot as plt
import numpy as np
some_digit = np.array(X.iloc[3003,]) #逗号可要可不要 只需表达取一整行标签数据
some_digit_image = some_digit.reshape(28,28)
plt.imshow(some_digit_image, cmap = mlp.cm.binary, interpolation='bilinear') 
plt.axis('off') #关闭坐标轴显示
plt.show() 
```
结果如下：
![fix](/images/QQ_1724669224009.png)
让我们来看看y 的值是多少：
```python
y[30003]
# 9
```

显示单个图片的函数：
```python
def plot_digit(data):
	image = data.reshape(28,28)
	plt.imshow(image,cmap = mpl.cm.binary,interpolation = "nearest")
	plt.axis("off")
```

调用显示单个图片的函数

```python
plt.figure(figsize = (9,9))
examplt_image = np.array(X.iloc[100])
plot_digit(example_image)
plt.show()
print(y[100])
```
![fix](/images/QQ_1724729742350.png)
显示更多图片的函数
```python
def plot_images(instances , images_per_row = 10 , **options):
	images_per_row = min(len(instances) , images_per_row) 
	#每一行多少张。当总数不足10时。
	images =[ instance.reshape(size,size) for instance in instances]
	n_rows = (len(instances)-1) // images_per_row +1
	n_empty = n_rows * images_per_row - len(instances) 
    images.append(np.zeros((size, size * n_empty))) #最后一行不够的补上空白格
	row_images = []
	for row in range(n_rows):
		rimages  = images[row * images_per_row : (row +1 ) * images_per_row]
		row_images.append(np.concatenate(rimages,axis = 1))
	image = np.concatenate(row_images,axis = 0)
	plt.imshow(images,cmap = mpl.cm.binary ,**options)
	plt.axis("off")
```
需要明白的是 此处的`rimages` 是一个数组，即`row_images.append` 添加的是一个数组
这样才能用 `np.concatenate(row_images,axis = 0)`
调用函数
```python
plt.figure(figsize= (9,9))
example_images = np.array(X[:100])
plot_digits(example_images , images_per_row = 10)
plt.show()
```

结果如下：
![fix](/images/QQ_1724733290811.png)


## 训练二元分类器

现在先简化问题，只尝试识别一个数字，比如数字5。那么这个“数字5检测器”就是一个二元分类器的示例，它只能区分两个类别：5和非5。为此分类任务创建目标向量：
先要对y数据类型处理，便于后续的分类处理：
```python
y = y.astype(np.uint8)
```

将y中的值转变为[0,255]区间的整数。

```python
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]

y_train_5 = (y_train == 5) # True for all 5s, False for all other digits
y_test_5 = (y_test == 5)
```

接着挑选一个分类器并开始训练。一个好的初始选择是随机梯度下降(SGD)分类器，使用Scikit_Learn 的SGDClassifier类即可。这个分类器的优势是能够有效处理非常大型的数据集。这部分是因为SGD独立处理训练实例，一次一个（这也使得SGD非常适合在线学习），稍后我们将会看到。此时先创建一个SGDClassifier并在整个训练集上进行训练。

```python
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)
#表示迭代1000次 误差为1e-3

sgd_clf.fit(X_train, y_train_5)


```
试试结果如何：

```python
for i in range(20):
    print(sgd_clf.predict(pd.DataFrame([X.loc[i,:]])))
```
结果如下：

```python
[ True]
[False]
[False]
[False]
[False]
[False]
[False]
[False]
[False]
[False]
[False]
[ True]
[False]
[False]
[False]
[False]
[False]
[False]
[False]
[False]
```
结果表示第一个和第十二个数是5，果真如此吗？
```python
print(y[0],y[11])
# 5 5
```

结果非常正确！
那么，下面评估一下这个模型的性能。


## 性能测量

评估分类器比评估回归器要困难得多，因此本章将用很多篇幅来讨论这个主题，同时会涉及许多性能考核的方法。

### 使用交叉验证测量准确率
正如第2章所述，交叉验证是一个评估模型的好办法。

相比于`Scikit-Learn`提供`cross_val_score（）`这一类交叉验证的函数，有时你可能希望自己能控制得多一些。在这种情况下，你可以自行实现交叉验证，操作也简单明了。下面这段代码与前面的`cross_val_score（）`大致相同，并打印出相同的结果：
```python
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone

skfolds = stratifiedKFold(n_splits = 3,shuffle = True ,random_state =42)

for train_index ,test_index in skfolds.split(X_train , y_train_5):
	clone_clf = clone(sgd_clf)
	x_train_folds = X_train.iloc[train_index]
	y_train_folds = y_train_5[train_index]

	x_test_fold = x_test.iloc[test_index]
	y_test_fold = y_test[test_index]

	clone_clf.fit(x_train_folds,y_train_folds)
	y_pred = clone_clf.predict(x_test_fold)
	n_correct = sum(y_pred == y_test_fold)
	print(n_correct / len(y_pred))
```

`stratifiedKFold`可以看作一个将原数集分为测试集，和训练集的函数。
`n_splits`：分成多少折。

结果如下：
```python
0.9669
0.91625
0.96785
```

每个折叠由`StratifiedKFold` 执行分层抽样产生。其所包含的各个类的比例符合整体比例。每个迭代会创建一个分类器的副本。用训练集对这个副本进行训练，然后用测试集进行预测。最后计算正确预测的次数，输出正确预测与预测总数的比例。

现在用`cross_val_score()`函数来评估`SGDClassfier` 模型。采用K-折交叉验证法(三个折叠)。
K-折交叉验证的意思是将训练集分解成K个折叠，然后每次留其中1个折叠进行预测，剩余的折叠用来训练。

```python
from sklearn.model_selection import cross_val_score
cross_val_score(sgd_clf,X_train,y_train_5 , cv = 3,scoring ="accuracy")
```
### 混淆矩阵
评估分类器性能的更好方法是混淆矩阵，其总体思路就是统计A类别实例被分成为B类别的次数。例如，要想知道分类器将数字3和数字5混淆多少次，只需要通过混淆矩阵的第5行第3列来查看。

要计算混淆矩阵，需要先有一组预测才能将其与实际目标进行比较。当然，可以通过测试集来进行预测，但是现在先不要动它（测试集最好留到项目的最后，准备启动分类器时再使用）。作为替代，可以使用`cross_val_predict（）`函数：

```python
from sklearn.model_selection import cross_val_predict

y_train_pred = cross_val_predict(sgd_clf , x_train , t_train , cv = 3)
```

与`cross_val_score（）`函数一样，`cross_val_predict（）`函数同样执行K-折交叉验证，但返回的不是评估分数，而是每个折叠的预测。这意味着对于每个实例都可以得到一个干净的预测（“干净”的意思是模型预测时使用的数据在其训练期间从未见过）。

现在可以使用`confusion_matrix（）`函数来获取混淆矩阵了。只需要给出目标类别`（y_train_5）`和预测类别`（y_train_pred）`即可：

3折的结果是将每一折结果都组合到一起，并按照它们在原始训练集中的顺序重新排列，形成 `y_train_pred` 数组。最终得到的`y_train_pred`与`x_train`长度相同。

由上文的`StratifiedKFold`可知，每一次迭代运算只获得一折预测值，三次则可得到三折预测值。

现在可以使用`confusion_matrix（）`函数来获取混淆矩阵了。只需要给出目标类别`（y_train_5）`和预测类别`(y_train_pred）`即可：
```python
from sklearn.metrics import confusion_matrix

confusion_matrix(y_train_5 , y_train_pred) #实际值在前 预测值在后。

```
结果如下：

```python
array([[53892,   687],  #实际负 即实际非5
       [ 1891,  3530]]) #实际正，即实际5
        #猜测负   #猜测正
``` 

左上表示两个负，右下表示两个正。687张被错误地分类成了“5”（假正类）；第二行表示所有“5”（正类）的图片中：1891张被错误地分为“非5”类别（假负类），3530张被正确地分在了“5”这一类别（真正类）

一个完美的分类器只有真正类和真负类，所以它的混淆矩阵只会在其对角线（左上
到右下）上有非零值：
```python
y_train_perfect_predictions = y_train_5 
confusion_matrix(y_train_5 , y_train_perfect_predictions)
```
结果如下：
```python
array([[54579,     0],
       [    0,  5421]])
```
混淆矩阵能提供大量信息，但有时你可能希望指标更简洁一些。正类预测的准确率是一个有意思的指标，它也称为分类器的精度。


$$精度 = \frac{TP}{TP+FP}$$
TP是**真正类**的数量，FP是**假正类**的数量。

分类器会忽略这个正类实例之外的所有内容。因此，精度通常与另一个指标一起使用，这个指标就是**召回率**

$$召回率 = \frac{TP}{TP+FN}$$
FN是**假负类**的数量。

精确率是分类正确的正样本 / 判定为正样本的总数；召回率是分类正确的正样本 / 真正正样本的总数。

![fix](/images/QQ_1724755975403.png)
混淆矩阵显示了真负（左上）、假正（右上）、假负（左下）和真正（右下）的示例
### 精度和召回率

`Scikit-Learn`提供了计算多种分类器指标的函数，包括精度和召回率：

```python
from sklearn.metrics import precision_score ,recall_score
precision_score(y_train_5 , y_train_pred) #3530 / (3530 + 687)
recall_score(y_train_5 , y_train_pred) #3530 / (3530+1891)
```

现在再看，这个5-检测器看起来并不像它的准确率那么光鲜亮眼了。当它说一张图片是5时，只有72.9%的概率是准确的，并且也只有75.6%的数字5被它检测出来了。

因此我们可以很方便地将精度和召回率组合成一个单一的指标，称为F1分数。当你需要一个简单的方法来比较两种分类器时，这是个非常不错的指标。F1分数是精度和召回率的谐波平均值。

$$F_1 = \frac{2}{\frac{1}{精度}+\frac{1}{召回率}} = 2 \times \frac{精度\times 召回率}{精度+召回率} = \frac{TP}{TP+\frac{FP+FN}{2}}$$

要计算F1分数，只需要调用`f1_score（）`即可
```python
from sklearn.metrics import f1_score
f1_score(y_train_5,y_train_pred)
# 0.7325171197343847
```
F1分数对那些具有相近的精度和召回率的分类器更为有利。这不一定能一直符合你的期望：在某些情况下，你更关心的是精度，而另一些情况下，你可能真正关心的是召回率。
遗憾的是，鱼和熊掌不可兼得，你不能同时增加精度又减少召回率，反之亦然。这称为精度/召回率权衡。

### 精度/召回率权衡

要理解这个权衡过程，我们来看看`SGDClassifier`如何进行分类决策。

要理解这个权衡过程，我们来看看SGDClassifier如何进行分类决策。对于每个实例，它会基于决策函数计算出一个分值，如果该值大于阈值，则将该实例判为正类，否则便将其判为负类。图3-3显示了从左边最低分到右边最高分的几个数字。假设决策阈值位于中间箭头位置（两个5之间）：在阈值的右侧可以找到4个真正类（真的5）和一个假正类（实际上是6）。因此，在该阈值下，精度为80%（4/5）。但是在6个真正的5中，分类器仅检测到了4个，所以召回率为67%（4/6）。现在，如果提高阈值（将其挪动到右边箭头的位置），假正类（数字6）变成了真负类，因此精度得到提升（本例中提升到100%），但是一个真正类变成一个假负类，召回率降低至50%。反之，降低阈值则会在增加召回率的同时降低精度。
![fix](/images/QQ_1724809511643.png)

总而言之：阈值右边相当于猜测为5的，阈值左边相当于猜测不为5的。在中间的阈值，共有6个5，猜出来了4个5，召回率为4/6；在右边的阈值，共有6个5，猜出了3个5，召回率为3/6。

还是很容易理解的。

在这个精度/召回率权衡中，图像按其分类器评分进行排名，而高于所选决策阈值的图像被认为是正的；阈值越高，召回率越低，但是（通常）精度越高

```python
y_score = sgd_clf.decision_function([some_digit])
y_score

#array([3456.80168233])
```
在类似本例题的二分类问题中：`decision_function` 方法通常用于返回给定数据点对于正类的“决策分数”或“置信度”。这个分数是一个单一的浮点数，它反映了分类器认为该数据点属于正类的程度。需要注意的是，这个分数并不直接对应于概率，而是分类器内部计算的一个值，它通常与分类器的决策边界有关。

```python
threshold = 0
y_some_digit_pred = (y_scores>threshold)
print(y_some_digit_pred)
#    array([ True])
```

SGDClassifier分类器使用的阈值是0，所以前面代码的返回结果与`predict（）`方法一样（也就是True）。我们来试试提升阈值：

```python
threshold = 8000
y_some_digit_pred = (y_scores > threshold)
y_some_digit_pred

#array([False])
```
这证明了提高阈值确实可以降低召回率。如果这张图确实是5，当阈值为0时，分类器可以检测到该图，但是当阈值提高到8000时，就错过了这张图。


那么要如何决定使用什么阈值呢？首先，使用`cross_val_predict（）`函数获取训练集中所有实例的分数，但是这次需要它返回的是决策分数而不是预测结果：
```python
y_scores = cross_val_predict(sgd_clf , x_train ,y_train_5 , cv = 3
							method = "decision_function")
```
这样就获取到了所有的`x_train` 的决策分数了
有了这些分数，可以使用`precision_recall_curve（）`函数来计算所有可能的阈值的精度和召回率：

```python
from sklearn.metrics import precision_recall_curve

precisions , recalls , thresholds = precision_recall_curve(y_train_5 , y_scores)

```

`thresholds`：对于排序后的预测分数，它会考虑每一个分数作为可能的分类阈值。即对于每个分数，高于该分数的样本被分类为正类，低于或等于该分数的样本被分类为负类。然后，它计算这个分类决策下的精确率和召回率。

总而言之 `thresholds` 就是上文的决策函数得到的决策变量从小到大的排列。
```python
len(thresholds)
# 60000
len(recalls)
# 60001
```
可以看到精确率和召回率的数是比阈值多1的。
这是因为阈值增大的时候减小召回值，使元素i>=阈值[i]的预测召回，最后一个元素为0，没有对应的阈值。

接下来可以作出精确率和召回率关于阈值的函数了：

```python
def plot_precision_recall_vs_threshold(precisions , recalls ,thresholds):
	plt.plot(thresholds,precisions[:-1],"b--",label = "precisions",linewidth = 2)
	plt.plot(thresholds , recall[:-1],"g-",label = "recalls",linewidth = 2)
	plt.legend(loc = "center right" , fontsize = 16)
	plt.xlabel("Thresholds",fontsize)#
	# fontsize=16 表示将 x 轴标签的字体大小设置为 16 磅。
	plt.grid(True) # 网格
	plt.axis([-50000,50000,0,1]) #轴的范围
```
调用函数：

```python
recall_90_precision = recalls[np.argmax(precisions>= 0.9)]
thresholds_90_precision = thresholds[np.argmax(precisions>=0.9)]

```
`np.argmax()` 会在括号内的布尔数组上寻找第一个 True 值的索引（因为布尔值在 NumPy 中被当作整数处理，其中 True 相当于 1，False 相当于 0）。如果数组中没有 True 值，则 `np.argmax() `的行为可能依赖于 NumPy 的版本和设置，但通常它会返回数组的第一个索引（在某些情况下可能是 0，但在某些版本的 NumPy 中可能是抛出错误或返回数组长度的值，这取决于对“最大”值的定义）。

总而言之，本例题中`np.argmax()`的用处是获取第一个`precisions>=0.9`的索引。


```python
plt.figure(figsize = (8,4)) #定义图片的大小
plot_precision_recall_vs_threshold(precisions,recalls,thresholds)  
#调用函数
plt.plot([thresholds_90_precision],[0.,0.9],"r:") #添加线条
plt.plot([-50000,threshold_90_precision],[0.9,0.9],"r:")
plt.plot([-50000,threshold_90_precision],[recall_90_precision,recall_90_precision],"r:")
plt.plot([threshold],[0.9],"ro")
plt.plot([threshold],[recall_90_precision],"ro")
plt.savefig("precision_recall_vs_threshold_plot")
plt.show()
```

![fix](/images/QQ_1724829822873.png)






为什么在图3-4中精度曲线比召回率曲线要崎岖一些？原因在于，当你提高阈值时，精度有时也有可能会下降（尽管总体趋势是上升的）。

接下来我们来看看 precision—recall 函数曲线图像。

```python
def plot_precision_vs_recall(precisions,recalls):
	plt.plot(recalls,precisions,"b-",linewidth = 2)
	plt.xlabel("Recall",fontsize = 16)
	plt.ylabel("Precision",fontsize = 16)
	plt.axis([0,1,0,1])
	plt.grid(True)
```
调用函数
```python
plt.figure(figsize = (6,4))
plot_precision_vs_recall(precisions,recalls)
plt.plot([0.0,recall_90_precision],[0.9,0.9],"r:")
plt.plot([recall_90_precision,recall_90_precision],[0.0,0.9],"r:")
plt.plot([recall_90_precision],[0.9],"ro")
plt.show
```
![fix](/images/QQ_1724831672100 1.png)

假设你决定将精度设为90%。查找图3-4并发现需要设置8000的阈值。更精确地说，你可以搜索到能提供至少90%精度的最低阈值

```python
threshold_90_precision = thresholds(np.argmax(precisiions >= 0.90))
y_train_pred_90 = (y_scores >= threshold_90_precision)
precision_score(y_train_5,y_train_pred_90) 
recall_score(y_train_5,y_train_pred_90)
```
两种计算方法上文已经提到了。

总而言之就是：获取90precision的阈值，反过来求该阈值下的精确率和召回率。


### ROC曲线

还有一种经常与二元分类器一起使用的工具，叫作受试者工作特征曲线（简称ROC）。它与精度/召回率曲线非常相似，但绘制的不是精度和召回率，而是真正类率（TPR）（召回率的另一名称）和假正类率（FPR）。FPR是被错误分为正类的负类实例比率。

$$TPR = \frac{TP}{TP+FN}.  FPR= \frac{FP}{FP+TN}$$
TPR表示猜5和所有的5的比值。
FPR表示猜5和所有不是5的比值。

要绘制ROC曲线，首先需要使用roc_curve（）函数计算多种阈值的TPR和FPR：

```python
from sklearn.matrics import roc_curve
fpr,tpr,thresholds = roc_curve(y_train_5,y_scores)
```
然后，使用`Matplotlib`绘制FPR对TPR的曲线。
```python
def plot_roc_curve(fpr,tpr,label=None)
	plt.plot(fpr,tpr,linewidth = 2,label = label)
	plt.plot([0,1],[0,1],'k--') #对角线 ：y = x
	plt.axis([0,1,0,1])
	plt.xlabel('False Positive Rate(Fall_Out)',fontsize = 16)
	plt.ylabel('True Positive Rate(Recall)',fontsize = 16)
	plt.grid(True)

plt.figure(figsize = (8,6))
plot_roc_curve(fpr,tpr)
fpr_90 = fpr[np.argmax(tpr>=recall_90_precision)]
plt.plot([fpr_90,fpr_90],[0,recall_90_precision],"r:")
plt.plot([0,fpr_90],[recall_90_precision,recall_90_precision],"r:")
plt.plot([fpr_90],[recall_90_precision],"ro")
plt.savefig("roc_curve_plot")
plt.show()
```
![fix](/images/QQ_1724834223572.png)
我们仔细分析一下这个图：
y表示的是猜中5与所有5的比值。
x表示的是猜是5和所有不是5的比值。
那么当x越小时，1-x则越大 。1-x表示的是猜不是5和所有不是5的比值，这是有益于本模型评估的值。

所以要图像尽量偏向左上角，效果则越好。

同样这里再次面临一个折中权衡：召回率（TPR）越高，分类器产生的假正类（FPR）就越多。虚线表示纯随机分类器的ROC曲线、一个优秀的分类器应该离这条线越远越好（向左上角）。

有一种比较分类器的方法是测量曲线下面积（AUC）。完美的分类器的ROC AUC等于1，而纯随机分类器的ROC AUC等于0.5。`Scikit-Learn`提供计算ROC AUC的函数：

```python
from sklearn.metrics import roc_auc_score
roc_auc_score(y_train_5,y_scores)
#0.96114559368217
```

由于ROC曲线与精度/召回率（PR）曲线非常相似，因此你可能会问如何决定使用哪种曲线。

有一个经验法则是，当正类非常少见或者你更关注假正类而不是假负类时，应该选择PR曲线，反之则是ROC曲线。




现在我们来训练一个RandomForestClassifier分类器，并比较它和SGDClassifier分类器的ROC曲线和ROC AUC分数。首先，获取训练集中每个实例的分数。

`dict_proba（）`方法会返回一个数组，其中每行代表一个实例，每列代表一个类别，意思是某个给定实例属于某个给定类别的概率
```python
from sklearn.ensemble import RandomForestClassifier

forest_clf = RandomForestClassifier(random_state =42)
y_probas_forest = cross_val_predict(forest_clf , X_train,y_train_5,cv = 3,method = "predict_proba") #概率
```

`roc_curve（）`函数需要标签和分数，但是我们不提供分数，而是提供类概率。我们直接使用正类的概率作为分数值：

```python
y_scores_forest = y_probas_forest[:,1]
fpr_forest,tpr_forest,thresholds_forest = roc_curve(y_train_5,y_scores_forest)

recall_for_forest = tpr_forest[np.argmax(fpr_forest>=fpr_90)]

```

现在可以绘制ROC曲线了。
```python
plt.figure(figsize = (8,6))
plt.plot(fpr,tpr,"b:",linewidth = 2,label = "SGD")
plot_roc_curve(fpr_forest,tpr_forest,"Random Forest")
plt.plot([fpr_90,fpr_90],[0,recall_90_precision],"r:")
plt.plot([0,fpr_90],[recall_90_precision,recall_90_precision],"r:")
plt.plot([fpr_90],[recall_90_precision],"ro")
plt.plot([fpr_90,fpr_90],[0,recall_for_forest],"r:")
plt.plot([0,fpr_90],[recall_for_forest,recall_for,forest]."r:")
plt.plot([fpr_90],[recall_for_forest],"ro")
plt.grid(True)
plt.legend(loc="lower right",fontsize = 16)
plt.savefig("roc_curve_comparison_plot")
plt.show()
```
![fix](/images/QQ_1724836552373.png)
比较ROC曲线：随机森林分类器优于SGD分类器，因为它的ROC曲线更靠近左上角，并且具有更大的AUC

RandomForestClassifier的ROC曲线看起来比SGDClassifier好很多，它离左上角更接近，因此它的ROC AUC分数也高得多。接下来我们看看它的auc值

```python
roc_auc_score(y_train_5,y_scores_forest)
#0.9952328942154235
```
再测一测精度和召回率的分数：
```python
y_train_pred_forest = cross_val_predict(forest_clf,X_train,y_train_5,cv = 3)
precision_score(y_train_5,y_train_pred_forest)
#0.9819324430479183
recall_score(y_train_5,y_train_pred_forest)
#0.8404773911581779
```

## 多类分类器

二元分类器在两个类中区分，而多类分类器（也称为多项分类器）可以区分两个以上的类。

有一些算法（如随机森林分类器或朴素贝叶斯分类器）可以直接处理多个类。也有一些严格的二元分类器（如支持向量机分类器或线性分类器）。但是，有多种策略可以让你用几个二元分类器实现多类分类的目的。

要创建一个系统将数字图片分为10类（从0到9），一种方法是训练10个二元分类器，每个数字一个（0-检测器、1-检测器、2-检测器，以此类推）。然后，当你需要对一张图片进行检测分类时，获取每个分类器的决策分数，哪个分类器给分最高，就将其分为哪个类。**这称为一对剩余（OvR）策略，也称为一对多（one-versus-all）。**

另一种方法是为每一对数字训练一个二元分类器：一个用于区分0和1，一个区分0和2，一个区分1和2，以此类推。**这称为一对一（OvO）策略**。如果存在N个类别，那么这需要训练N×（N-1）/2个分类器。对于MNIST问题，这意味着要训练45个二元分类器！当需要对一张图片进行分类时，你需要运行45个分类器来对图片进行分类，最后
看哪个类获胜最多。**OvO的主要优点在于，每个分类器只需要用到部分训练集对其必须区分的两个类进行训练。**

Scikit-Learn可以检测到你尝试使用二元分类算法进行多类分类任务，**它会根据情况自动运行OvR或者OvO**。我们用`sklearn.svm.SVC`类来试试SVM分类器

```python
from sklearn.svm import SVC

svm_clf = SVC(gamma = "auto",random_state= 42)
svm_clf.fit(X_train[:1000],y_train[:1000])
svm_clf.predict([some_digit])

# array([5],dtype = uint8)
```
这段代码使用原始目标类0到9`（y_train）`在训练集上对SVC进行训练，而不是以“5”和“剩余”作为目标类
而在内部，Scikit-Learn实际上训练了45个二元分类器，获得它们对图片的决策分数，然后选择了分数最高的类。
可以调用decision_function（）方法查看是不是如此。
```python
some_digit_scores = svm.clf.decision_function([some_digit])
some_digit_scores

```
输出结果如下
```python
  
array([[ 2.81585438, 7.09167958, 3.82972099, 0.79365551, 5.8885703 , 9.29718395, 1.79862509, 8.10392157, -0.228207 , 4.83753243]])
```
不再是每个实例返回1个。分数最大的是第六个，对应的索引是5。
我们可以用`np.argmax()`函数来进一步验证
```python
np.argmax(some_digit_scores)
# 5
```
```python
svm.clf.classes_
#array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)
```

当训练分类器时，目标类的列表会存储在classes_属性中，按值的大小排序。
在本例里，classes_数组中每个类的索引正好对应其类本身（例如，索引上第5个类正好是数字5这个类），但是一般来说，不会这么恰巧。

如果想要强制Scikit-Learn使用一对一或者一对剩余策略(前文提到SVC可以自动检测是一对一还是一对多，这里是强制使用），可以使`OneVsOneClassifier`或`OneVsRestClassifier`类。只需要创建一个实例，然后将分类器传给其构造函数（它甚至不必是二元分类器）。例如，下面这段代码使用OvR策略，基于SVC创建了一个多类分类器：

```python
from sklearn.multiclass import OneVsRestClassifier
ovr_clf = OneVsRestClassfier(SVC(gamma="auto",random_state = 42))
ovr_clf.fix(x_train[:1000],y_train[:1000])
ovr_clf.predict([some_digit])
# array([5], dtype=uint8)
len(ovr_clf.estimators_)
# 10
```
由于本身的SVC是一对一，这里强制更换成为了一对多ovr。所以只有10个二元分类器。
并且决策函数得到的数据也不同。

```python
ovr_clf.decision_function([some_digit])
len(ovr_clf.estimators_) #10

```
结果如下
```python
array([[-0.9990256 , -0.99963766, -0.99949709, -0.99902667, -0.99986906,
         0.10132159, -0.99976287, -0.99933311, -0.99943631, -0.99924045]])
```

训练SGDClassifier或者RandomForestClassifier同样简单：
```python
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(max_iter = 1000,tol = 1e-3 , random_state = 42)
sgd_clf.fit(X_train ,y_train)
sgd_clf.predict([some_digit])
```

这次Scikit-Learn不必运行OvR或者OvO了，因为SGD分类器直接就可以将实例分为多个类。调用d`ecision_function（）`可以获得分类器将每个实例分类为每个类的概率列表：让我们看一下SGD分类器分配到的每个类：

```python
sgd_clf.decision_function([some_digit])
```
结果如下
```python
array([[-31893.03095419, -34419.69069632,  -9530.63950739,
          1823.73154031, -22320.14822878,  -1385.80478895,
        -26188.91070951, -16147.51323997,  -4604.35491274,
        -12050.767298  ]])
```
现在，你当然要评估这个分类器。与往常一样，可以使用交叉验证。使用cross_val_score（）函数来评估SGDClassifier的准确性：
```python
from sklearn.model_selection import cross_val_score
cross_val_score(sgd_clf,X_train,y_train,cv=3,scoring = "accuracy")
```
结果如下：
```python
array([0.87365, 0.85835, 0.8689 ])
```
在所有的测试折叠上都超过了85%。如果是一个纯随机分类器，准确率大概是10%，所以这个结果不是太糟，但是依然有提升的空间。例如，将输入进行简单缩放(标准化)可以将准确率提到89%以上：

`标准差标准化（standardScale）使得经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：`

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler
X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))
cross_val_score(sgd_clf,X_train_scaled , y_train , cv=3)

```
结果如下：

```python
array([0.8983, 0.891 , 0.9018])
```
### 误差分析
如果这是一个真正的项目，你将遵循机器学习项目清单中的步骤。
在这里，假设你已经找到了一个有潜力的模型，现在你希望找到一些方法对其进一步改进。方法之一就是分析其错误类型。

首先看看混淆矩阵。就像之前做的，使用`cross_val_predict（）`函数进行预测，然后调用`confusion_matrix（）`函数：

```python
from sklearn.metrics import confusion_matrix

y_train_pred = cross_val_predict(sgd_clf,X_train_scaled,y_train,cv=3)

conf_mx = confusion_matrix(y_train,y_train_pred)
conf_mx
```
结果如下
```python
array([[5578, 0, 22, 7, 8, 45, 35, 5, 222, 1],
 [ 0, 6410, 35, 26, 4, 44, 4, 8, 198, 13],
 [ 28, 27, 5232, 100, 74, 27, 68, 37, 354, 11],
 [ 23, 18, 115, 5254, 2, 209, 26, 38, 373, 73],
 [ 11, 14, 45, 12, 5219, 11, 33, 26, 299, 172],
 [ 26, 16, 31, 173, 54, 4484, 76, 14, 482, 65],
 [ 31, 17, 45, 2, 42, 98, 5556, 3, 123, 1],
 [ 20, 10, 53, 27, 50, 13, 3, 5696, 173, 220],
 [ 17, 64, 47, 91, 3, 125, 24, 11, 5421, 48],
 [ 24, 18, 29, 67, 116, 39, 1, 174, 329, 5152]])
```

数字有点多，使用`Matplotlib`的`matshow（）`函数来查看混淆矩阵的图像表示通常更加方便

```python
import matplotlib.pyplot as plt

plt.matshow(conf_mx , cmap = plt.cm.Reds)
plt.show() 

```
![fix](/images/QQ_1724946556656.png)
混淆矩阵看起来很不错，因为大多数图片都在主对角线上，这说明它们被正确分类。数字5看起来比其他数字稍稍暗一些，这可能意味着数据集中数字5的图片较少，也可能是分类器在数字5上的执行效果不如在其他数字上好。

让我们把焦点放在错误上。首先，你需要将混淆矩阵中的每个值除以相应类中的图片数量，这样你比较的就是错误率而不是错误的绝对值（后者对图片数量较多的类不公平）

```python
row_sums = conf_mx.sum(axis = 1 ,keepdims = True)
norm_conf_mx = conf_mx / row_sums
```
这样就可以得到一个新的矩阵。我们来看看这个矩阵的数据上什么样子的。
还是使用上文的`matshow（）`函数。

```python
np.fill_diagonal(norm_conf_mx , 0) 
#diagonal 是对角的意思 这段代码的作用是将矩阵的对角的数据都用0替换
plt.matshow(norm_conf_mx,cmap = plt.cm.Reds) #cmap 颜色 红色比较明显一些
plt.show()
```

![fix](/images/QQ_1724995840711.png)

我们可以清晰的看到对角线的颜色是白色的。
现在可以清晰地看到分类器产生的错误种类了。记住，每行代表实际类，而每列表示预测类。第8列看起来非常亮，说明有许多图片被错误地分类为数字8了。

然而，第8行不那么差，告诉你实际上数字8被正确分类为数字8。注意，错误不是完全对称的，比如，数字3和数字5经常被混淆（在两个方向上）

通过上图来看，你的精力可以花在改进数字8的分类错误上。例如，可以试着收集更多看起来像数字8的训练数据，以便分类器能够学会将它们与真实的数字区分开来。或者，也可以开发一些新特征来改进分类器——例如，写一个算法来计算闭环的数量（例如，数字8有两个，数字6有一个，数字5没有）。再或者，还可以对图片进行预处理（例如，使用
`Scikit-Image`、`Pillow`或`OpenCV`）让某些模式更为突出，比如闭环之类的


分析单个的错误也可以为分类器提供洞察：它在做什么？它为什么失败？但这通常更加困难和耗时。例如，我们来看看数字3和数字5的示例（`plot_digits（）``函数只是使用Matplotlib`的`imshow（）`函数）：

```python
def plot_digits(instances ,images_per_row =10 ,**options):
	images_per_row = min(len(instances),images_pre_row)
	images = [instance.reshape(28,28) for instance in instances]
	n_rows = (len(instances)-1)//images_per_row +1
	row_images = []
	n_empty = n_rows * images_per_row - len(instances)
	images.append(np.zeros((28,28 * n_empty))) #凑整
	for row in range(n_rows):
		rimages = images[row *images_per_row :(row+1)*images_per_row]
		row_images.append(np.concatenate(rimages,axis = 1))
	image = np.concatenate(row_images ,axis = 0)
	plt.imshow(image,cmap = mpl.cm.binary,**options)
	plt.axis("off")

cl_a ,cl_b = 3,5
X_aa = X_train[(y_train ==cl_a)&(y_train_pred ==cl_a)]
X_ab = X_train[(y_train ==cl_a)&(y_train_pred == cl_b)]
X_ba = X_train[(y_train==cl_b)&(y_train_pred==cl_a)]
X_bb = X_train[(y_train == cl_b)&(y_train_pred == cl_b)]

plt.figure(figsize = (8,8))
plt.subplot(221);plot_digits(X_aa[:25],images_per_row =5)
plt.subplot(222);plot_digits(X_ab[:25],images_per_row =5)
plt.subplot(223);plot_digits(X_ba[:25],images_per_row =5)
plt.subplot(224);plot_digits(X_bb[:25],images_per_row =5)
plt.savefig("error_analysis_digits_plot")
plt.show()
```
结果如下
![fix](/images/QQ_1724997156348.png)
左侧的两个5×5矩阵显示了被分类为数字3的图片，右侧的两个5×5矩阵显示了被分类为数字5的图片。分类器弄错的数字（即左下方和右上方的矩阵）里，确实有一些写得非常糟糕，即便是人类也很难做出区分

然而，对我们来说，大多数错误分类的图片看起来还是非常明显的错误，我们很难理解分类器为什么会弄错。原因在于，我们使用的简单的SGDClassifier模型是一个线性模型。它所做的就是为每个像素分配一个各个类别的权重，当它看到新的图像时，将加权后的像素强度汇总，从而得到一个分数进行分类。而数字3和数字5只在一部分像素位上有区别，所以分类器很容易将其弄混。

*数字3和数字5之间的主要区别是在于连接顶线和下方弧线的中间那段小线条的位置*。
如果你写的数字3将连接点略往左移，分类器就可能将其分类为数字5，反之亦然。换言之，这个分类器对图像移位和旋转非常敏感。因此，减少数字3和数字5混淆的方法之一，就是对图片进行预处理，确保它们位于中心位置并且没有旋转。这也同样有助于减少其他错误。


## 多标签分类 KNeighborsClassifier

到目前为止，每个实例都只会被分在一个类里。而在某些情况下，你希望分类器为每个实例输出多个类。例如，人脸识别的分类器：如果在一张照片里识别出多个人怎么办？当然，应该为识别出来的每个人都附上一个标签。假设分类器经过训练，已经可以识别出三
张脸——爱丽丝、鲍勃和查理，那么当看到一张爱丽丝和查理的照片时，它应该输出`[1，0，1]`（意思是“是爱丽丝，不是鲍勃，是查理”）这种输出多个二元标签的分类系统称为多标签分类系统。

为了阐释清楚，这里不讨论面部识别，让我们来看一个更为简单的示例：

```python
from sklearn.neighbor import KNeighborsClassifier

y_train_large = (y_train>=7)
y_train_odd = (y_train %2 == 1) #奇数
y_multilabel = np.c_[y_train_large ,y_train_odd] 

knn_clf = KNeighborsClassifier
knn_clf.fit(X_train ,y_multilabel)
```
`y_multilabel = np.c_[y_train_large ,y_train_odd] `将两个布尔数组连接组成一个含有两个标签点数组`y_multilabel`
创建一个`y_multilabel`数组，其中包含两个数字图片的目标标签：第一个表示数字是否是大数（7、8、9），第二个表示是否为奇数。
下一行创建一个`KNeighborsClassifier`实例（它支持多标签分类，不是所有的分类器都支持），然后使用多个目标数组对它进行训练。现在用它做一个预测，注意它输出两个标签：
```python
knn_clf.predict([some_digit])
#array([[False,  True]])
```

我们来看看`[some_digit]`是什么
```python
plt.imshow(np.array([some_digit]).reshape(28,28))
plt.show()
```
![fix](/images/QQ_1724998587194.png)（数字五）

结果是正确的！数字5确实不大（False），为奇数（True）。

### 评估多标签分类的方法


评估多标签分类器的方法很多，如何选择正确的度量指标取决于你的项目。比如方法之一是测量每个标签的F1分数（或者之前讨论过的任何其他二元分类器指标），然后简单地计算平均分数。下面这段代码计算所有标签的平均F1分数：

```python
from sklearn.metrics import f1_score
y_train_knn_pred = cross_val_predict(knn_clf,X_train,y_multilabel,cv=3)
f1_score(y_multilabel,y_train_knn_pred,average = "macro")
```

[f1_score 是基于精确率和召回率得到的一个反映模型预测功能的值]
这里是计算两个标签的f1值并取平均。


## 多输出分类

我们即将讨论的最后一种分类任务称为多输出-多类分类（或简单地称为多输出分类）。简单来说，它是多标签分类的泛化，其标签也可以是多类的（比如它可以有两个以上可能的值，不只是输出两个布尔值）。

为了说明这一点，构建一个系统去除图片中的噪声。给它输入一张有噪声的图片，它将（希望）输出一张干净的数字图片，与其他MNIST图片一样，以像素强度的一个数组作为呈现方式。请注意，这个分类器的输出是多个标签（一个像素点一个标签），每个标签可以有多个值（像素强度范围为0到225）。所以这是个多输出分类器系统的示例

多输出系统也不仅仅限于分类任务，可以让一个系统给每个实例输出多个标签，同时包括类标签和值标签。

还先从创建训练集和测试集开始，使用NumPy的`randint（）`函数为MNIST图片的像素强度增加噪声。目标是将图片还原为原始图片：

```python
noise = np.random.randint(0,100,(len(X_train),784))
# 第一个参数表示下限，第二个参数表示上限，第三个参数表示格式，长度为X_train，特征值有784个
X_train_mod = X_train +noise
noise = np.random.randint(0,100,(len(X_train),784))
X_test_mod = X_test +noise
y_train_mod = X_train
y_test_mod = X_test

```

输出一下数据看看噪声的效果。

```python
some_index= 0
plt.subplot(121);plt.imshow(np.array(X_test_mod.iloc[some_index]).reshape(28,28))
plt.subplot(122);plt.imshow(np.array(y_test_mod.iloc[some_index]).reshape(28,28))
plt.savefig("noisy_digit_examplt_plot")
plt.show()
```
结果如下：

![fix](/images/QQ_1725000150896.png)
左边是有噪声的输入图片，右边是干净的目标图片。
现在通过训练分类器，清洗这张图片。
```python
knn_clf.fit(X_train_mod,t_train_mod)
clean_digit = knn_clf.predict([X_test_mod.iloc[some_index]])
plt.imshow(np.array(clean_digit).reshape(28,28))
plt.savefig("cleaned_digit_examplt_plot")
plt.show()
```
![fix](/images/QQ_1725001002690.png)
