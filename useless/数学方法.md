### 方向导数

偏导数给出了当 x 沿着某坐标轴(比如 x1 )变动时，函数 $y y = f(x)$
变化的速率。
然而偏导数并不能像二维函数一样表示 $x x \to y$的 的变化速率，这里就引入了偏导数。

有时我们想知道当x沿着任意方向变化时，函数 $f f(x)$的 的变化速率

任意给定 $X X^*(x_1^*,x_2^*,x_3^*,\dots,x_n^*)$， ，考虑 $f f(X)$在 在 $X X^*$处 处，沿着固定的方向 $V V = (v_1,v_2,\dots,v_n)$的 的方向导数。其中将 $V V$的 的长度标准化为1
即 $$||V||=\sqrt{v_1^2+\dots+v_b^2} = 1 $$
沿着方向 $V V$， ， $X X$变 变化了 $\ \Delta X$， ，经过 $X X^*$的 的直线方程为 $$X = X^*+\Delta X $$
随着参数 $\ \Delta X \in R$变 变化，可得到整条直线。
函数  $f f(X)$沿 沿着此直线方向的取值可写为
 $$f(X^*+\Delta X ) = f(x_1^*+\Delta x_1*v_1,x_2^*+\Delta x_2*v_2,x_3^*+\Delta x_3 *v_3,\dots,x_n^*+\Delta x_n *v_n) $$

使用微分性质，则方向导数： $$\frac{\partial f(X^*)}{\partial V}= \lim_{\Delta X\to0}\frac{f(X^*+\Delta X )-f(X^*)}{\Delta X} $$
有 $$x_1 = \Delta X *v_1 $$
 $$\frac{\partial f(X^*)}{\partial V} = \frac{\partial f(X^*)}{\partial x_1}v_1 +\frac{\partial f(X^*)}{\partial x_2}v_2+\dots +\frac{\partial f(X^*)}{\partial x_n}v_n $$
写成矩阵的形式为
 $$[\frac{\partial f(X)}{\partial x_1},\frac{\partial f(X)}{\partial x_2},\dots,\frac{\partial f(X)}{\partial x_n}]
\left[
\begin{matrix}
v_1 \\
v_2\\
\dots \\
v_n
\end{matrix}
\right
] $$
方向导数 $\ \frac{\partial f(X^*)}{\partial V}$是 是各偏导数 $\ \frac{\partial f(X^*)}{\partial x_j}$的 的线性组合，而组合的权重为 $\ \Delta X$方 方向 $V V$沿 沿着各坐标轴的分量 $( (v_1,v_2,\dots,v_n)$

梯度向量 $\ \nabla f ( x)$   为函数 $f f(x)$   增长最快的方向，而负梯度方向 $- -\nabla f ( x)$为 为该函数下降最快的方向

证明：

在n维空间 $R R^n$中 中，记梯度向量 $\ \nabla f ( X^*)$   与方向 $V V$的 的夹角为 $\ \theta$   ，则根据线性代数知识，此夹角的余弦为 $$\cos \theta = \frac{\nabla f(X^*)V}{||\nabla f(X^*)||*||V||} $$
由于 $| ||V|| =1$
故沿方向 $V V$的 的方向导数可写为 $$\frac{\partial f(X^*)}{\partial V} = \nabla f(X^*) V =||\nabla f(X^*)|| *\cos \theta   $$
由于  $- -1 \leq\cos \theta \leq1$， ，故当 $\ \cos \theta = 1$   方向导数取最大 为梯度的模, $| ||\nabla f(X^*)||$
因此，梯度向量 $\ \nabla f(X^*)$为 为在 $x x^*$处 处，函数  $f f ( x)$增 增加最快的方向，称为“梯度上升”

同理可得，当 $\ \cos \theta = -1$   ，方向导数最小，为 $- -||\nabla f(X^*)||$， ，因此负梯度向量 $- -\nabla f(x^*)$
为在 $x x^*$处 处，函数f(x)下降最快的方向，称为"梯度下降"
函数  $f f ( x)$上 上升最快与下降最快的方向正好相反。


在几何上，应如何想象梯度向量？这可通过函数  $f f ( x)$的 的“水平集”(level set)来考察。

任意给定 $x x^*$， ，函数  $f f ( x)$   相应的水平集可定义为 $$
C=\{ X:f(X) = f(X^*)\} $$
水平集也称为“等值集”(contour set)；比如，地形图的“等高线”或气压图的“等压线”，
![fix](//images/images/QQ_1730219356592.png)


梯度向量  $\ \nabla f(X^*)$   与 水平集 $C C=\{ X:f(X) = f(X^*)\}$正 正交

证明：给定水平集 $C C=\{ X:f(X) = f(X^*)\}$， ，假定 $$X(t) = (x_1^*+x_1(t),\dots,x_n^*+x_n(t)) $$
为在水平集 $C C$上 上，经过 $X X^*$的 的 一条任意曲线，当 $t t=0$时 时， $X X(t)=X^*$
而当 $t t$变 变化时，即得到 $R R^n$空 空间的一条曲线

对 $X X(t)$求 求导，有 $$\frac{dX(0)}{dt} = 
\left(
\begin{matrix}
\frac{dx_1(0)}{dt},\dots,\frac{dx_b(0)}{dt}
\end{matrix}
\right)
 $$
将曲线方程 $X X^*(t)$代 代入水平集的方程中，则有

 $$f(X(t)) = f(x_1^*+x_1(t),\dots,x_n^*+x_n(t))=f(X^*) $$

在 $t t = 0$处 处，对方程求导(方向导数，方向沿 $X X(t)$) )。水平集 $f f(X(t))$关 关于 $t t$对 对导数是 $0 0$。 。
由链式法则可以得到
 $$\frac{\partial f(X^*)}{\partial x_1}\frac{dx_1(0)}{dt}+\dots+\frac{\partial f(X^*)}{\partial x_n}\frac{dx_n(0)}{dt}=0 $$

则 $$\nabla f(X^*)\frac{dX(0)}{dt} = 0 $$
 $\ \frac{dX(0)}{dt} = \left(\begin{matrix}\frac{dx_1(0)}{dt},\dots,\frac{dx_b(0)}{dt}\end{matrix}\right)$为 为曲线 $X X(t)$在 在 $X X^*$处 处对切线方向

则梯度向量 $\ \nabla f(X^*)$与 与此切线方向垂直(正交)

由于 $X X(t)$   为水平集 $C C$上 上的任意曲线，而它们在 $X X^*$处 处的切线方向均与梯度向量
 $\ \nabla f ( X)$垂 垂直，故在水平集C这一曲面上，通过点 $X X^*$的 的一切曲线在点 $X X^*$处 处的切线都在同一个平面上，即水平集 $C C$在 在点 $X X^*$的 的切平面

因此，故梯度向量 $\ \nabla f ( X)$   与水平集C正交

### 向量微分
**在进行最优化时，常需对向量求微分**

##### 向量微分规则(向量都是列向量)
对于线性函数 $y y = X^T \beta$其 其向量微分为 $\ \frac{\partial(x^T \beta)}{\partial X} = \beta$
将此线性函数展开写： $$y = X^T\beta = \beta_1x_1+\beta_2x_2+\dots+\beta_nx_n $$
其中参数向量 $\ \beta = (\beta_1 \beta_2 \dots \beta_n)^T$

由于 $\ \frac{\partial y}{\partial x_i} = \beta_i$
 $$\frac{\partial X^T\beta}{\partial X} = 
\left[
\begin{matrix}
\frac{\partial (X^T\beta)}{\partial x_1}\\
\frac{\partial (X^T\beta)}{\partial x_n}\\
\dots \\
\frac{\partial (X^T\beta)}{\partial x_n}
\end{matrix}
\right]
=
\left[
\begin{matrix}
\beta_1\\
\beta_2\\
\dots\\
\beta_n
\end{matrix}
\right] = \beta
 $$
**此向量微分的规则类似于对一次函数求导**


##### 二次型的向量微分规则

对于二次型的函数  $y y= X^T A X$， ，其向量微分为 $\ \frac{\partial X^TAX}{\partial X} = (A+A$^ ^T)X
特别的，如果A为对称矩阵，那么有 $\ \frac{\partial X^TAX}{\partial} =2AX$
 $\ \frac{\partial X^T A X}{\partial X} = (A+A^T)X$
证明如下：

 $$f(x) = [x_1,x_2,x_3,\dots ,x_n]
\left[
\begin{matrix}
a_{11} & a_{12} &a_{13}&\dots& a_{1n}\\
a_{21} &a_{22} &a_{23} &\dots &a_{2n}\\
a_{31} &a_{32} &a_{33} & \dots &a_{3n}\\
\dots &\dots &\dots &\dots&\dots \\
a_{n1} &a_{n2} &a_{n3} &\dots &a_{nn}

\end{matrix}
\right
]
\left[
\begin{matrix}
x_1\\
x_2\\
x_3\\
\dots\\
x_n
\end{matrix}
\right
]
=\sum ^n_{i=1} a_{ii}{x_1}^2+\sum^{n-1}_{i=1}\sum^n_{j=i+1}(a_{ij}+a_{ji})x_ix_j
 $$


 $f f(x)$对 对 $x x_1$求 求导就可以得到 $$2a_{11}x_1 +(a_{21}+a_{12})x_2+\dots +(a_{n1}+a_{1n})x_n $$
可以用矩阵表示为：
 $$[2a_{11},(a_{12}+a_{21}),(a_{31}+a_{13}),\dots,(a_{1n}+a_{n1})]
\left[
\begin{matrix}
x_1 \\
x_2 \\
x_3\\ 
\dots \\
x_n
\end{matrix}
\right
] $$
对每一个x求导并且最终组成一个矩阵。矩阵如下：

 $$
\left[
\begin{matrix}
2a_{11}&(a_{12}+a_{21})&(a_{13}+a_{31})&\dots&(a_{1n}+a_{n1}) \\
(a_{21}+a_{12}) &2a_{22} &(a_{23}+a_{32}) &\dots &(a_{2n}+a_{n2}) \\
(a_{31}+a_{13}) &(a_{32}+a_{23}) &2a_{33}&\dots &(a_{3n}+a_{n3})\\
\dots &\dots &\dots &\dots &\dots\\
(a_{n1}+a_{1n}) &(a_{n2}+a_{2n})& (a_{n3}+a_{3n}) &\dots &2a_{nn}
\end{matrix}
\right] $$
仔细看，这个矩阵就是 $A A+A^T$
那么结果就是 $( (A+A^T)X$

##### 复合函数的向量微分规则

对于复合函数 $y y = f(X(Z))$其 其中向量微分为 $\ \frac{\partial y}{\partial Z} = (\frac{\partial X}{\partial Z})^T\frac{\partial f}{\partial X}$

将此复合函数展开写 $$y = f(X(Z)) = f(x_1(Z_1,\dots,Z_k),\dots,x_n(Z_1,\dots,Z_k)) $$
其中 $X X= (x_1,\dots,x_n)$， ，而 $Z Z = (z_1,\dots,z_k)$

考虑对 $Z Z_k$   求偏导数。根据微积分的链式法则，此复合函数的偏导数可写为 $$\frac{\partial y}{\partial z_k} = \frac{\partial f}{\partial x_1}\frac{\partial x_1}{\partial z_k}+\frac{\partial f}{\partial x_2}\frac{\partial x_2}{\partial z_k}+\dots+\frac{\partial f}{\partial x_n}\frac{\partial x_n}{\partial z_k}=(\frac{x_1}{\partial z_k} \dots \frac{\partial x_n}{\partial z_k})
\left(
\begin{matrix}
\frac{\partial f}{\partial x_1}\\
\frac{\partial f}{\partial x_2} \\
\dots \\
\frac{\partial f}{\partial x_n}
\end{matrix}
\right) $$
上式对 $K K = 1,\dots,K$均 均成立，将这些方程叠放，可以得到梯度向量：
 $$\frac{\partial y}{\partial Z} = 
\left(
\begin{matrix}
\frac{\partial y}{\partial z_1} \\
\frac{\partial y}{\partial z_2} \\
\dots \\
\frac{\partial y}{\partial z_n}
\end{matrix}
\right) = 
\left(
\begin{matrix}
\frac{\partial x_1}{\partial z_1} \dots \frac{\partial x_n}{\partial z_1}\\
\frac{\partial x_1}{\partial z_2} \dots \frac{\partial x_n}{\partial z_2} \\
\dots\ \dots \ \dots\\
\frac{\partial x_1}{\partial z_k} \dots \frac{\partial x_n}{\partial z_k}
\end{matrix}
\right
)
\left(
\begin{matrix}
\frac{\partial f}{\partial x_1}\\
\frac{\partial f}{\partial x_2}\\
\dots \\
\frac{\partial f}{\partial x_n}
\end{matrix}
\right)
 =(\frac{\partial X}{\partial Z})^T\frac{\partial f}{\partial X} $$

其中， $( (\frac{\partial X}{\partial Z})^T$为 为 $X X(Z)$的 的雅各比矩阵 $\ \frac{\partial X}{\partial Z}$的 的转置

上式似乎与一元复合函数的微分规则不同，但如果将上式两边
同时转置则有 $$(\frac{\partial y}{\partial Z})^T = (\frac{\partial f}{\partial X})^T(\frac{\partial X}{\partial Z}) $$

将梯度向量定义为行向量，则复合函数的向量
微分规则在形式上与一元复合函数相同。

在较早的文献中，为了保持这种形式上的一致，一般将梯度向量定义为偏导数的行向量。
### 最优化

#### 一元最优化
机器学习的主要方法为最优化，尤其是最小化问题。有时也涉及最大化问题。

最大化问题可等价地写为最小化问题，只要将目标函数加上负号即可：
 $$\max_xf(x) = \min_x[-f(x)] $$
因此，我们主要讨论最小化问题。
考虑以下无约束的一元最小化问题 $$\min_xf(x) $$
![fix](//images/images/QQ_1730388729390.png)
函数  $f f ( x)$   在其“山谷”底部 $X X^*$处 处达到局部最小值

在山底 $x x^*$处 处，  $f f (x)$   的切线恰好为水平线，故切线斜率为 0。
故一元最小化问题的必要条件为
 $$f'(x^*)=0 $$
由于此最小化的必要条件涉及一阶导数，故称为**一阶条件**

直观上， 如果 $f f'(x^*)<0$   ，则在  $x x^*$处 处增加  $x x$可 可使函数值 $f f ( x)$   进一步下降，故 $f f ( x)$   不是最小值；反之，如果 $f f'(x^*)>0$， ，则在 $x x^*$处 处减少 $x x$可 可使函数值f(x)进一步下降，故 $f f(x^*)$也 也不是最小值。因此，在最小值处，必然有 $f f'(x^*) = 0$。 。

根据同样的逻辑，最大化问题的一阶条件与最小化问题相同，都要求在最优值 $x x^*$处 处的切线斜率为0，即 $f f'(x^*)=0$
![fix](//images/images/QQ_1730389257440.png)
最小化问题与最大化问题的区别在于最优化的**二阶条件**。
最小化要求 $f f''(x^*)\geq 0$( ( $f f'(x^*) =0$， ，则 $x x<x^*，f'(x)<0$， ，在 $x x^*$处 处为凸函数，单调递减到最小，即**最小化**)

最大化要求 $f f''(x^*)\leq 0$( ( $f f'(x^*) =0$， ，则 $x x<x^*，f'(x)>0$， ，在 $x x^*$处 处为凹函数，单调递增到最大，即**最大化**)

对于最优化的一阶条件与二阶条件，下面给出较为严格的证明：
**假设 $f f(x)$在 在 $x x^*$处 处达到局部最小值，则对于在 $x x^*$附 附近的任意小的扰动 $\ \Delta x$   ，都有** $$f(x^*) \leq f(x^*+\Delta x) $$——————————————————————————————————————
**泰勒展开**
由导数和微分的概念，我们知道了：如果函数 $f f$在 在 $x x_0$可 可导，则有 $$f(x) = f(x_0)+f'(x_0)(x-x_0) + o(x-x_0) $$
即在 $x x_0$附 附近，用一次多项式 $f f(x_0)+f'(x_0)(x-x_0)$逼 逼近函数 $f f(x)$时 时，其误差为 $( (x-x_0)$的 的高阶无穷小
但在很多场合，取一次多项式逼近是不够的，往往需要用二次或高于二次的多项式取逼近并要求误差为 $o o((x-x_0)^n)$， ，其中n为多项式的次数。
为此我们考察任一n次多项式 $$p_n(x) = a_0+a_1(x-x_0)+a_2(x-x_0)^2+\dots+a_n(x-x_0)^n $$
逐次求他在 $x x_0$处 处的各阶导数，得到 $p p_n(x_0) = a_0$， ， $p p_n'(x_0) = a_1$， ， $p p_n''(x_0) = 2!a_2$， ， $p p_n^{(n)}(x_0) = n! a_n$

则有 $$f(x) = f(x_0)+f'(x_0)(x-x_0) +\frac{1}{2}f''(x_0)(x-x_0)^2 +\frac{1}{3!}f''(x_0)(x-x_0)^3+\dots+\frac{1}{n!}f^{(n)}(x_0)(x-x_0)^n $$
(泰勒展开)
——————————————————————————————————————
假设函数 $f f(x)$二 二阶连续可导。可将上式右边在 $x x^*$处 处进行二阶"泰勒展开"
 $$f(x^*+\Delta x) = f(x^*)+f'(x^*)\Delta x +\frac{1}{2}f''(x^* + \theta \Delta x)\Delta x^2 $$
其中， $0 0<\theta<1$   。则 $f f(x^*) \leq f(x^*+\Delta x)$可 可转化为
 $$f'(x^*)\Delta x+\frac{1}{2!}f''(x^*+\theta \Delta x)(\Delta x)^2\geq 0 $$
上式对任意小的 $\ \Delta x$都 都成立


如果 $\ \Delta x>0$， ，在上式两边同除以  $\ \Delta x$   ，并且求右极限（ $\ \Delta x \to 0^+$) )
可得
 $$\lim_{\Delta x \to o^+}[f'(x^*)+\frac{1}{2}f''(x^*+\theta \Delta x)(\Delta x)] = f'(x^*)\geq 0 $$

反之，如果 $\ \Delta x< 0$   。在上边两式同除以 $\ \Delta x$， ，并求左极限（ $\ \Delta x\to 0^-$) )
可得 $$\lim_{\Delta x \to 0^-}[f'(x^*) +\frac{1}{2}f''(x^*+\theta \Delta x)\Delta x] = f'(x^*)\leq 0 $$
综合以上两个不等式可知，
最小化问题的必要条件为 $f f'(x^*) = 0$( (由最小化问题可以推得 $f f'(x^*) = 0$) )。**一阶要求**
将此一阶条件代入基本不等式可得
 $$f''(x^* +\theta \Delta x)(\Delta x)^2\geq 0 $$
在上式中，由于 $( (\Delta x)^2 \geq 0$。 。故 $f f''(x^*+\theta \Delta x)\geq 0$
由于此式对于任意小的\Delta x都成立，而二阶导数f''(x)假设为连续函数，故最小化的二阶条件要求：
 $$f''(x^*)\geq 0 $$
根据同样的逻辑，“严格局部最小值”的充分条件：一阶条件 $f f'(x^*) = 0$， ，二阶条件 $f f''(x^*) \geq0$。 。(两个条件缺一不可。 )
如果此一阶条件和二阶条件 $f f''(x^*)>0$均 均成立，则 $f f'(x^*)\Delta x+\frac{1}{2!}f''(x^*+\theta \Delta x)(\Delta x)^2> 0$可 可以去掉等号


同理可证，最大化的二阶条件 $f f''(x^*)\leq 0$。 。“严格局部最大值”的充分条件包括：一阶条件 $f f'(x^*) = 0$， ，二阶条件 $f f''(x^*) <0$


#### 多元最优化


更一般的，考虑以下无约束的多元最小化问题
 $$\min_Xf(X) = f(x_1,x_2,\dots ,x_n) $$
其中， $X X = (x_1,x_2 ,\dots , x_n)^T$

根据最小化问题的一阶条件要求，在最小值 $X X^*$处 处，梯度向量等于**0**
即 $$\nabla f(X^*) = \frac{\partial f(X^*)}{\partial X} = 
\left[
\begin{matrix}
\frac{\partial f(X^*)}{\partial x_1} \\
\frac{\partial f(X^*)}{\partial x_2} \\
\dots \\
\frac{\partial f(X^*)}{\partial x_n} \\
\end{matrix}
\right]
=0 $$
证明： 
假设函数 $f f(X)$   在 $X X^* = （x_1^* ,x_2^* ,\dots ,x_n^*)^T$达 达到最小值，则一元函数 $f f(x_1,x_2^*,\dots , x_n^*)$在 在 $x x_1 = x_1^*$
处达到最小值。故根据一元函数的一阶条件可得， $\ \frac{\partial f(X^*)}{\partial x_1} = \frac{\partial f(x_1^*,x_2^*,\dots,x_3^*)}{\partial x_1} = 0$

由此可知，在最优值 $X X^*$处 处，所有的偏导数均等于0（多元最小化的一阶条件于此相同)
 $$\frac{\partial f(X^*)}{\partial x_1}=\frac{\partial f(X^*)}{\partial x_2}=\dots=\frac{\partial f(X^*)}{\partial x_n} = 0 $$
`多元最大化的一阶条件于此相同`


**二阶条件：**

假设 $f f(X)$在 在 $X X^*$达 达到局部最小值，则对于在 $X X^*$附 附近任意小的扰动 $\ \Delta X$， ，都有 $$f(X^*) \leq f(X^*+h\Delta X) $$
其中， $h h$为 为任意小的正数， $\ \Delta X = (\Delta x_1,\Delta x_2,\dots ,\Delta x_n)^T$， ，为 $n n$维 维空间 $R R^n$的 的一个方向，而 $\ \Delta x_j$   为对x_j的任意小的扰动， $j j = 1,\dots , n$

**假设函数 $f f(X)$二 二阶连续可导**，可将上式右边的 $X X^*$处 处进行二阶泰勒展开
矩阵的二阶求导需要用到**黑塞矩阵**


`黑塞矩阵和雅各比矩阵`
——————————————————————
**黑塞矩阵**
`黑塞矩阵为多元函数 $f f(X)$的 的所有二阶偏导数排成顶一个矩阵`

 $$\nabla^2f(X) = H(X) = \frac{\partial ^2f(X)}{\partial X^2} = \frac{\partial^2f(X)}{\partial X\partial X^T} = \frac{\partial (\frac{\partial f(X)}{\partial X})}{\partial X^T}= $$
 $$
\left(
\begin{matrix}
\frac{\partial ^2 f(X)}{\partial x_1^2} \ \frac{\partial ^2 f(X)}{\partial x_1 \partial x_2} \ \dots \ \frac{\partial ^2 f(X)}{\partial x_1\partial x_n} \\
\frac{\partial ^2 f(X)}{\partial x_2\partial x_1}\  \frac{\partial ^2 f(X)}{\partial x_2^2} \ \dots \ \frac{\partial ^2 f(X)}{\partial x_2\partial x_n} \\
\dots \ \dots \ \dots\  \dots \\
\frac{\partial ^2 f(X)}{\partial x_n\partial x_1} \ \frac{\partial ^2 f(X)}{\partial x_n\partial x_2} \ \dots \ \frac{\partial ^2 f(X)}{\partial x_n^2}
\end{matrix}
\right) $$
其中， $\ \frac{\partial (\frac{\partial f(X)}{\partial X})}{\partial X^T}$表 表示对梯度(列)向量的每个分量分别求导并排成一行（分母为 $\ \partial X^T$) )
由于混合偏导数与求导顺序无关，故黑塞矩阵为对称矩阵。

**雅各比矩阵**

`有时我们需要同时考虑多个响应变量，参考复合函数的向量微分规则` $\ \frac{\partial y}{\partial Z} = (\frac{\partial X}{\partial Z})^T\frac{\partial f}{\partial X}$） ）

 $$\frac{\partial y}{\partial Z} = 
\left(
\begin{matrix}
\frac{\partial y}{\partial z_1} \\
\frac{\partial y}{\partial z_2} \\
\dots \\
\frac{\partial y}{\partial z_n}
\end{matrix}
\right) = 
\left(
\begin{matrix}
\frac{\partial x_1}{\partial z_1} \dots \frac{\partial x_n}{\partial z_1}\\
\frac{\partial x_1}{\partial z_2} \dots \frac{\partial x_n}{\partial z_2} \\
\dots\ \dots \ \dots\\
\frac{\partial x_1}{\partial z_k} \dots \frac{\partial x_n}{\partial z_k}
\end{matrix}
\right
)
\left(
\begin{matrix}
\frac{\partial f}{\partial x_1}\\
\frac{\partial f}{\partial x_2}\\
\dots \\
\frac{\partial f}{\partial x_n}
\end{matrix}
\right)
 =(\frac{\partial X}{\partial Z})^T\frac{\partial f}{\partial X} $$

 $\ \left(\begin{matrix}\frac{\partial x_1}{\partial z_1} \dots \frac{\partial x_n}{\partial z_1}\\\frac{\partial x_1}{\partial z_2} \dots \frac{\partial x_n}{\partial z_2} \\\dots\ \dots \ \dots\\\frac{\partial x_1}{\partial z_k} \dots \frac{\partial x_n}{\partial z_k}\end{matrix}\right)$为 为雅各比矩阵 $\ \frac{\partial X}{\partial Z}$的 的转置


 ——————————————————————



 $$f(X^*+h\Delta X)=f(X^*)+h\frac{\partial f(X^*)}{\partial X} +\frac{1}{2}h^2(\Delta X)^T\frac{\partial ^2 f(X^*+\theta h \Delta X)}{\partial X^2}(\Delta X) $$
 $( (\Delta X)^T\frac{\partial ^2 f(X^*+\theta h \Delta X)}{\partial X^2}(\Delta X)$为 为二次型

其中， $0 0<\theta <1$， ，而二次型 $( (\Delta X)^T\frac{\partial ^2 f(X^*+\theta h \Delta X)}{\partial X^2}(\Delta X)$的 的矩阵为黑塞矩阵 $\ \frac{\partial ^2 f(X^*+\theta h \Delta X)}{\partial X^2}$在 在 $( (X^*+\theta h \Delta X)$处 处的取值
 $f f(X^*) \leq f(X^*+h\Delta X)$可 可转化为 $h h\frac{\partial f(X^*)}{\partial X} +\frac{1}{2}h^2(\Delta X)^T\frac{\partial ^2 f(X^*+\theta h \Delta X)}{\partial X^2}(\Delta X)\geq 0$

在上式中，代入一阶条件 $\ \frac{\partial f(X^*)}{\partial X} = 0$， ，并消去 $\ \frac{1}{2}h^2$可 可得
 $$(\Delta X)^T\frac{\partial ^2 f(X^*+\theta h \Delta X)}{\partial X^2}(\Delta X)\geq 0 $$
则黑塞矩阵 $\ \frac{\partial ^2 f(X^*+\theta h \Delta X)}{\partial X^2}$为 为半正定矩阵。

几何上，这要求在 $X X^*$处 处，函数 $f f(X)$为 为凸函数。
反之，对于最大值问题，其二阶条件则要求黑塞矩阵为半负定，在几何上则要求 $f f(X)$在 在X $^ ^*$处 处为凹函数

### 条件极值
#### 条件极值问题：等式约束 拉格朗日乘子从二元到多元

有时我们还会遇到如下“约束极值”，比如 $$\min_{x_1,x_2}f(x_1,x_2) \ ,\ s.t. \ g(x_1,x_2) =b $$
其中，" $s s.t.$" "表示"subject to"，即“可行解”
收到非线性等式" $g g(x_1,x_2) = b$" "的约束

求解方法之一是“消元法”，即根据约束条件，将x_2写成x_1的函数，然后代入目标函数，将其变成无约束的一元极值问题。
在一定条件下，上述约束条件定义了一个隐函数 $x x_2 = h(x_1)$

对约束条件" $g g(x_1,x_2) =b$" "进行全微分可得（这里的 $g g(x_1,x_2)$为 为定值，可参考圆方程） $$dg=\frac{\partial g}{\partial x_1} dx_1+ \frac{\partial g}{\partial x_2} = 0 $$
由此可得隐函数 $x x_2 = h(x_1)$的 的导数
 $$\frac{dh}{dx_1} = \frac{dx_2}{dx_1} = -\frac{\partial g / \partial x_1}{\partial g/\partial x_2} $$
将隐函数x_2 = h(x_1)代入目标函数，可将此二元约束极值问题转化为一元无约束极值问题
 $$\min_{x_1} F(x_1) = f(x_1,h(x_1)) $$
根据无约束极值的一阶条件可得
 $$\frac{dF(x_1)}{dx_1} = \frac{\partial f}{\partial x_1}+\frac{\partial f}{\partial x_2}\frac{dh}{d x_1} = 0 $$
将 $\ \frac{dh}{dx_1} = \frac{dx_2}{dx_1} = -\frac{\partial g / \partial x_1}{\partial g/\partial x_2}$代 代入得
 $$\frac{dF(x_1)}{dx_1} = \frac{\partial f}{\partial x_1} - (\frac{\partial f/\partial x_2}{\partial g / \partial x_2}) \frac{\partial g}{\partial x_1} =0 $$

则有 $$\frac{\partial f /\partial x_1}{\partial f/\partial x_2} = \frac{\partial g /\partial x_1}{\partial g/ \partial x_2} $$
则有 $\ \partial f /\partial x_1 = K\partial g/\partial x_1,\partial f/\partial x_2 =K\partial g /\partial x_2$
即 $$\partial f/\partial x_1 -K\partial g /\partial x_1 =0, \
\partial f/\partial x_2 -K\partial g /\partial x_2 =0 $$
且有
 $$K = \frac{\partial f/\partial x_2}{\partial g / \partial x_2} $$
则条件极值问题的一阶条件为 $$\frac{\partial f}{\partial x_j} - \lambda \frac{\partial g}{\partial x_j}=0 (j = 1,2) $$


可通过定义“拉格朗日函数”来得到上述一阶条件
 $$\min_{x_1,x_2,\lambda}L(x_1,x_2,\lambda) = f(x_1,x_2)+\lambda[b-g(x_1,x_2)] $$
其中 $\ \lambda$为 为“拉格朗日乘子”，也是优化的变量。
 $$\left\{

\begin{aligned}
L_{x_1}(x_1,x_2,\lambda ) =\partial f/\partial x_1 -K\partial g /\partial x_1 =0 \\
\
L_{x_2}(x_1,x_2,\lambda ) =\partial f/\partial x_2 -K\partial g /\partial x_2 =0 \\
g(x_1,x_2) = b
\end{aligned}

\right\} $$

由此，可讲条件极值看作 $L L(x_1,x_2,\lambda)$的 的无条件优化问题，其一阶条件为

 $$\frac{\partial L}{\partial x_j} = \frac{\partial f(X^*)}{\partial x_j}-\lambda^* \frac{\partial g(X^*)}{\partial x_j} =0(j = 1,2) $$
 $$\frac{\partial L}{\partial \lambda} = b - g(x_1^*,x_2^*) = 0 $$
此方程正式原来的约束条件 $g g(x_1,x_2) = b$


这意味着，在最优解 $X X^*$处 处，目标函数f(X)的梯度向量与约束条件 $g g(X)$的 的梯度平行（二者可能方向相同或者相反），二者仅相差一个倍数 $\ \lambda^*$
即 $$
\left(
\begin{matrix}
\frac{\partial f(X^*)}{\partial x_1}\\
\frac{\partial f(X^*)}{\partial x_2}\\
\end{matrix}
\right)=
\lambda^*
\left(
\begin{matrix}
\frac{\partial g(X^*)}{\partial x_1}\\
\frac{\partial g(X^*)}{\partial x_2}\\
\end{matrix}
\right) $$
**几何意义**：由于梯度向量与水平集(等值线)正交，故在最优解 $X X^*$处 处，目标函数 $f f(X)$的 的等值线正好与函数 $g g(X)$的 的等值线 $g g(X)=b$相 相切



拉格朗日乘子 $\ \lambda$有 有何意义？
显然最优解 $( (x_1^*,x_2^*,x_n^*)$为 为参数 $b b$的 的函数可写为

 $$x_1^* = x_1(b) ,\ x_2^* = x_1(b),\ \lambda^* = \lambda(b) $$
将上式代入拉格朗日函数可得

 $$L(b) = f(x_1(b),x_2(b)) + \lambda(b) [b -g(x_1(b),x_2(b))] $$
上式对b求导数可得
 $$\frac{\partial L(b)}{\partial b} = \frac{\partial f}{\partial x_1}\frac{\partial x_1}{\partial b} +\frac{\partial f}{\partial x_2}\frac{\partial x_2}{\partial b} +\lambda'(b)[b-g(x_1(b),x_2(b))] +\lambda^*[1-\frac{\partial g}{\partial x_1}\frac{\partial x_1}{\partial b} -\frac{\partial g}{\partial x_2}\frac{\partial x_2}{\partial b}] $$
由于 $b b-g(x_1,x_2) =0$
则 $$\frac{\partial L(b)}{\partial b} = (\frac{\partial f}{\partial x_1}\frac{\partial x_1}{\partial b} -\lambda^*\frac{\partial g}{\partial x_1}\frac{\partial x_1}{\partial b})+(\frac{\partial f}{\partial x_2}\frac{\partial x_2}{\partial b} -\lambda^*\frac{\partial g}{\partial x_2}\frac{\partial x_2}{\partial b})+\lambda^* $$
由一阶条件 $\ \frac{\partial f}{\partial x_1} -\lambda^* \frac{\partial g}{\partial x_1} =0$， ， $\ \frac{\partial f}{\partial x_2} -\lambda^* \frac{\partial g}{\partial x_2} =0$

则 $( (\frac{\partial f}{\partial x_1}\frac{\partial x_1}{\partial b} -\lambda^*\frac{\partial g}{\partial x_1}\frac{\partial x_1}{\partial b})=0$， ， $( (\frac{\partial f}{\partial x_2}\frac{\partial x_2}{\partial b} -\lambda^*\frac{\partial g}{\partial x_2}\frac{\partial x_2}{\partial b}) =0$

由此，在最优解处， $L L(b) = f(x_1(b),x_2(b))$
且 $\ \frac{\partial L(b)}{\partial b} = \frac{\partial f(x_1(b),x_2(b))}{\partial b}= \lambda^*$
故最优拉格朗日乘子 $\ \lambda ^* =\lambda(b)$， ，等于放松约束条件 $b b$对 对目标函数最优值 $f f(x_1(b),x_2(b))$的 的边际作用

如果将约束条件“ $g g(x_1,x_2） = b$” ”视作资源约束(可用资源总量为b)，则在经济学上可将 $\ \lambda^* = \lambda (b)$解 解释为资源的“影子价格”，反应此资源的重要性或者价值




考虑收到 $m m$个 个非线性等式约束的 $n n$元 元最小化问题： $$\min_X f(X) = f(x_1,x_2,\dots,x_n) $$
 $$s.t. \ g_1(X) = b_1,\dots , g_m(X) = b_m $$
此时由于有m个约束条件，故须在拉格朗日函数中引入m个拉格朗日乘子（ $\ \lambda_1,\dots ,\lambda_m$） ）:
 $$\min_{X,\lambda}L(X,\lambda_1,\dots,\lambda_m) = f(X) + \sum_{k =1}^m \lambda_k[b_k -g_k(X)] $$
相应的一阶条件为 $$\frac{\partial L(X,\lambda_1,\dots,\lambda_m)}{\partial X} = \frac{\partial f(X)}{\partial X} - \sum_{k = 1}^m\lambda_k \frac{\partial g_k(X)}{\partial X} = 0 $$
 $$\frac{\partial L(X,\lambda_1,\dots,\lambda_m)}{\partial \lambda_k} = b_k - g_k(X) =0 (k=1,2,\dots,m) $$
更简洁的可以定义

 $$\lambda = 
\left(
\begin{matrix}
\lambda_1  \\
\lambda_2 \\
\dots \\
\lambda_m
\end{matrix}
\right) 
 , \ b = 
 \left(
 \begin{matrix}
 b_1 \\ 
b_2 \\
\dots \\
b_m
 \end{matrix}
 \right)
 ,\ g(X) =
 \left(
 \begin{matrix}
 g_1(X) \\
 g_2(X) \\
 \dots \\
 g_m(X)
 \end{matrix}
 \right)
   $$
 则拉格朗日函数可写为
  $$\min_{X,\lambda} L(X,\lambda) = f(X) +\lambda^T[b-g(X)] $$
 使用向量微分规则，一阶条件可写为 $$\frac{\partial L(X,\lambda)}{\partial X} = \frac{\partial f(X)}{\partial X} - [\frac{\partial g(X)}{\partial X}]^T\lambda =0 $$
  $$\frac{\partial L(X,\lambda)}{\partial \lambda} =b- g(X) =0 $$
 其中，方程的 $\ \frac{\partial g(X)}{\partial X}$为 为 $g g(X)$的 的雅各比矩阵，并使用了复合函数的向量微分规则。

最优解 $( (X^*,\lambda^*)$包 包含 $( (n+m)$个 个变量，须同时满足![fix](//images/images/QQ_1731085355976.png)
成立，有 $( (n+m)$个 个方程


在几何上，一阶条件意味着，目标函数的梯度向量 $\ \frac{\partial f(X)}{\partial X}$为 为各约束条件梯度向量 $\ \frac{\partial g_k(X)}{\partial X}$的 的线性组合 $\ \frac{\partial f(X)}{\partial X} = [\frac{\partial g(X)}{\partial X}]^T\lambda$   
**雅各比矩阵参考**

![fix](//images/images/QQ_1731085557833.png)
组合权重即为相应的拉格朗日乘子 $\ \lambda_k$
 $$\frac{\partial f(X)}{\partial X} = (\frac{\partial g_1(X)}{\partial X}\dots\frac{\partial g_m(X)}{\partial X})
\left(
\begin{matrix}
\lambda_1 \\
\dots \\
\lambda_m
\end{matrix}
\right 
)
 = \sum_{k=1}^m \lambda_k\frac{\partial g_k(X)}{\partial X} $$
 拉格朗日乘子向量 $\ \lambda$   可以解释为资源约束的影子价格

例如，拉格朗日乘子 $\ \lambda_1$   可解释为放松约束条件  $g g_1(X) = b_1$对 对目标函数最优值的边际作用，以此类推



二阶条件则要求，在最小值 $X X^*$处 处，目标函数 $f f(X)$的 的黑塞矩阵 $\ \frac{\partial ^2 f(X^*)}{\partial X^2}$   在约束极 $\ \{ X: g(X) =b\}$中 中半正定。`（二次型大于零得到黑塞矩阵半正定）`

如果不限制在约束极 $\ \{  X:g(X)= b\}$内 内，则黑塞矩阵也可以是不定的。例如考虑以下目标函数
 $$y = f(X) = x_1^2 -x_2^2 $$
此函数的几何形状为鞍形。此函数是不定的![fix](//images/images/QQ_1731086634942.png)
函数 $y y = x_1^2 - x_2^2$是 是不定的，故并无最大值和最小值

如果加上约束条件 $x x_2 = 0$   ，则函数 $y y = x_1^2 -x_2^2 = x_1^2$
故在约束集 $\ \{(x_1,x_2 :x_2 =0)\}$中 中正定，并在 $x x_1 =0$处 处达到最小值。

反之，如果加上约束条件  $x x_1 = 0$， ，则函数  $y y = x_1^2 -x_2^2 = - x_2^2$， ，故在约束集 $\ \{ (x_1,x_2) : x_1 =0 \}$
中负定，并在 $x x_2 =0$   处取得最大值


则对于函数 $y y = x_1^2 -x_2^2$， ，原点 $( (0,0)$为 为其"鞍点"，在此鞍点 $( (0,0)$处 处，沿着 $x x_1$   方向，函数 $y y = x_1^2 -x_2^2$在 在鞍点达到最小值，而沿着 $x x_2$的 的方向，则函数 $y y = x_1^2 - x_2^2$在 在鞍点达到最大值。（无约束）


**在一定条件下**，可以证明约束极值问题的最优解 $( (X^*,\lambda^*)$， ，正是拉格朗日函数 $L L(X,\lambda)$的 的鞍点。具体而言，在鞍点 $( (X^*,\lambda^*)$处 处，沿着 $X X$的 的方向，拉格朗日函数 $L L(X,\lambda)$在 在 $X X^*$处 处达到最大值；而沿着 $\ \lambda$方 方向，则拉格朗日函数 $L L(X,\lambda)$在 在 $\ \lambda^*$处 处达到最小值



#### 条件极值问题：非负约束 内点解与边界解


在有些优化问题中，要求优化变量x只能取非负值。此时，最
小化问题可写为

 $$\min_X f(X) = f(x_1,\dots,x_n) $$
 $$s.t. \ x \geq 0 $$
对目标函数在最优解 $X X^*$处 处进行二阶泰勒展开，依然可以得到相同的基本不等式

 $$h\frac{\partial f(X^*)}{\partial X}\Delta X +\frac{1}{2}h^2(\Delta X)^T\frac{\partial ^2 f(X^*+\theta h \Delta X)}{\partial X^2}(\Delta X)\geq 0 $$
如果 $X X^*$为 为内点解（满足s.t.），即 $X X^*>0$， ，则上式对于任意方向的 $\ \Delta X$都 都成立。
故一阶条件依然要求梯度向量为0
即 $\ \frac{\partial f(X^*)}{\partial X} =0$


如果最优解 $X X^*$   的某个分量 $x x_j^*$发 发生于边界，即 $x x_j^* =0$， ，其他的分量都满足 $x x_i \geq 0$， ，满足约束条件  $X X\geq 0$。 。假设其他分量都变动均为0 ，即  $\ \Delta x_i = 0 (i \neq j)$， ，则上式基本不等式可以写成。
 $$h\frac{\partial f(X^*)}{\partial x_j}\Delta x_j +\frac{1}{2}h^2\frac{\partial ^2 f(X^*+\theta h \Delta X)}{\partial x_j^2}(\Delta x_j)^2\geq 0 $$
其中， $\ \Delta X = (0,\dots , 0 ,\Delta x_j , 0 ,\dots ,0)$

上式两边同除以 $\ \Delta x_j > 0$   ，并让  $\ \Delta x_j \to 0 ^+$， ，可得
 $$\frac{\partial f(X^*)}{\partial x_j} \geq 0  $$
对于内点解 $x x_j^* > 0$   ，一阶条件为 $\ \frac{\partial f(X^*)}{\partial x_j} =0$； ；对于边界解 x_j^* = 0，则一阶条件为 $\ \frac{\partial f(X^*)}{\partial x_j}\geq0$
![fix](//images/images/QQ_1731121745040.png)
综上所述，要么 $\ \frac{\partial f(X^*)}{\partial x_j}=0$（ （内点解），要么  $x x_j^*=0$（ （边界解）
故二者的乘积必然为0
 $\ \frac{\partial f(X^*)}{\partial x_j}*x_j^*=0$

上式称为“互补松弛条件”，它意味着如果 $x x_j^*>0$， ，则不等式 $\ \frac{\partial f(X^*)}{\partial x_j}\geq0$必 必须取等号， $\ \frac{\partial f(X^*)}{\partial x_j} = 0$
反之，如果 $\ \frac{\partial f(X^*)}{\partial x_j}>0$， ，则不等式 $x x_j^*=0$


由于方程 $\ \frac{\partial f(X^*)}{\partial x_j}*x_j^*=0$对 对于 $j j = 1 ,\dots ,n$均 均成立，加总这n个方程可得

 $$\sum_{j=1}^n\frac{\partial f(X^*)}{\partial x_j} *x_j^* = [\frac{\partial f(X^*)}{\partial X}]^T X^* =0 $$
因此对于非负约束的极值问题，其一阶条件包括以下(2n+1)个方程。
 $$\frac{\partial f(X^*)}{\partial X}\geq0 \ ,\ [\frac{\partial f(X^*)}{\partial X}]^T X^* =0 \ , \ X^* \geq 0 $$
 $[ [\frac{\partial f(X^*)}{\partial X}]^T$是 是一个向量矩阵。
#### 条件极值问题：不等式约束

考虑以下更一般的不等式约束极值问题：

 $$\min_X f(X) = f(x_1,x_2,\dots,x_n) $$
 $$s.t. g(X)\leq b, \ X\geq 0 $$
其中， $g g(X) = (g_1(X) \dots ,g_m(X))^T$   ， $b b = (b_1,\dots,b_n)^T$
求解方法之一是，引入m 个“松弛变量”
 $s s = (s_1,\dots,s_m)^T$
 $$s = b - g(X) = (s_1,\dots ,s_m)^T $$
讲上式代入目标函数，可将不等式约束变为等式约束
 $$s.t. g(X)+s = b , X \geq 0 , s \geq 0 $$




### 最优化算法


**机器学习的最优化问题通常没有解析解，故一般需使用迭代算法来逼近最优解**
对于最小化问题 $\ \min_X f(X)$， ，迭代算法的最一般公式是 $$
X_{t+1} = X_t -\Delta X_t $$
其中， $X X_t$为 为第t 步迭代的取值， $X X_{t+1}$为 为第t+1步迭代的取值，而 $\ \Delta X_t$为 为该步迭代的变动幅度。

变化幅度 $\ \Delta X_t$可 可写成梯度向量的线性函数
 $$X_{t+1} = X_t - \eta \nabla f(X_t) $$
 $\ \eta$为 为学习率。当 $\ \eta>0$函 函数的变动方向为负梯度向量，即 $X X_{t+1}-X_t =  - \eta \nabla f(X_t)$

很小的学习率 $\ \eta$   ，可防止沿着负梯度方向走的太远。另一方面，学习率太小，则迭代收敛的速度可能很慢。
使用公式反复迭代，直至梯度向量等于0(或十分接近于0)，即可停止迭代，认为达到局部最小值。

在标准的梯度下降法中，步长 $\ \eta$一 一般是固定的。
![fix](//images/images/QQ_1732009691266.png)
(通过梯度下降法寻找函数 $z z = x^2 +2y^2$的 的最小值的示意图)
这里介绍另一种方法
**最速下降法**——————————————
 $$\eta_t = \underset{\eta}{\text{argmin}} \ f \left( \mathbf{x}_t - \eta \nabla f \left( \mathbf{x}_t \right) \right) $$
从 $X X_t$出 出发，沿着 $- -\eta \nabla f(X_i)$的 的直线方向，搜索能使得 $f f(X_i - \eta \nabla f(X_i))$最 最小化的 $\ \eta$值 值。由于在进行梯度下降时，选择最优的步长 $\ \eta_t$， ，使得函数的下降幅度最大，故此法称为 最速下降法。


牛顿法
 $$\mathbf{x}_{t + 1} = \mathbf{x}_t - \eta[\mathbf{H}(\mathbf{x}_t)]^{-1}\nabla f(\mathbf{x}_t) $$

即在迭代算法的最一般公式中，如果令 $A A_t = [H(X_t)]^{-1}$， ，就可以得到牛顿法的公式

推导过程如下。

将 $f f(X)$在 在 $X X_t$处 处进行二阶泰勒展开，并忽略高阶项可得
 $$ f(\mathbf{x}) \approx f(\mathbf{x}_t)+[\nabla f(\mathbf{x}_t)]' (\mathbf{x}-\mathbf{x}_t)+\frac{1}{2}(\mathbf{x}-\mathbf{x}_t)'\mathbf{H}(\mathbf{x}_t)(\mathbf{x}-\mathbf{x}_t)  $$
