
本章我们将从最简单的模型之一——线性回归模型，开始介绍两种非常不同的训练模型的方法：
- 通过“闭式”方程，直接计算出最拟合训练集的模型参数（也就是使训练集上的成本函数最小化的模型参数）。
- 使用迭代优化的方法，即梯度下降（GD），逐渐调整模型参数直至训练集上的成本函数调至最低，最终趋同于第一种方法计算出来的模型参数。我们还会研究几个梯度下降的变体，包括批量梯度下降、小批量梯度下降以及随机梯度下降。等我们进入到第二部分神经网络的学习时，会频繁地使用这几个的变体。



## 线性回归

生活满意度的回归模型：$$life-satisfaction = \theta_0 +\theta_1 \times GDP-per-capita$$
这个模型就是输入特征`GDP_per_capita`的线性函数，$\theta _0$ 和 $\theta _1$是模型的参数。

更为概括地说，线性模型就是对输入特征加权求和，再加上一个我们称为偏置项（也称为截距项）的常数，以此进行预测。
线性回归模型预测公式如下：
$$\hat y = \theta _0 +\theta _1 x_1 +\theta_2x_2 +\dots +\theta_nx_n$$
在此等式中：
- $\hat y$是预测值。
- $n$是特征数量。
- $x_i$是第i个特征值。
- $\theta _j$是第j个模型参数（从0开始，$\theta_0$也算）


也可以使用向量化的形式更简洁地表示
$$\hat y = h_{\theta}(x) = \theta \times x$$

在此等式中：
- $\theta$是模型的参数向量，其中包含偏差项$\theta_0$和特征权重$\theta_1$至
n。
- $x$是实例的特征向量，包含从$x_0$到$x_n$，$x_0$始终等于1。
- $\theta \times x$是向量$\theta$和$x$的点积，它当然等于$\theta _0 x_0 +\theta_1 x_1 +\theta_2 x_2+\dots +\theta_n x_n$
- $h_\theta$是假设函数，使用模型参数$\theta$。

在机器学习中，向量通常表示为列向量，是有单一列的二维数组。如果$\theta$和$x$为列向量，则预测为 ，其中$\theta ^T$为$\theta$（行向量而不是列向量）的转置，且$\theta ^T$ $x$为$\theta ^T$和$x$的矩阵乘积。当然这是相同的预测，除了现在是以单一矩阵表示而不是一个标量值。


这就是线性回归模型，我们该怎样训练线性回归模型呢？回想一下，训练模型就是设置模型参数直到模型最拟合训练集的过程。为此，我们首先需要知道怎么测量模型对训练数据的拟合程度是好还是差。我们了解到回归模型最常见的性能指标是均方根误差（RMSE）。

因此，在训练线性回归模型时，你需要找到最小化RMSE的$\theta$值。在实践中，将均方误差（MSE）最小化比最小化RMSE更为简单，二者效果相同（因为使函数最小化的值，同样也使其平方根最小）
**MSE公式如下**
$$MSE = (X,h_\theta) = \frac{1}{m}\sum^m_{i=1}(\theta^Tx^{(i)} - y^{(i)})^2$$
$y^{(i)}$表示第i个真实值。

### 标准方程

为了得到使成本函数最小的θ值，有一个闭式解方法——也就是一个直接得出结果的数学方程，即标准方程。
**公式：标准方程**$$\hat \theta = (X^TX)^{-1}X^Ty$$
这个方程中：

- $\hat \theta$是使成本函数最小的θ值。

- $y$是包含$y^{(i)}$到$y^{(m)}$的目标值向量。

```python
import numpy as np
import matplotlib.pyplot as plt
X = 2*np.random.rand(100,1) #生成一个形状为(100 ,1)的数组，从[0,2)中随机取值
y = 4+3*X +np.random.randn(100,1) 
```

`np.random.rand(a,b) ` 生成一个在`[0,1)`上随机取值，形状为(a,b)的二维均匀分布数组
`np.random.randn(a,b)`  生成一个均值为0，标准差为1 ，形状为(a,b)的二维正态分布数组

画图查看分布情况

```python
plt.plot(X,y,"r.")
plt.xlabel("$x_1$",fontsize=18)
plt.ylabel("$y$",fontsize = 18)
plt.axis([0,2,0,13])
plt.savefig("generated_data_plot")
plt.show()
```
结果如下
![fix](/images/QQ_1725080864315.png)
现在我们使用标准方程来计算$\hat \theta$。使用Numpy 的线性代数模块$(np.linalg)$中的$inv()$函数来对矩阵求逆，并使用$dot()$方法计算矩阵内积
```python
X_b = np.c_[np.ones((100,1)),X] 
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
```
$$\hat \theta = (X^TX)^{-1}X^Ty$$
我们实际用来生成数据的函数是$y=4+3x_1+$高斯噪声。
我们要求的是$\theta_0$ 和$\theta_1$，$\theta_0$的系数是1，故需要添加一个1的矩阵。
看看结果如何

```python
array([[4.21509616],
       [2.77011339]])
```
我们期待的是$\theta_0 = 4$和$\theta_1 = 3$得到的是$\theta_0=4.215$，$\theta_1=2.770$。非常
接近，噪声的存在使其不可能完全还原为原本的函数。

现在可以用$\hat \theta$作出预测。
```python
X_new = np.array([[0],[2]])
X_new_b = np.c_[np.ones((2,1)),X_new]
y_predict = X_new_b.dot(theta_best)
y_predict
```
结果如下：
```python
array([[4.21509616]
 [9.75532293]])
```
绘制模型的预测结果：
```python
plt.plot(X_new,y_predict,"r-")
plt.plot(X,y,"b.")
plt.axis([0,2,0,15])
plt.show()
```
结果如下：
![fix](/images/QQ_1725082201305.png)


使用Scikit-Learn执行线性回归也很简单

```python
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X,y)
lin_reg.intercept_ #截距 
lin_reg.coef_ #系数
#(array([4.21509616]), array([[2.77011339]]))
```
LinearRegression类基于`scipy.linalg.lstsq（）`函数（名称代表“最小二乘”），你可以直接调用它：

```python
theta_best_svd ,residuals,rank, s = np.linalg.lstsq(X_b,y,rcond = 1e-6)
theta_best_svd
```

函数的返回值是一个元组，包含以下四个元素：

- `theta_best_svd`：这是最小二乘解，即模型参数的最优估计。在线性回归的上下文中，这些参数对应于自变量（包括截距项，如果添加了的话）的系数。
- `residuals`：这是残差数组，即实际观测值与模型预测值之间的差异。
- `rank`：这是设计矩阵`X_b`的数值秩。它反映了矩阵中独立（非零）的行（或列）的数量。
- `s`：这是奇异值的数组。这些值是从设计矩阵的奇异值分解中得到的，可以用来评估数值稳定性和模型的复杂度。
结果如下：
```python
array([[4.21509616],
       [2.77011339]])
```


此函数计算$\hat \theta = X^+ y$，其中$X^+$是$X$的伪逆（具体来说是MoorePenrose逆）。你可以使用$np.linalg.pinv（）$来直接计算这个伪逆：
```python
np.linalg.pinv(X_b).dot(y)
```

伪逆：$$X^+ = (X^TX)^{-1}X^T$$
## 梯度下降
梯度下降是一种非常通用的优化算法，能够为大范围的问题找到最优解。梯度下降的中心思想就是迭代地调整参数从而使成本函数最小化。

假设你迷失在山上的浓雾之中，你能感觉到的只有你脚下路面的坡度。快速到达山脚的一个策略就是沿着最陡的方向下坡。这就是梯度下降的做法：通过测量参数向量θ相关的误差函数的局部梯度，并不断沿着降低梯度的方向调整，直到梯度降为0，到达最小值！

具体来说，首先使用一个随机的θ值（这被称为随机初始化），然后逐步改进，每次踏出一步，每一步都尝试降低一点成本函数（如MSE），直到算法收敛出一个最小值
![fix](/images/QQ_1725083252944.png)

在梯度下降的描述中，模型参数被随机初始化并反复调整使成本函数最小化。学习步长与成本函数的斜率成正比，因此，当参数接近最小值时，步长逐渐变小

梯度下降中一个重要参数是每一步的步长，这取决于超参数学习率。如果学习率太低，算法需要经过大量迭代才能收敛，这将耗费很长时间。

反过来说，如果学习率太高，那你可能会越过山谷直接到达另一边，甚至有可能比之前的起点还要高。这会导致算法发散，值越来越大，最后无法找到好的解决方案

线性回归模型的MSE成本函数是个二次函数，所以连接曲线上任意两点的线段永远不会跟曲线相交。也就是说，不存在局部最小值，只有一个全局最小值。它同时也是一个连续函数，所以斜率不会产生陡峭的变化。这两点保证的结论是：即便是乱走，梯度下降都可以趋近到全局最小值（只要等待时间足够长，学习率也不是太高）

### 批量梯度下降
要实现梯度下降，你需要计算每个模型关于参数$\theta_j$的成本函数的梯度。换言之，你需要计算的是如果只改变$\theta_j$，成本函数会改变多少。这被称为偏导数。记作$$\frac{\partial}{\partial \theta_j}MSE(\theta)$$
成本函数的偏导数：*参照二次函数求导*$$\frac{\partial}{\partial \theta_j}MSE(\theta) = \frac{2}{m}\sum^m_{i=1}(\theta^Tx^{(i)}-y^{(i)})x^{(i)}_j$$

梯度向量记作$$\nabla_ \theta MSE(\theta) = 
\left(
\begin{matrix}
\frac{\partial}{\partial\theta_0}MSE(\theta)\\
\frac{\partial}{\partial\theta_1}MSE(\theta)\\
\frac{\partial}{\partial\theta_2}MSE(\theta)\\
\dots \\
\frac{\partial}{\partial\theta_n}MSE(\theta) 
\end{matrix}
\right
)
=\frac{2}{m}X^T(\theta^TX-y)
$$，包含所有成本函数（每个模型参数一个）的偏导数。

一旦有了梯度向量，哪个点向上，就朝反方向下坡。$\nabla_ \theta MSE(\theta)$。这时学习率$\eta$就发挥作用了：用梯度向量乘以$\eta$确定下坡步长的大小

公式：$$
\theta = \theta - \eta \nabla_\theta MSE(\theta)$$
```python
eta = 0.1 
n_iterations = 1000
m = 100

theta = np.random.randn(2,1) #initialization

for iteration in range(n_iterations):
	gradients = 2/m *X_b.T.dot(X_b.dot(theta)-y)
	theta = theta -eta*gradients
```

让我们看一下产生的theta：
```python
theta
```
结果如下
```python
array([[4.21509616],
       [2.77011339]])
```
来看看预测结果如何：

```python
y_predict = X_new_b.dot(theta)
y_predict

```

结果如下
```python
array([[4.21509616],
       [9.75532293]])
```

```python

def plot_gradient_descent(theta ,eta):
	m = len(X_b)
	plt.plot(X,y,"b.")
	n_iterations = 1000
	for iteration in range(n_iterations):
		if iteration <10:
			y_predict = X_new_b.dot(theta)
			style = "b-" if iteration >0 else "r--" #第一次为红色 其余为蓝色
			plt.plot(X_new , y_predict ,style)
		gradients = 2/m*X_b.T.dot(X_b,dot(theta)-y)
		theta = theta - eta*gradients 
		
	plt.xlabel("$x_1$",fontsize = 18)
	plt.axis([0,2,0,15])
	plt.title(r"$\eta = {}$".format(eta),fontsize =16)
```
```python
np.random.seed(42)
theta = np.random.randn(2,1)

plt.figure = (figsize = (10,4))
plt.subplot(131);plt_gradient_descent(theta ,eta = 0.02)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.subplot(132);plt_gradient_descent(theta ,eta = 0.1)
plt.subplot(133);plt_gradient_descent(theta ,eta = 0.5)

plt.savefig("gradient_descent_plot")
plt.show()

```
![fix](/images/QQ_1725088862729.png)

### 随机梯度下降 [随机序列求偏导]

批量梯度下降的主要问题是它要用整个训练集来计算每一步的梯度，所以训练集很大时，算法会特别慢。与之相反的极端是随机梯度下降，每一步在训练集中随机选择一个实例，并且仅基于该单个实例来计算梯度。显然，这让算法变得快多了，因为每次迭代都只需要操作少量的数据。它也可以被用来训练海量的数据集，因为每次迭代只需要在内存中运行一个实例即可。

另一方面，由于算法的随机性质，它比批量梯度下降要不规则得多。成本函数将不再是缓缓降低直到抵达最小值，而是不断上上下下，但是从整体来看，还是在慢慢下降。随着时间的推移，最终会非常接近最小值，但是即使它到达了最小值，依旧还会持续反弹，永远不会停止。所以算法停下来的参数值肯定是足够好的，但不是最优的。
![fix](/images/QQ_1725086180985.png)
代码实现：
```python
n_epochs = 50
t0 ,t1 = 5,50 
def learning_schedule(t):
	returen t0 / (t+t1)

theta = np.random.randn(2,1)

for epoch in range(n_epochs):
	for i in range(m):
		random_index = np.random.randint(m) #随机序列求导
		xi = X_b[random_index:random_index+1]
		yi = y[random_index:random_index+1]
		gradients = 2*xi.T.dot(xi.dot(theta)-yi)
		eta = learning_schedule(epoch*m+i)
		theta = theta -eta*gradients
```
按照惯例，我们进行m个回合的迭代。每个回合称为一个轮次。
```python
theta
```

结果如下
```python
array([[4.21509296],
       [2.75615066]])
```
此代码仅在训练集中遍历了50次，并达到了一个很好的解决方案：


我们来看看图示
```python
n_epochs = 50
t0, t1 = 5,50
def learning_schedule(t):
	return t0/(t+t1)
theta = np.random.randn(2,1)

for epoch in range(n_epochs):
    for i in range(m):
        if epoch ==0 and i<20:
            y_predict = X_new_b.dot(theta)
            style = "b-" if i>0 else "r--"
            plt.plot(X_new ,y_predict,style)
            
            
        random_index = np.random.randint(m)
        xi = X_b[random_index :random_index+1]
        yi = y[random_index :random_index+1]
        gradients =2*xi.T.dot(xi.dot(theta)-yi)
        eta = learning_schedule(epoch * m +i) #逐步减小
        theta = theta - eta*gradients
	
plt.plot(X,y,"b.")
plt.xlabel("$x_1$",fontsize = 18)
plt.ylabel("$y$",rotation=0,fontsize = 18)
plt.axis([0,2,0,15])
plt.savefig("sgd_plot")
plt.show()
```
结果如下：
![fix](/images/QQ_1725091468981.png)
请注意，由于实例是随机选取的，因此某些实例可能每个轮次中被选取几次，而其他实例则可能根本不被选取。如果要确保算法在每个轮次都遍历每个实例，则另一种方法是对训练集进行混洗（确保同时对输入特征和标签进行混洗），然后逐个实例进行遍历，然后对其进行再次混洗，以此类推。但是，这种方法通常收敛较慢。

要使用带有Scikit-Learn的随机梯度下降执行线性回归，可以使用SGDRegressor类，该类默认优化平方误差成本函数。以下代码最多可运行1000个轮次，或者直到一个轮次期间损失下降小于0.001为止`（max_iter=1000，tol=1e-3）`。它使用默认的学习调度（与前一个学习调度不同）以0.1（$eta_0=0.1$）的学习率开始。最后，它不使用任何正则化
```python
from sklearn.linear_model import SGDRegressor
sgd_reg = SGDRegressor(max_iter=1000 , tol=1e-3,penalty = None ,eta=0.1)
sgd_reg.fit(X,y.ravel()) #将y平铺成一维数组

sgd_reg.intercept_
sgd_reg.coef_
```
结果如下：
```python
(array([4.24365286]), array([2.8250878]))
```
### 小批量梯度下降

在每一步中，不是根据完整的训练集（如批量梯度下降）或仅基于一个实例（如随机梯度下降）来计算梯度，小批量梯度下降在称为小型批量的随机实例集上计算梯度。小批量梯度下降优于随机梯度下降的主要优点是，你可以通过矩阵操作的硬件优化来提高性能，特别是在使用GPU时。
**代码实现**
```python
n_iterations = 50
minibatch_size = 20

np.random.seed(42)
theta = np.random.randn(2,1) #random initialization

t0 ,t1 = 200,1000
def learning_schedule(t):
	return t0 /(t+t1)

t= 0 
for epoch in range(n_iterations):
    shuffled_indices = np.random.permutation(m) #打乱
    X_b_shuffled = X_b[shuffled_indices]
    y_shuffled = y[shuffled_indices]
    for i in range(0,m,minibatch_size):
	    t += 1 # eta 递减
	    xi = X_b_shuffled[i:i+minibatch_size]
	    yi = y_shuffled[i:i+minibatch_size]
	    gradients = 2/minibatch_size*xi.T.dot(xi.dot(theta) - yi)
	    eta = learning_schedule(t)
	    theta = theta - eta*gradients
	    theta_path_mgd.append(theta)
print(theta)
```
![fix](/images/QQ_1725102453753.png)

## 多项式回归

以上的都是线性模型，如果数据比直线更加复杂该如何呢？一个简单的方法就是将每个特征的幂次方添加为一个新特征，然后在此扩展特征集上训练一个线性模型。这种技术称为多项式回归。

让我们看一个示例。首先，让我们基于一个简单的二次方程式（注：二次方程的形式为$y=ax^2+bx+c$。）（加上一些噪声）生成一些非线性数据。
$$y = \frac{1}{2}x^2 +x+2$$

```python
m =100
X = 6*np.random.rand(m,1) #(-3,3)随机取样 组成一个数组
y = 0.5*X**2 +X+2+np.random.randn(m,1) #添加噪声
```
查看分布情况
```python
plt.plot(X,y,"b.")
plt.xlabel("$x_1$",fontsize = 18)
plt.ylabel("$y$",rotation = 0,fontsize = 18)
plt.axis([-3,3,0,10])
plt.savefig("quadratic_data_plot")
plt.show()
```
![fix](/images/QQ_1725166463757.png)

显然，一条直线永远无法正确地拟合此数据。因此，让我们使用Scikit-Learn的PolynomialFeatures类来转换训练数据，将训练集中每个特征的平方（二次多项式）添加为新特征（在这种情况下，只有一个特征）：
```python
from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree = 2,include_bias = False)
X_poly = poly_features.fit_transform(X)
X[:5]
X_poly[:5]
```
`include_bias = False`的作用是不显示0次项。
结果如下：
```python
array([[-0.75275929,  0.56664654],
       [ 2.70428584,  7.3131619 ],
       [ 1.39196365,  1.93756281],
       [ 0.59195091,  0.35040587],
       [-2.06388816,  4.25963433]])
```
X_poly现在包含X的原始特征以及该特征的平方。现在，你可以将LinearRegression模型拟合到此扩展训练数据中。
```python
from sklearn.Linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X_poly,y)
lin_reg.intercept_,lin_reg.coef_
```

训练模型 取截距和系数 结果如下：
```python
(array([1.78134581]), array([[0.93366893, 0.56456263]]))
```
画图：
```python
import matplotlib.pyplot as plt
X_new = np.linspace(-3,3,100).reshape(100,1)
poly_features = PolynomialFeatures(degree = 2,include_bias = False)
X_new_poly = poly_features.fit_transform(X_new)
y_new = lin_reg.predict(X_new_poly)
plt.plot(X,y,"b.")
plt.plot(X_new,y_new,"r-",linewidth = 2,label = "Predictions")
plt.xlabel("$x_1$",fontsize = 18)
plt.ylabel("$y$",rotation = 0,fontsize = 18)
plt.legend(loc = "upper left",fontsize = 14)
plt.axis([-3,3,0,10])
plt.savefig("quadratic_predictions_plot")
plt.show()
```
![fix](/images/QQ_1725270464764.png)
模型估算$\hat y = 0.56x_1^2 +0.93x_1 +1.78$，而实际上的原始函数为$y = 0.5x_1^2 +1.0x_1 +2.0$


请注意，当存在多个特征时，多项式回归能够找到特征之间的关系（这是普通线性回归模型无法做到的）。PolynomialFeatures还可以将特征的所有组合添加到给定的多项式阶数。例如，如果有两个特征a和b，则degree=3的PolynomialFeatures不仅会添加特征a2、a3、b2和b3，还会添加组合ab、a2b和ab2。


## 学习曲线
你如果执行高阶多项式回归，与普通线性回归相比，拟合数据可能会更好。。请注意300阶多项式模型是如何摆动以尽可能接近训练实例的。


```python
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

for style ,width degree in (("g-",1,300),("b--",2,2),("r-+",2,1)):
	polybig_features = PolynomialFeatures(degree=degree,include_bias = False)
	std_scaler = StandardScaler()
	lin_reg = LinearRegression()
	polynomial_regression = Pipeline([
	("poly_features" ,polybig_features),
	("std_scaler",std_scaler),
	("lin_reg",lin_reg),])
	polynomial_regression.fit(X,y)
	y_newbig = polynomial_regression.predict(X_new)
	plt.plot(X_new , y_newbig , style , label = str(degree),linewidth = width)

plt.plot(X,y,"b.")
plt.legend(loc = "upper left",fontsize =16)
plt.xlabel("$x_1$",fontsize = 18)
plt.ylabel("$y$",rotation = 0,fontsize = 18)
plt.axis([-3,3,0,10])
plt.savefig("high_degree_polynomials_poly")
plt.show()
```
![fix](/images/QQ_1725271631369.png)
这种高阶多项式回归模型严重过拟合训练数据，而线性模型则欠拟合。在这种情况下，最能泛化的模型是二次模型，因为数据是使用二次模型生成的。

但是总的来说，你不知道数据由什么函数生成，那么如何确定模型的复杂性呢？你如何判断模型是过拟合数据还是欠拟合数据呢?

如果模型在训练数据上表现良好，但根据交叉验证的指标泛化较差，则你的模型过拟合。如果两者的表现均不理想，则说明欠拟合。这是一种区别模型是否过于简单或过于复杂的方法。

还有一种方法是观察学习曲线：这个曲线绘制的是模型在训练集和验证集上关于训练集大小（或训练迭代）的性能函数。要生成这个曲线，只需要在不同大小的训练子集上多次训练模型即可。下面这段代码在给定训练集下定义了一个函数，绘制模型的学习曲线：

```python
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

def plot_learning_curves(model,X,y):
	X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.2)
	train_errors ,val_errors = [] ,[]
	for m in range(1,len(X_train)):
		model.fit(X_train[:m],y_train[:m])
		y_train_predict = model.predict(X_train[:m])
		y_val_predict = model.predict(X_val)
		train_errors.append(mean_squared_error(y_train[:m],y_train_predict))
		val_errors.append(mean_squared_error(y_val,y_val_predict))
	plt.plot(np.sqrt(train_errors),"r-+",linewidth = 2,label = "train")
	plt.plot(np.sqrt(val_errors),"b-",linewidth = 3,label = "val")
	plt.legend(loc = "upper right",fontsize = 14)
	plt.xlabel("Training set size",fontsize = 14)
	plt.ylabel("RMSE",fontsize = 14)
```

导入线性回归模型（一条直线）
```python
lin_reg = LinearRegression()
plot_learning_curves(lin_reg,X,y)
plt.axis([0,80,0,3])
plt.savefig("underfitting_learning_curves_plot")
plt.show()
```
需要注意的是，val_errors 中的每个元素都是在模型使用不同大小的训练集（`X_train[:m]`）进行训练后，对完整验证集 X_val 进行预测并计算得到的 MSE，因此每次得到的数据都有不同。
![fix](/images/QQ_1725278013043.png)
这种欠拟合的模型值得解释一下。首先，让我们看一下在训练数据上的性能：当训练集中只有一个或两个实例时，模型可以很好地拟合它们，这就是曲线从零开始的原因。但是，随着将新实例添加到训练集中，模型就不可能完美地拟合训练数据，这既因为数据有噪声，又因为它根本不是线性的。因此，训练数据上的误差会一直上升，直到达到平稳状态，此时在训练集中添加新实例并不会使平均误差变好或变差。

当在很少的训练实例上训练模型时，它无法正确泛化，这就是验证误差最初很大的原因。然后，随
着模型经历更多的训练示例，它开始学习，因此验证错误逐渐降低。但是，直线不能很好地对数据进行建模，因此误差最终达到一个平稳的状态，非常接近另外一条曲线。

这些学习曲线是典型的欠拟合模型。两条曲线都达到了平稳状态。它们很接近而且很高。

现在让我们看一下在相同数据上的10阶多项式模型的学习曲线。

```python
from sklearn.pipeline import Pipeline

polynomial_regression = Pipeline([
								  ("poly_features",PolynomialFeatures(degree = 10,include_bias = False)),
								  ("lin_reg",LinearRegression(),)
])

plot_learning_curves(polunomial_regression,X,y)
plt.axis([0,80,0,3])
plt.savefig("learning_curves_plot")
plt.show()
```
![fix](/images/QQ_1725278373911.png)

这些学习曲线看起来有点像以前的曲线，但是有两个非常重要的区别：
- 与线性回归模型相比，训练数据上的误差要低得多。

- 曲线之间存在间隙。这意味着该模型在训练数据上的性能要比在验证数据上的性能好得多，这是过拟合模型的标志。但是，如果你使用更大的训练集，则两条曲线会继续接近。


统计学和机器学习的重要理论成果是以下事实：模型的泛化误差可以表示为三个非常不同的误差之和：
**偏差**
这部分泛化误差的原因在于错误的假设，比如假设数据是线性的，而实际上是二次的。高偏差模型最有可能欠拟合训练数据。

**方差**
这部分是由于模型对训练数据的细微变化过于敏感。具有许多自由度的模型（例如高阶多项式模型）可能具有较高的方差，因此可能过拟合训练数据。


**不可避免的误差**
这部分误差是因为数据本身的噪声所致。减少这部分误差的唯一方法就是清理数据（例如修复数据源（如损坏的传感器），或者检测并移除异常值）。

增加模型的复杂度通常会显著提升模型的方差并减少偏差。反过来，降低模型的复杂度则会提升模型的偏差并降低方差。这就是为什么称其为权衡。

## 正则化线性模型

减少过拟合的一个好方法是对模型进行正则化（即约束模型）：它拥有的自由度越少，则过拟合数据的难度就越大。正则化多项式模型的一种简单方法是减少多项式的次数。

对于线性模型，正则化通常是通过约束模型的权重来实现的。现在，我们看一下岭回归、Lasso回归和弹性网络，它们实现了三种限制权重的方法。

### 岭回归

岭回归（也称为Tikhonov正则化）是线性回归的正则化版本：将等于$\alpha \sum^n_{i=1} \theta_i ^2$的正则化项添加到成本函数。这迫使学习算法不仅拟合数据，而且还使模型权重尽可能小。注意仅在训练期间将正则化项添加到成本函数中。训练完模型后，你要使用非正则化的性能度量来评估模型的性能。

训练过程中使用的成本函数与用于测试的性能指标不同是很常见的。除正则化外，它们可能不同的另一个原因是好的训练成本函数应该具有对优化友好的导数，而用于测试的性能指标应尽可能接近最终目标。例如，通常使用成本函数（例如对数损失（稍后讨论））来训练分类器，但使用精度/召回率对其进行评估。

超参数α控制要对模型进行正则化的程度。如果$\alpha$=0，则岭回归仅是线性回归。如果$\alpha$非常大，则所有权重最终都非常接近于零，结果是一条经过数据均值的平线。

**岭回归成本函数**$$J(\theta) = MSE(\theta)+\alpha \frac{1}{2}\sum^n_{i=1}\theta^2_i$$
在执行岭回归之前缩放数据（例如使用StandardScaler）很重要，因为它对输入特征的缩放敏感。大多数正则化模型都需要如此。

```python
np.random.seed(42)
m = 20
X = 3*np.random.rand(m,1)
y = 1+0.5*X +np.random.randn(m,1) / 1.5
X_new = np.linspace(0,3,100).reshape(100,1)
```

```python
from sklearn.linear_model import Ridge
ridge_reg = Ridge(alpha = 1,solver = "cholesky",random_state = 42)
ridge_reg.fit(X,y)
ridge_reg.predict([[1.5]])
```

