SVM的基本思想可以用一些图来说明。两个类可以轻松地被一条直线（它们是线性可分离的）分开。左图显示了三种可能的线性分类器的决策边界。其中虚线所代表的模型表现非常糟糕，无法正确实现分类。
其余两个模型分类效果不错，但是它们的决策边界与实例过于接近，如果出现面对新实例，不会太好。相比之下，右图中的实线代表SVM分类器的决策边界，这条线不仅分离了两个类，并且尽可能远离了最近的训练实例。
你可以将SVM分类器视为在类之间拟合可能的**最宽的街道**（平行的虚线所示）。因此这也叫作**大间隔分类。**
![[QQ_1728193653663.png]]
要注意的，是在两条虚线所夹空间之外的地方增加更多训练实例不会对决策边界产生影响，也就是说它完全由位于虚线边缘的实例所决定。这些实例被称为**支持向量**。
（支持向量：虚线边缘的实例）

SVM对特征的缩放非常敏感。如图，在左图中，垂直刻度比水平刻度大得多，因此可能的最宽的街道接近于水平。在特征缩放（例如使用Scikit-Learn的StandardScaler 高斯分布）后，决策边界看起来好了很多（见右图）。
![[QQ_1728196983386.png]]
### 软间隔分类


如果我们严格地让所有实例都位于正确的一边，这就是硬间隔分类。硬间隔分类有两个主要问题。
1. 它只在数据是线性可分离的时候才有效
2. 它对异常值非常敏感。
![[QQ_1728197141809.png]]
左图的数据根本找不出硬间隔；而右最终显示结果也不尽人意，无法很好地泛化。

要尽可能在保持街道宽阔和限制间隔违例（即位于街道之上，甚至在错误的一边的实例）之间找到良好的平衡，**这就是软间隔分类**。

使用Scikit-Learn创建SVM模型时，我们可以指定许多超参数。C是这些超参数之一。左右两图分别为c值较高和较低时的图![[QQ_1728197622009.png]]
(注意看图的大小并未发生变化。)

左图的间隔冲突很糟糕，通常最好要少一些。但是左侧的模型存在很多间隔违例的情况，但泛化效果可能会更好。


$$\sum_{t = 1}^ T$$