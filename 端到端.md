
学习机器学习最好使用真实数据进行实验，而不仅仅是人工数据。
本章我们从StatLib库中选择了加州住房价格的数据集。该数据集基于1990年加州人口普查的数据。
## 数据可视化的操作
```python

import os
import tarfile
import urllib
#导入库

HOUSING_PATH = os.path.join("datasets", "housing")
#文件路径

```
第一步需要导入文件。由于我们已经下载了文件，所以下载文件的步骤就省略。
```python

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
 
    
       
housing = load_housing_data()
housing.head()
#文件读取并输出前五项

housing.info() #简要概括数据信息
housing["ocean_proximity"].value_counts() #查看有多少种分类存在，每种类别下分别有多少个区域
housing.describe() # 显示数值属性的摘要

```

**以下为这四种描述方式的图示**

![图片](assets/IMG_1.png)
`housing.head() `

![图片](assets/IMG_2.png)
`housing.info() `

![图片](assets/IMG_3.png)
`housing["ocean_proximity"].value_counts() `

![图片](assets/IMG_4.png)
`housing.describe() `


**另一种快速了解数据类型的方法是绘制每个数值属性的直方图。**


```python

import matplotlib.pyplot as plt
housing.hist(bins=50, figsize=(20,15)) #bins = 50 指定了直方图中使用的箱数
plt.savefig(HOUSING_PATH) #figsize=(20, 15)设置图形的尺寸为宽20英寸，高15英寸。
plt.show() 
  
```
  

  ![图片](assets/IMG_5.png)
  
  **以上是数据可视化的操作**

  *接下来要进行下一步操作*
  
## 创建测试集
**分为两种：随机抽样和分层随机抽样**
```python
  import numpy as np
 
def split_train_test(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data)) # 打乱序列 洗牌
    test_set_size = int(len(data) * test_ratio)  # 测试集
    test_indices = shuffled_indices[:test_set_size] 
    train_indices = shuffled_indices[test_set_size:] # 获取索引
    return data.iloc[train_indices], data.iloc[test_indices] #索引
    
```

 
```python

train_set, test_set = split_train_test(housing, 0.2)
len(train_set)  #16512
len(test_set)   #4128
# 调用上一步定义的函数
```
  
这样就可以将原来的数据打乱，得到训练集和测试集。
**不过有一个问题**：由于是随机打乱，所以每一次调用都会得到一个不同的训练集和测试集。
**解决方法**：定义种子
```python
np.random.seed（42）
```
从而让它始终生成相同的随机索引。

**但是这种解决方案在下一次获取更新的数据时都会中断**

**Scikit-Learn**提供了一些函数，可以通过多种方式将数据集分成多个子集。最简单的函数是**train_test_split（）**  

```python

from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)

test_set.head()

```

![图片](assets/IMG_6.png)

**这样，测试集就创建好了。**

```python

housing["median_income"].hist()

```
收入立方图）
![图片](assets/IMG_8.png)

   如果你咨询专家，他们会告诉你，要预测房价中位数，收入中位数是一个非常重要的属性。于是你希望确保在收入属性上，测试集能够代表整个数据集中各种不同类型的收入。由于收入中位数是一个连续的数值属性，所以你得先创建一个收入类别的属性。我们先来看一下收入中位数的直方图（见图2-8）：大多数收入中位数值聚集在1.5～6（15000～60 000美元）左右，但也有一部分远远超过了6万美元。
   
   在数据集中，每一层都要有足够数量的实例，也就是说，你不应该将层数分得太多，每一层应该要足够大才行。下面这段代码是用pd.cut（）来创建5个收入类别属性的（用1～5来做标签），**0～1.5是类别1，1.5～3是类别2**，以此类推：
  
  
```python
housing["income_cat"] = pd.cut(housing["median_income"],

bins=[0., 1.5, 3.0, 4.5, 6., np.inf],

labels=[1, 2, 3, 4, 5])

```
  
`pd.cut()`将`housing[“median_income”]`分割成5栏。`np.inf`表示无穷大。
运行以上代码后，`housing` 数据框将会添加一个新的列 `income_cat`，表示每个 `median_income` 所在的类别。


```python

housing["income_cat"].value_counts()

```
![图片](assets/IMG_9.png)


**查看数据类型**

```python

housing["income_cat"].hist()

```
**重新绘制直方图**

![图片](assets/IMG_10.png)

现在，你可以根据收入类别进行**分层抽样**了。

对于分层抽样的随机取样创建测试集，我们使用**Scikit-Learn**的**StratifiedShuffleSplit**类：

接下来简要介绍一下**StratifiedShuffleSplit** 的使用方法。就以本题为例：
```python

X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1,2]])
Y = np.array([0, 0, 0, 1, 1])
ss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=seed) 

for train_index, test_index in ss.split(X, Y):
    print(train_index, test_index)
   
 结果：
[4 0] [1 2 3]
[1 4] [0 2 3]
[3 1] [0 2 4]
[2 3] [1 4 0]
[3 0] [4 1 2]

    
```

`ss= StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) `

`ss=`类似于`import numpy as np`，相当于设置一个快捷变量来调用函数。接下来介绍参数

`n_splits=5`表示获取五次测试集和训练集

`test_size`表示测试集占整体的比例

`random_state`表示种子

取样的步骤分为以下两步
1. 根据`test_splits`
我们设置了0.5， 说明，每一组测试集和训练集1:1，那么我们就可以先分了，5个数据样本， 1:1的比例，那么得到训练集和测试集的个数2.5和2.5，但是分数怎么行？ 所以测试集会上取整变成了3， 训练集个数变成了2， 所以有2个训练集样本，3个测试集一样， 每一组都是这样算，一共5组（这个书就是`n_splits`控制）
2. **分层抽样**
我们已经有了每一组训练集应该2个，测试集应该3个，那么这个2个和3个应该怎么划分呢？ 记住采用了分层抽样 ，是这样分的， 来看一下类别比例，我们发现，Y里面3个0,2个1，也就是3个负样本，2个正样本，比例3:2。
我们分层抽样的话， 按照这个比例来，训练样本2个，含负样本的个数2 * 3/5=1.2 含正样本的个数2 * 2/5=0.8，四舍五入后，1个负样本，1个正样本。 所以你可以看看，这五组的训练样本，肯定是包含1个正样本，1个负样本。 而测试集那边同理， 3个样本， 负:正=3:2 所以负样本的个数3*3/5=1.8, 正样本的个数3 * 2/5=1.2 四舍五入， 2个负样本，1个正样本。

**回归本题**

```python

from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing["income_cat"]):
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index]
    
```
测试集为整体的`20%`
`for `遍历获取标签索引，代入索引得到数据。

```python

strat_test_set["income_cat"].value_counts() / len(strat_test_set)

housing["income_cat"].value_counts() / len(housing)

```

![图片](assets/IMG_11.png)
`strat_test_set`

![图片](assets/IMG_12.png)
`housing`

可以看到两组数据非常接近。
接下来看看不分层抽样的随机抽样与分层抽样和整体的关系。

```python

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)

```
**（不分层的随机抽样）**

```python

def income_cat_proportions(data):
    return data["income_cat"].value_counts() / len(data)
    
compare_props = pd.DataFrame({
    "Overall": income_cat_proportions(housing),
    "Stratified": income_cat_proportions(strat_test_set),
    "Random": income_cat_proportions(test_set),
}).sort_index()
compare_props["Rand. %error"] = 100 * compare_props["Random"] / compare_props["Overall"] - 100
compare_props["Strat. %error"] = 100 * compare_props["Stratified"] / compare_props["Overall"] - 100

```

这段代码的解读如下：
先假设一个函数`income_cat_proportions()`它返回一个包含原始数据集中各类别比例的序列或数组。

创建一个数据框`compare_props  = pd.DataFrame ({  }).sort_index() `其中包含以上三种包含各类别比例的数据。

而后又增加两组数据`"Rand. %error”`  和`"Strat. %error”`，分别表示与整体相比，随机抽样和分层抽样的差别。

```python

compare_props
```


![图片](assets/IMG_14.png)

正如你所见，分层抽样的测试集中的比例分布与完整数据集中的分布几乎一致，而纯随机抽样的测试集结果则是有偏的。

现在你可以删除income_cat属性，将数据恢复原样了。

```python

for set_ in (strat_train_set, strat_test_set):
    set_.drop("income_cat", axis=1, inplace=True)
    
```

遍历分层抽样的训练集和测试集，删去`income_cat `一列

`axis=1` 表示列 ，`axis=0` 表示行  


到现在为止，我们还只是在快速浏览数据，从而对手头上正在处理的数据类型形成一个大致的了解。本阶段的目标是再深入一点。

首先，把测试集放在一边，你能探索的只有训练集。此外，如果训练集非常庞大，你可以抽样一个探索集，这样后面的操作更简单快捷一些。不过我们这个案例的数据集非常小，完全可以直接在整个训练集上操作。让我们先创建一个副本，这样可以随便尝试而不损害训练集：

```python

housing = strat_train_set.copy()

```
重新定义一个housing




## 将地理数据可视化
```python

housing.plot(kind="scatter", x="longitude", y="latitude")

```

![图片](assets/IMG_15.png)
根据经纬度画点图。

```python

housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1)

```

这里加入参数`alpha = 0.1`，即透明度。让点稀疏的地方颜色浅，点密集的地方颜色深。

![图片](assets/IMG_16.png)
现在，再来看看房价。每个圆的半径大小代表了每个区域的人口数量（选项s），颜色代表价格（选项c）。我们使用一个名叫jet的预定义颜色表（选项cmap）来进行可视化，颜色范围从蓝（低）到红（高）

```python

housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4,
    s=housing["population"]/100, label="population", figsize=(10,7),
    c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True,
    sharex=False)
plt.legend()
```

![图片](assets/IMG_17.png)

**在散点图中**

  `s: `指定散点图点的大小，默认为20，通过新传入的变量，实现气泡图的绘制（size的首字母）
  
  `c: `指定散点图点的颜色，默认为蓝色（color的首字母）
  
  `marker:` 指定散点图点的形状，默认为圆形
  
  `cmap: `指定色图，只有当c参数是一个浮点型的数组时才起作用
  
  `norm: `指定数据亮度， 标准化到0~1之间，使用该参数仍需要c为浮点型的数组
  
  `vmin、vmax: `亮度设置，与norm类似，如果使用了norm则该参数无效
  
  `alpha: `设置散点的透明度
  
  `edgecolors: `设置散点边界线的颜色
  
  `linewidths: `设置散点边界线的粗细
  
  `colorbar:`当我们给图配渐变色时，常常需要在图旁边把colorbar显示出来。
  
  `plt.legend（）`函数主要的作用就是给图加上图例，


接下来看看没有加`cmap=plt.get_cmap("jet”)`的图像

![图片](assets/IMG_18.png)

可见颜色并没有那么鲜明，但是也还是比较直观的。

接下来我们可以将加州的地理图片代入坐标图中。

```python

import matplotlib.image as mpimg
california_img=mpimg.imread(os.path.join(images_path, filename))
ax = housing.plot(kind="scatter", x="longitude", y="latitude", figsize=(10,7),
                       s=housing['population']/100, label="Population",
                       c="median_house_value", cmap=plt.get_cmap("jet"),
                       colorbar=False, alpha=0.4,
                      )
plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,
           cmap=plt.get_cmap("jet"))
plt.ylabel("Latitude", fontsize=14)
plt.xlabel("Longitude", fontsize=14)

prices = housing["median_house_value"]
tick_values = np.linspace(prices.min(), prices.max(), 11)
cbar = plt.colorbar(ticks=tick_values/prices.max())
cbar.ax.set_yticklabels(["$%dk"%(round(v/1000)) for v in tick_values], fontsize=14)
cbar.set_label('Median House Value', fontsize=16)

plt.legend(fontsize=16)
plt.show()

```

![图片](assets/IMG_23.png)

代码解读如下：

`mpimg.imread(...):` 使用` matplotlib.image `模块的` imread `函数读取该路径下的图像文件，返回图像的数据（通常是一个数组）。

`os.path.join(images_path, filename): `生成图像文件的完整路径。`images_path `是存放图像的文件夹路径，`filename` 是图像文件的名称。

`california_img:` 将读取到的图像数据存储在变量` california_img `中，后续可以用于显示或处理。



表明你房价与地理位置（例如靠近海）和人口密度息息相关，这一点你可能早已知晓。一个通常很有用的方法是使用聚类算法来检测主集群，然后再为各个集群中心添加一个新的衡量邻近距离的特征。海洋邻近度可能就是一个很有用的属性，不过在北加州，沿海地区的房价并不是太高，所以这个简单的规则也不是万能的。

## 寻找相关性  

现在看看每个属性与房价中位数的相关性分别是多少:

我们使用`corr()`函数来求相关系数，具体步骤如下：

1.  `.corr()` 方法计算这些数值列之间的相关性矩阵。
2.  `corr_matrix["median_house_value"]` 提取与 "median\_house\_value" 列的相关性数据。
3.  `.sort_values(ascending=False)` 将相关性值按降序排序，以便查看与` "median_house_value" `的相关性最强的变量。

```python
numeric_cols = housing.select_dtypes(include=['float64', 'int64']).columns  
 #选择数据类型为浮点数的栏目
corr_matrix = housing[numeric_cols].corr()  
corr_matrix["median_house_value"].sort_values(ascending=False)

```
结果如下

![图片](assets/IMG_19.png)

相关系数的范围从-1变化到1。越接近1，表示有越强的正相关。例如，当收入中位数上升时，房价中位数也趋于上升。当系数接近于-1时，表示有较强的负相关。我们可以看到纬度和房价中位数之间呈现出轻微的负相关（也就是说，越往北走，房价倾向于下降）。最后，系数靠近0则说明二者之间没有线性相关性。


相关系数仅测量线性相关性（“如果x上升，则y上升/下降”）。所以它有可能彻底遗漏非线性相关性（例如“如果x接近于0，则y会上升”）。

还有一种方法可以检测属性之间的相关性，就是使用`pandas`的`scatter_matrix`函数，它会绘制出每个数值属性相对于其他数值属性的相关性。现在我们有11个数值属性，可以得到 

$$11^2=121$$ 

个图像，篇幅原因无法完全展示，这里我们仅关注那些与房价中位数属性最相关的，可算作是最有潜力的属性。


```python
 from pandas.plotting import scatter_matrix
attributes = ["median_house_value", "median_income", "total_rooms",
 "housing_median_age"]
scatter_matrix(housing[attributes], figsize=(12, 8))

```

如果pandas绘制每个变量对自身的图像，那么主对角线（从左上到右下）将全都是直线，这样毫无意义。所以取而代之的方法是，pandas在这几个图中显示了每个属性的直方图

![图片](assets/IMG_20.png)

最有潜力能够预测房价中位数的属性是收入中位数，所以我们放大来看看其相关性的散点图

```python

housing.plot(kind="scatter", x="median_income", y="median_house_value",
 alpha=0.1)
 
```

![图片](assets/IMG_22.png)



此图像存在一些问题。首先，二者的相关性确实很强，你可以清楚地看到上升的趋势，并且点也不是太分散。其次，前面我们提到过50万美元的价格上限在图中是一条清晰的水平线，不过除此之外，图2-16还显示出几条不那么明显的直线：45万美元附近有一条水平线，35万美元附近也有一条，28万美元附近似乎隐约也有一条，再往下可能还有一些。为了避免你的算法学习之后重现这些怪异数据，你可能会尝试删除这些相应区域。


## 机器学习算法的数据准备


现在，让我们先回到一个干净的训练集（再次复制`strat_train_set`），然后将预测器和标签分开，因为这里我们不一定对它们使用相同的转换方式（需要注意`drop（）`会创建一个数据副本，但是不影响`strat_train_set）`：

```python

housing = strat_train_set.drop("median_house_value", axis=1) # drop labels for training set
housing_labels = strat_train_set["median_house_value"].copy()

```
### 数据清理
```python

sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()
sample_incomplete_rows
```
以上代码可以实现对缺失值的显示

代码解读如下

1. `housing.isnull():` 这个方法会返回一个与数据框同样形状的布尔数据框，标记出每个元素是否为缺失值（NaN）。
2. `.any(axis=1): `这个方法检查每一行中是否有任何缺失值。如果某一行有缺失值，结果为True，否则为False。
3. `housing[housing.isnull().any(axis=1)]: `这会选择出所有包含缺失值的行。
4. `.head(): `这个方法返回选中的前几行（默认是5行）。

大部分的机器学习算法无法在缺失的特征上工作，所以我们要创建一些函数来辅助它。前面我们已经注意到total\_bedrooms属性有部分值缺失，所以我们要解决它。有以下三种选择：
1. 放弃这些相应的区域。
2. 放弃整个属性。
3. 将缺失的值设置为某个值（0、平均数或者中位数等）。

通过`DataFrame`的`dropna（）`、`drop（）`和`fillna（）`方法，可以轻松完成这些操作：

```python
housing.dropna(subset=["total_bedrooms"])    # option 1
```

`.dropna(subset=["total_bedrooms"]) `删除指定行

`subset`: 这个参数指定了需要关注的列。只有这些列中的缺失值会被考虑。
 
`[“total_bedrooms”]`: 这是一个包含列名的列表，表示关注的列为`“total_bedrooms”`。


```python
housing.drop("total_bedrooms", axis=1)       # option 2
```

`.drop("total_bedrooms", axis=1)`: 这个方法会删除指定的列（这里是`“total_bedrooms”`）。`axis=1`表示操作的是列，`axis=0`则表示操作的是行。

```python

median = housing["total_bedrooms"].median() # option 3
housing["total_bedrooms"].fillna(median, inplace=True)
```

此处代码出现问题，会显示报错。

![图片](assets/IMG_25.png)

原因如下：

这个警告信息的核心问题是涉及“链式赋值（chained assignment）”的问题。具体来说：

1. 链式赋值：你正在通过链式赋值的方式（例如 housing["total_bedrooms"].fillna(median, inplace=True)）修改一个DataFrame或Series。这可能会导致你实际上是在对一个副本进行操作，而不是对原始对象进行修改。

2. 未来版本的变化：在未来的Pandas 3.0版本中，inplace=True的用法将不再有效，因此你当前的代码可能在将来会导致错误。

所以我们可以将原代码修改为
```python

housing.fillna({ "total_bedrooms": median }, inplace=True)

```
`.fillna():` 这是一个方法，用于填补数据框（DataFrame）或序列（Series）中的缺失（NaN）。



[整体缺失值替换]如果需要计算出训练集的中位数值，然后用它填充训练集中的缺失值，但也别忘了保存这个计算出来的中位数值，因为后面可能需要用到。当重新评估系统时，你需要更换测试集中的缺失值；或者在系统上线时，需要使用新数据替代缺失值。`Scikit-Learn`提供了一个非常容易上手的类来处理缺失值：`SimpleImputer`。使用方法如下：首先，你需要创建一个`SimpleImputer`实例，指定你要用属性的中位数值替换该属性的缺失值：

```python
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="median")
```

由于中位数值只能在数值属性上计算，所以我们需要创建一个没有文本属性`ocean_proximity`的数据副本：

```python
housing_num = housing.drop("ocean_proximity", axis=1)
```
使用`fit()`方法将`imputer`实例适配到训练数据：即获取`housing_num`中的数据


```python
imputer.fit(housing_num)
```
这里imputer仅仅只是计算了每个属性的中位数值，并将结果存储在其实例变量statistics_中。虽然只有total_bedrooms这个属性存在缺失值，但是我们无法确认系统启动之后新数据中是否一定不存在任何缺失值，所以稳妥起见，还是将imputer应用于所有的数值属性
![[QQ_1723774791098.png]]
现在，你可以使用这个“训练有素”的`imputer`将缺失值替换成中位数值从而完成训练集转换：

```python
X = imputer.transform(housing_num)
```
此时变成一个numpy数组。
```python
housing_tr=pd.DataFrame(X,columns=housing_num.columns,index=housing_num.index)
```
## 处理文本和分类属性

到目前为止，我们只处理数值属性，但现在让我们看一下文本属性。在此数据集中，只有一个：`ocean_proximity`属性。我们看看前10个实例的值：

```python
housing_cat = housing[["ocean_proximity"]]
housing_cat.head(10)
```
![[QQ_1723775990479.png]]

它不是任意文本，而是有限个可能的取值，每个值代表一个类别。因此，此属性是分类属性。大多数机器学习算法更喜欢使用数字，因此让我们将这些类别从文本转到数字。为此，我们可以使用`Scikit-Learn`的`OrdinalEncoder`类

`OrdinalEncoder` 是 `sklearn.preprocessing` 模块中的一个工具，用于将分类特征转换为有序的整数值，这在许多机器学习算法中非常有用。
```python
from sklearn.preprocessing import OrdinalEncoder

ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat) #获得numpy数组
housing_cat_encoded[:10] #输入前十项
```
注意事项如下：

1.  `OrdinalEncoder` 会根据每列类别的出现顺序进行编码，因此如果类别的顺序对你的模型有影响，请确保按照适当的顺序提供数据。
2.  如果你有多个特征，可以将数据传入一个二维数组中，`OrdinalEncoder` 会分别处理每一列。
![[QQ_1723776346663.png]]

现在我们来看看 `housing[["ocean_proximity"]]`中有多少类。

`ordinal_encoder.categories_` 是 `OrdinalEncoder` 类的一个属性，用于获取编码器在拟合过程中识别的每个特征的类别列表。
```python
ordinal_encoder.categories_
```

![[QQ_1723776613897.png]]
类似于：
```python
housing["ocean_proximity"].value_counts() #查看有多少种分类存在，每种类别下分别有多少个区域
```
这种表征方式产生的一个问题是，机器学习算法会认为两个相近的值比两个离得较远的值更为相似一些。在某些情况下这是对的（对一些有序类别，像“坏”“平均”“好”“优秀”），但是，对`ocean_proximity`而言情况并非如此（例如，类别0和类别4之间就比类别0和类别1之间的相似度更高）。为了解决这个问题，常见的解决方案是给每个类别创建一个二进制的属性：当类别是`“<1H OCEAN”`时，一个属性为1（其他为0），当类别是`“INLAND”`时，另一个属性为1（其他为0），以此类推。这就是**独热编码**，因为只有一个属性为1（热），其他均为0（冷）。

**独热编码：**`OneHotEncoder`

假设数据中有三种颜色：red、blue 和 green，转换后的结果可能看起来像这样：
```python
[[1. 0. 0.]  
 [0. 1. 0.]  
 [0. 0. 1.]  
 [0. 1. 0.]  
 [1. 0. 0.]]  

```
接下来我们将独热编码应用到`ocean_proximity`中
```python
from sklearn.preprocessing import OneHotEncoder

cat_encoder = OneHotEncoder()
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)#转换和上面一样
housing_cat_1hot
```

注意到这里的输出是一个SciPy稀疏矩阵，而不是一个NumPy数组。

**SciPy稀疏矩阵**：因为在独热编码完成之后，我们会得到一个几千列的矩阵，并且全是0，每行仅有一个1。占用大量内存来存储0是一件非常浪费的事情，因此稀疏矩阵选择仅存储非零元素的位置。

当然如果你实在想把它转换成一个（密集的）NumPy数组，只需要调用`toarray（）`方法即可：
也可以传入参数  `sparse_output`来输出一个Numpy数组
```python
housing_cat_1hot.toarray()#option 1


cat_encoder = OneHotEncoder(sparse_output=False) #option 2
```
结果如下：

```python
array([[1., 0., 0., 0., 0.],

[1., 0., 0., 0., 0.],

[0., 0., 0., 0., 1.],

...,

[0., 1., 0., 0., 0.],

[1., 0., 0., 0., 0.],

[0., 0., 0., 1., 0.]])
```

你可以再次使用编码器的categories_实例变量来得到类别列表：

```python
cat_encoder.categories_
```

结果如下：

```python
[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],
 dtype=object)]
```

如果类别属性具有大量可能的类别（例如，国家代码、专业、物种），那么独热编码会导致大量的输入特征，这可能会减慢训练并降低性能。如果发生这种情况，你可能想要用相关的数字特征代替类别输入。例如，你可以用与海洋的距离来替换ocean_proximity特征（类似地，可以用该国家的人口和人均GDP来代替国家代码）。或者，你可以用可学习的低维向量（称为嵌入）来替换每个类别。每个类别的表征可以在训练期间学习。

### 自定义转换器

**编写自己的转换器**：你当然希望让自己的转换器与`Scikit-Learn`自身的功能（比如流水线）无缝衔接，而由于Scikit-Learn依赖于鸭子类型的编译，而不是继承，所以你所需要的只是创建一个类，然后应用以下三种方法：`fit（）`（返回self）、`transform（）`、`fit_transform（）`。

你可以通过添加`TransformerMixin`作为基类，直接得到最后一种方法。同时，如果添加`BaseEstimator`作为基类（并在构造函数中避免`*args`和`*kargs`），你还能额外获得两种非常有用的自动调整超参数的方法（`get_params（）`和`set_params（）`）。例如，我们前面讨论过的组合属性，这里有个简单的转换器类，用来添加组合后的属性：

```python
from sklearn.base import BaseEstimator, TransformerMixin

# column index 获取序列索引
col_names = "total_rooms", "total_bedrooms", "population", "households"
rooms_ix, bedrooms_ix, population_ix, households_ix = [
    housing.columns.get_loc(c) for c in col_names] 

class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs
        self.add_bedrooms_per_room = add_bedrooms_per_room
    def fit(self, X, y=None):
        return self  # 不做fit处理
    def transform(self, X):
        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]
        population_per_household = X[:, population_ix] / X[:, households_ix]
        if self.add_bedrooms_per_room:
            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
            return np.c_[X, rooms_per_household, population_per_household,
                         bedrooms_per_room]
        else:
            return np.c_[X, rooms_per_household, population_per_household]

attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)
housing_extra_attribs = attr_adder.transform(housing.values)
housing_extra_attribs
#housing.values 将housing转换为numpy数组
#并最后输出housing_extra_attribs
```

以下是代码的详细解释：

1. 初始化方法` (__init__)`:
`add_bedrooms_per_room`: 这是一个布尔参数，默认为 True，决定是否在特征变换中添加“每个房间的卧室数量”这一特征。

2. fit 方法:
该方法是必需的，尽管在这里没有实际的训练或拟合过程，因此它仅仅返回自己 (return self)。通常，fit 方法用于学习模型参数。

3. transform 方法:
这个方法对输入特征进行变换，它接受一个数组 X 作为输入，其中包含多个特征（如房间数量、卧室数量、家庭数量等）。
`rooms_per_household `计算每个家庭的房间数量。
`population_per_household `计算每个家庭的人口数量。
如果 `add_bedrooms_per_room` 为 `True`，则计算 `bedrooms_per_room`（每个房间有多少卧室），并将所有三个新特征与原始特征合并成一个新数组。
如果 `add_bedrooms_per_room` 为 `False`，则只会返回前两个计算出的特征（房间数量和人口数量）。

4. 合并特征:
`np.c_ `是 `NumPy `中用于按列连接数组的函数，这里的功能是把生成的新特征与原始特征结合在一起。

这样，一个转化数据的类就创建好了。我们可以在原数据的基础上获取一些新的数据。在本例中，转换器有一个超参数`add_bedrooms_per_room`默认设置为`True`（提供合理的默认值通常是很有帮助的）。这个超参数可以让你轻松知晓添加这个属性是否有助于机器学习算法。更一般地，如果你对数据准备的步骤没有充分的信心，就可以添加这个超参数来进行把关。

输出的结果如下：
```python
array([[-121.46, 38.52, 29.0, ..., 'INLAND', 5.485835694050992,
        3.168555240793201],
       [-117.23, 33.09, 7.0, ..., 'NEAR OCEAN', 6.927083333333333,
        2.6236979166666665],
       [-119.04, 35.37, 44.0, ..., 'INLAND', 5.3933333333333335,
        2.223333333333333],
       ...,
       [-122.72, 38.44, 48.0, ..., '<1H OCEAN', 4.1104651162790695,
        2.6627906976744184],
       [-122.7, 38.31, 14.0, ..., '<1H OCEAN', 6.297405189620759,
        2.411177644710579],
       [-122.14, 39.97, 27.0, ..., 'INLAND', 5.477157360406092,
        3.1725888324873095]], dtype=object)

```

接下来把数据重新放入原数据框

```python
housing_extra_attribs = pd.DataFrame(
    housing_extra_attribs,
    columns=list(housing.columns)+["rooms_per_household", "population_per_household"],
    index=housing.index) #numpy数组，栏目，索引
housing_extra_attribs.head()
```

![[QQ_1723863832554.png]]
这样，数据数据转换就完成了。

### 转换流水线

许多数据转换的步骤需要以正确的顺序来执行。而Scikit-Learn正好提供了Pipeline类来支持这样的转换。下面是一个数值属性的流水线示例：

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

num_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy="median")),#中位数操作
        ('attribs_adder', CombinedAttributesAdder()),
        ('std_scaler', StandardScaler()),
    ])

housing_num_tr = num_pipeline.fit_transform(housing_num)
```
 从 `sklearn.pipeline` 导入 `Pipeline`，用于创建数据处理的连续步骤，并将一系列连续操作存储于`num_pipeline`中。
![[QQ_1723872387198.png]]
`'imputer', SimpleImputer(strategy="median")`这是上文的中位数填补缺失值的办法。

`CombinedAttributesAdder()`则是上文的**自定义转换器**

`'std_scaler', StandardScaler()` 
作用：去均值和方差归一化。且是针对每一个特征维度来做的，而不是针对样本。 
标准差标准化（standardScale）使得经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：$$x' = \frac{x-\mu}{\sigma}$$
其中μ为所有样本数据的均值，σ为所有样本数据的标准差。

Pipeline构造函数会通过一系列名称/估算器的配对来定义步骤序列。除了最后一个是估算器之外，前面都必须是转换器（也就是说，必须有`fit_transform（）`方法）。至于命名可以随意，你喜欢就好（只要它们是独一无二的，不含双下划线），它们稍后在超参数调整中会有用。当调用流水线的`fit（）`方法时，会在所有转换器上按照顺序依次调用`fit_transform（）`，将一个调用的输出作为参数传递给下一个调用方法，直到传递到最终的估算器，则只会调用`fit（）`方法。流水线的方法与最终的估算器的方法相同。在本例中，最后一个估算器是`StandardScaler`，这是一个转换器，因此流水线有一个`transform（）`方法，可以按顺序将所有的转换应用到数据中（这也是我们用过的`fit_transform（）`方法）。

输出结果如下。
```python
array([[-0.94135046,  1.34743822,  0.02756357, ...,  0.01739526,
         0.00622264, -0.12112176],
       [ 1.17178212, -1.19243966, -1.72201763, ...,  0.56925554,
        -0.04081077, -0.81086696],
       [ 0.26758118, -0.1259716 ,  1.22045984, ..., -0.01802432,
        -0.07537122, -0.33827252],
       ...,
       [-1.5707942 ,  1.31001828,  1.53856552, ..., -0.5092404 ,
        -0.03743619,  0.32286937],
       [-1.56080303,  1.2492109 , -1.1653327 , ...,  0.32814891,
        -0.05915604, -0.45702273],
       [-1.28105026,  2.02567448, -0.13148926, ...,  0.01407228,
         0.00657083, -0.12169672]])
```


到目前为止，我们分别处理了类别列和数值列。拥有一个能够处理所有列的转换器会更方便，将适当的转换应用于每个列。
`Scikit-Learn`为此引入了`ColumnTransformer`，好消息是它与`pandas DataFrames`一起使用时效果很好。
让我们用它来将所有转换应用到房屋数据：

```python
from sklearn.compose import ColumnTransformer

num_attribs = list(housing_num)   #数值
cat_attribs = ["ocean_proximity"] #类别
#以上这两个表示的是column，即行索引。


full_pipeline = ColumnTransformer([
        ("num", num_pipeline, num_attribs), #对数值特征应用流水线转换
        ("cat", OneHotEncoder(), cat_attribs), #对类别特征进行独热编码
    ])

housing_prepared = full_pipeline.fit_transform(housing)
```

这段代码通过 `ColumnTransformer` 实现了对数值和类别特征的同时处理。数值特征使用之前定义的 `num_pipeline` 进行预处理，类别特征则使用 `OneHotEncoder` 进行独热编码。处理后的结果存储在 `housing_prepared` 中，为后续的模型训练做准备。

![[QQ_1723874684833.png]]

结果如下：
```python
array([[-0.94135046,  1.34743822,  0.02756357, ...,  0.        ,
         0.        ,  0.        ],
       [ 1.17178212, -1.19243966, -1.72201763, ...,  0.        ,
         0.        ,  1.        ],
       [ 0.26758118, -0.1259716 ,  1.22045984, ...,  0.        ,
         0.        ,  0.        ],
       ...,
       [-1.5707942 ,  1.31001828,  1.53856552, ...,  0.        ,
         0.        ,  0.        ],
       [-1.56080303,  1.2492109 , -1.1653327 , ...,  0.        ,
         0.        ,  0.        ],
       [-1.28105026,  2.02567448, -0.13148926, ...,  0.        ,
         0.        ,  0.        ]])
```


`OneHotEncoder`返回一个稀疏矩阵，而`num_pipeline`返回一个密集矩阵。当稀疏矩阵和密集矩阵混合在一起时，`ColumnTransformer`会估算最终矩阵的密度（即单元格的非零比率），如果密度低于给定的阈值，则返回一个稀疏矩阵（通过默认值为`sparse_threshold=0.3`）。在此示例中，它返回一个密集矩阵。我们有一个预处理流水线，该流水线可以获取全部房屋数据并对每一列进行适当的转换。

## 选择和训练模型

### 训练和评估训练集

首先，如同我们在第1章所做的，先训练一个**线性回归模型**

```python
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)
```
![[QQ_1723878778977 1.png]]
![[QQ_1723878693490.png]]
1. 导入包
2. 创建实例
3. 训练模型。

现在你有一个可以工作的线性回归模型了。让我们用几个训练集的实例试试：

```python
some_data = housing.iloc[:5]
some_labels = housing_labels.iloc[:5]
some_data_prepared = full_pipeline.transform(some_data)

print("Predictions:", lin_reg.predict(some_data_prepared))
```

从 `housing` DataFrame 中选择前5行（包括第0行）的所有列，并将这部分数据赋值给 `some_data` 变量。

```python
Predictions: [ 85657.90192014 305492.60737488 152056.46122456 186095.70946094
 244550.67966089]
```
让我们再来看看实际的值
```python
print("Labels:", list(some_labels))
```
结果如下：
```python
Labels: [72100.0, 279600.0, 82700.0, 112500.0, 238300.0]
```

我们可以使用`Scikit-Learn的mean_squared_error（）`函数来测量整个训练集上回归模型的RMSE：

```python
from sklearn.metrics import mean_squared_error

housing_predictions = lin_reg.predict(housing_prepared)
lin_mse = mean_squared_error(housing_labels, housing_predictions)
lin_rmse = np.sqrt(lin_mse)
lin_rmse
#68627.87390018745
```

`68627.87390018745`
**欠拟合**：显然这不是一个好看的成绩：大多数区域的`median_housing_values`分布在120 000～265 000美元之间，所以典型的预测误差达到68 628美元只能算是差强人意。

想要**修正欠拟合**，可以通过选择更强大的模型，或为算法训练提供更好的特征，又或者减少对模型的限制等方法。首先，让我们尝试一个更复杂的模型，看看它到底是怎么工作的。

**决策树**：`DecisionTreeRegressor`这是一个非常强大的模型，它能够从数据中找到复杂的非线性关系

```python
from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor()

tree_reg.fit(housing_prepared, housing_labels)
```

还是一样的创建实例，训练模型。
接下来测量RMSE：
```python
housing_predictions = tree_reg.predict(housing_prepared)
tree_mse = mean_squared_error(housing_labels, housing_predictions)
tree_rmse = np.sqrt(tree_mse)
tree_rmse
# 0.0
```

完全没有错误？这个模型真的可以做到绝对完美吗？当然，更有可能的是这个模型对数据严重**过拟合了**。我们怎么确认呢？前面提到过，在你有信心启动模型之前，都不要触碰测试集，所以这里，你需要拿训练集中的一部分用于训练，另一部分用于模型验证。

### 使用交叉验证来更好地进行评估
评估决策树模型的一种方法是使用`train_test_split`函数将训练集分为较小的训练集和验证集，然后根据这些较小的训练集来训练模型，并对其进行评估。这虽然有一些工作量，但是不会太难，并且非常有效。(详细查看**创建测试集**)


另一个不错的选择是使用`Scikit-Learn`的K-折交叉验证功能。以下是执行K-折交叉验证的代码：它将训练集随机分割成10个不同的子集，每个子集称为一个折叠，然后对决策树模型进行10次训练和评估——每次挑选1个折叠进行评估，使用另外的9个折叠进行训练。产生的结果是一个包含10次评估分数的数组：

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(tree_reg, housing_prepared, housing_labels,
                         scoring="neg_mean_squared_error", cv=10)
tree_rmse_scores = np.sqrt(-scores)
```

在这段代码中，使用了`cross_val_score`函数从`sklearn.model_selection`模块来评估个决策树回归模型`（tree_reg）`，在给定数据集（`housing_prepared`作为特征`housing_labels`作为目标变量）上的性能。这里使用了10折交叉验证（`cv=10`）来更准地估计模型的泛化能力。

关于**K-折交叉验证**，我们简单介绍一下并于普通验证做一下对比：

**简单交叉验证**
方法：将原始数据集随机划分成训练集和验证集两部分。比如说，将样本按照70%~30%的比例分成两部分，70%的样本用于训练模型；30%的样本用于模型验证。
缺点：
1. 数据都只被所用了一次，没有被充分利用
2. 在验证集上计算出来的最后的评估指标与原始分组有很大关系。

**k-折交叉验证**
为了解决简单交叉验证的不足，提出k-fold交叉验证。
1. 首先，将全部样本划分成k个大小相等的样本子集；
2. 依次遍历这k个子集，每次把当前子集作为验证集，其余所有样本作为训练集，进行模型的训练和评估；
3. 最后把k次评估指标的平均值作为最终的评估指标。在实际实验中，k通常取10.
举个例子：这里取k=10，如下图所示：
![[QQ_1723980213180.png]]

`cross_val_score`函数使用方法如下：
`sklearn.model_selection.cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=’warn’, n_jobs=None, verbose=0, fit_params=None, pre_dispatch=‘2*n_jobs’, error_score=’raise-deprecating’)`
参数：
`estimator`： 需要使用交叉验证的算法
`X`： 输入样本数据
`y`： 样本标签
`groups`： 将数据集分割为训练/测试集时使用的样本的组标签（一般用不到）
`scoring`： 交叉验证最重要的就是他的验证方式，选择不同的评价方法，会产生不同的评价结果。我们的代码中使用的是：`scoring="neg_mean_squared_error"`，即负均方误差。
负号是为了让 `cross_val_score` 函数能够正确地根据“分数越高，性能越好”的原则来比较不同模型或参数配置的 `MSE`，而后续取负号和开平方根则是为了将结果转换回常用的` RMSE` 度量。


`Scikit-Learn`的交叉验证功能更倾向于使用效用函数（越大越好）而不是成本函数（越小越好），所以计算分数的函数实际上是负的`MSE`（一个负值）函数，这就是为什么上面的代码在计算平方根之前会先计算出`-scores`。让我们看看结果：

```python
def display_scores(scores):
    print("Scores:", scores)
    print("Mean:", scores.mean())
    print("Standard deviation:", scores.std())

display_scores(tree_rmse_scores)
```

```python
Scores: [72831.45749112 69973.18438322 69528.56551415 72517.78229792
 69145.50006909 79094.74123727 68960.045444   73344.50225684
 69826.02473916 71077.09753998]
Mean: 71629.89009727491    #平均值
Standard deviation: 2914.035468468928 #标准差
```

请注意，交叉验证不仅可以得到一个模型性能的评估值，还可以衡量该评估的精确度（即其标准差）。这里该决策树得出的评分约为71 407，上下浮动±2439。如果你只使用了一个验证集，就收不到这样的结果信息。交叉验证的代价就是要多次训练模型，因此也不是永远都行得通。

保险起见，我们来看看线性回归的模型性能评估：

```python
lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,
                             scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores = np.sqrt(-lin_scores)
display_scores(lin_rmse_scores)
```

结果如下：
```python
Scores: [71762.76364394 64114.99166359 67771.17124356 68635.19072082
 66846.14089488 72528.03725385 73997.08050233 68802.33629334
 66443.28836884 70139.79923956]
Mean: 69104.07998247063
Standard deviation: 2880.328209818064
```
这里有一个更好查看数据的方法：`pd.Series(np.sqrt(-scores)).describe()`
```python
scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring="neg_mean_squared_error", cv=10)
pd.Series(np.sqrt(-scores)).describe()
```
结果如下：
```python
count       10.000000
mean     69104.079982
std       3036.132517
min      64114.991664
25%      67077.398482
50%      68718.763507
75%      71357.022543
max      73997.080502
dtype: float64
```
可见决策树模型的平均值和标准差比线性回归的还要高，确实是过拟合了。

我们再来试试最后一个模型：`RandomForestRegressor`。这里我们将跳过大部分代码，因为与其他模型基本相同：


```python
from sklearn.ensemble import RandomForestRegressor

forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)
forest_reg.fit(housing_prepared, housing_labels)
housing_predictions = forest_reg.predict(housing_prepared)
forest_mse = mean_squared_error(housing_labels, housing_predictions)
forest_rmse = np.sqrt(forest_mse)
forest_rmse
```

这里先直接求出RMSE，不用交叉验证。
结果如下：
```python
18650.698705770003
```

再来用交叉验证求
```python
from sklearn.model_selection import cross_val_score

forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,scoring="neg_mean_squared_error", cv=10)

forest_rmse_scores = np.sqrt(-forest_scores)
display_scores(forest_rmse_scores)
```
结果如下：
```python
Scores: [51559.63379638 48737.57100062 47210.51269766 51875.21247297
 47577.50470123 51863.27467888 52746.34645573 50065.1762751
 48664.66818196 54055.90894609]
Mean: 50435.58092066179
Standard deviation: 2203.3381412764606
```

## 微调模型 （***）

假设你现在有了一个有效模型的候选列表。现在你需要对它们进行微调。我们来看几个可行的方法。

### 网格搜索
一种微调的方法是手动调整超参数，直到找到一组很好的超参数值组合。这个过程非常枯燥乏味，你可能坚持不到足够的时间来探索出各种组合。
相反，你可以用`Scikit-Learn`的`GridSearchCV`来替你进行探索。你所要做的只是告诉它你要进行实验的超参数是什么，以及需要尝试的值，它将会使用交叉验证来评估超参数值的所有可能组合。例如，下面这段代码搜索`RandomForestRegressor`的超参数值的最佳组合：

```python
from sklearn.model_selection import GridSearchCV

param_grid = [
    # try 12 (3×4) combinations of hyperparameters
    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
    # then try 6 (2×3) combinations with bootstrap set as False
    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
  ]

forest_reg = RandomForestRegressor(random_state=42)
# train across 5 folds, that's a total of (12+6)*5=90 rounds of training 
grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                           scoring='neg_mean_squared_error',
                           return_train_score=True)
grid_search.fit(housing_prepared, housing_labels)
```