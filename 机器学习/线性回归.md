## 线性回归

### 案例1

```python
import statsmodels.api as sm
import statsmodels.formula.api as smf
import seaborn as sns
```

其中，第 1 个接口(statsmodels.api，简记 sm)基于数组
第 2 个接口(statsmodels.formula.api，简记 smf)则基于公式

然后，从 statsmodels 的 datasets 子模块载入 Engel(1857)的数据框

```python
data = sm.datasets.engel.load_pandas().data
data.head()
```

加载statsmodels.api中的恩格尔数据集并以pandas的方式输出。并输出前五行结果

```python
income foodexp
0 420.157651 255.839425
1 541.411707 310.958667
2 901.157457 485.680014
3 639.080229 402.997356
4 750.875606 495.560775

```

**Engel(1857)对于食物开支占家庭收入比重(恩格尔系数)的研究**

结果显示，此数据框包括两个变量，即 income(家庭年收入)
与foodexp(家庭年食物开支)，观测对象为 1857 年的 235 个比利时家庭。

```python
model = smf.ols('foodexp ~ income' , data = data) # ols表示最小二乘
```

其第 1 个参数“'foodexp ~ income'”即所谓“公式”(formula)，表示将 foodexp 对 income 进行回归(默认包含常数项)
得到 ols 类的一个实例(instance)，且记为“model”

```python
result = model.fit()
result.params
```

结果如下

```python
Intercept    147.475389
income         0.485178
dtype: float64
```

$$\widehat{foodexp} = 147.475 + 0.485 \, income$$

此线性模型的估计结果为以上“拟合线”

使用 seaborn 模块，画 income 与 foodexp 的散点图，并在图上画出线性拟合方程(4.15)及其置信区间：

```python
sns.regplot(x = 'income',y = 'foodexp' , data = data)
#regplot 表示 regression plot
```

![[QQ_1746705910765.png]]
Cobb-Douglas 生产函数(Cobb and Douglas,1928)是经济学常用的生产函数，其回归方程可写为

### 案例2

$$\ln y = \alpha + \beta \ln k + \gamma \ln l + \varepsilon$$

```python
import pandas as pd
data = pd.read_csv('Cobb-Douglas.csv')
data.head()
```

```python
year k l y lnk lnl lny
0 1899 100 100 100 4.605170 4.605170 4.605170
1 1900 107 105 101 4.672829 4.653960 4.615120
2 1901 114 110 112 4.736198 4.700481 4.718499
3 1902 122 118 122 4.804021 4.770685 4.804021
4 1903 131 123 124 4.875197 4.812184 4.820282
```

其中，y，k与l 分别为美国制造业的产出、资本与劳动力指数(已将 1899年取值标准化为 100)，而 lny，lnk 与 lnl 则为相应的对数。
其次，使用 statsmodels 的 ols 类进行线性回归：

```python
model = smf.ols('lny ~ lnk+lnl' , data = data)
result = model.fit()
results.params
```

类似上一种方法可得结果

```python
Intercept   -0.177310
lnk          0.233054
lnl          0.807278
dtype: float64
```

可通过 mpl_toolkits 模块的 mplot3d 子模块画三维拟合图。先导入Numpy，Matplotlib 以及 mpl_toolkits 的 mplot3d

```python
import numpy as np
import matplotlib.pyplot import plt
from mpl_toolkits import mplot3d
```

```python
xx = np.linspace(data.lnk.min(),data.lnk.max(),100)
yy = np.linspace(data.lnl.min(),data.lnl.max(),100)
```

分别将 lnk 和 lnl 分成100等分

```python
xx.shape
yy.shape
```

观察形状

```python
(100,)
```

`XX` 复制相同行是为了在二维空间中，让 x 方向的每个取值与 y 方向的每个取值进行全排列组合，形成规则的坐标网格，方便后续的数值计算与可视化。

```python
XX, YY = np.meshgrid(xx, yy)
```

计算zz

```python
ZZ = results.params[0] +XX * results.params[1] +YY * results.params[2]
```

最后，输入以下命令，画三维散点图与拟合回归平面：

```python
fig = plt.figure()
ax = plt.axes(projection = '3d')
ax.scatter3D(data.lnk , data.lnl , data.lny , c = 'b')
ax.plot_surface(XX,YY,ZZ, rstride = 8,cstride = 8,alpha = 0.3 , cmap = 'viridis')
ax.set_xlabel('lnk')
ax.set_ylabel('lnl')
ax.set_zlabel('lny')
```

第 2 行命令“ax = plt.axes(projection='3d')”表示，在此 ax 画轴上画三维图
第3行命令使用mplot3d的scatter3D()方法将样本数据画三维散点图，其中参数“c='b'”表示颜色蓝色
第4 行命令使用 mplot3d 的 plot_surface()方法画拟合的回归平面。其中，参数“rstride=8”与“cstride=8”分别指定行(row)与列(column)方向的画图步幅(stride)为8；参数“alpha=0.3”控制平面的透明度，而“cmap='viridis'”表示使用“viridis”作为“色图”(color map)。

![[QQ_1746710096695.png]]
**由于线性模型最为简单，故本章将以线性回归为例，说明机器学习的一些重要原理，包括过拟合与泛化能力、偏差与方差的权衡、模型评估方法等。**

### OLS显式解

最小二乘法寻找能使残差平方和

$$\begin{align*} \min_{\tilde{\beta}} \text{SSR}(\tilde{\beta}) &= \sum_{i=1}^n e_i^2 = \mathbf{e}'\mathbf{e} \quad \text{(将平方和写成向量内积的形式)} \\ &= (\mathbf{y} - \mathbf{X}\tilde{\beta})'(\mathbf{y} - \mathbf{X}\tilde{\beta}) \quad \text{(代入残差向量的表达式)} \\ &= (\mathbf{y}' - \tilde{\beta}'\mathbf{X}')(\mathbf{y} - \mathbf{X}\tilde{\beta}) \quad \text{(矩阵转置的运算性质)} \\ &= \mathbf{y}'\mathbf{y} - \mathbf{y}'\mathbf{X}\tilde{\beta} - \tilde{\beta}'\mathbf{X}'\mathbf{y} + \tilde{\beta}'\mathbf{X}'\mathbf{X}\tilde{\beta} \quad \text{(乘积展开)} \\ &= \mathbf{y}'\mathbf{y} - 2\mathbf{y}'\mathbf{X}\tilde{\beta} + \tilde{\beta}'\mathbf{X}'\mathbf{X}\tilde{\beta} \quad \text{(合并同类项)} \\ &\quad  \end{align*}$$
这个损失函数是关于$\beta$的二次型函数。
将$SSR$对向量$\beta$求导，可得最小化问题的一阶条件

$$\begin{align*} \frac{\partial (\text{SSR})}{\partial \tilde{\boldsymbol{\beta}}} &= \frac{\partial \left( \mathbf{y}'\mathbf{y} - 2\mathbf{y}'\mathbf{X}\tilde{\boldsymbol{\beta}} + \tilde{\boldsymbol{\beta}}'\mathbf{X}'\mathbf{X}\tilde{\boldsymbol{\beta}} \right)}{\partial \tilde{\boldsymbol{\beta}}} \\ &= -2\mathbf{X}'\mathbf{y} + 2\mathbf{X}'\mathbf{X}\tilde{\boldsymbol{\beta}} = \mathbf{0} \end{align*}$$
$$\hat{\boldsymbol{\beta}} \equiv (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\mathbf{y}$$
前提是$(\mathbf{X}'\mathbf{X})^{-1}$可逆，则X满列秩。
换言之，不存在某个列向量可被其他列向量线性表出的情形
如果数据矩阵 X 不满列秩，则称存在 严格多重共线性。

OLS 一般不适用于“高维数据”(high-dimensional data)。
由于高维数据的变量个数大于样本容量  即$p<n$。
故$rank(X) <n< p$(矩阵X的秩小于或等于其行数n)，故数据矩阵X不满列秩。此时$(\mathbf{X}'\mathbf{X})^{-1}$不存在，故 OLS 没有唯一解。

对于高维回归，一般须进行“正则化”(regularization)处理，即在损失函数中加入“惩罚项”(penalty term)，进行“惩罚回归“。

### 几何解释及OLS的正交性

$$\mathbf{X}'\mathbf{y} - \mathbf{X}'\mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{0}$$
提出一个X'可得
$$\mathbf{X}'(\underbrace{\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}}}_{= \mathbf{e}}) = \mathbf{X}'\mathbf{e} = \mathbf{0}$$
这意味着，残差向量e与数据矩阵X正交。更具体地，将上式展开写：

$$\mathbf{X}'\mathbf{e} = \underbrace{\begin{pmatrix} 1 & 1 & \cdots & 1 \\ x_{12} & x_{22} & \cdots & x_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ x_{1p} & x_{2p} & \cdots & x_{np} \end{pmatrix}}_{\mathbf{X}'} \underbrace{\begin{pmatrix} e_1 \\ e_2 \\ \vdots \\ e_n \end{pmatrix}}_{\mathbf{e}} = \underbrace{\begin{pmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{pmatrix}}_{\mathbf{0}} = \mathbf{0}$$
残差向量e与每个变量都正交。
且由第一行的常数项可得

![[QQ_1746857788979.png]]
由于拟合值为各变量的线性组合,故拟合值向量 ^y正好在超平面col( X) 上

$\hat{\mathbf{y}}'\mathbf{e} = (\mathbf{X}\hat{\boldsymbol{\beta}})'\mathbf{e} = \hat{\boldsymbol{\beta}}'\underbrace{\mathbf{X}'\mathbf{e}}_{= 0} = \hat{\boldsymbol{\beta}}' \cdot \mathbf{0} = 0$可得^y也与残差正交。

将$\beta$代入式中得
$$\hat{\mathbf{y}} = \mathbf{X}\hat{\boldsymbol{\beta}} = \underbrace{\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'}_{\equiv \mathbf{P}_\mathbf{X}}\mathbf{y} \equiv \mathbf{P}_\mathbf{X}\mathbf{y}$$
$P_X$为**投影矩阵**。其作用是将y (或任何n维列向量)投影到X的列空间col( X)

$$\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}} = \mathbf{y} - \mathbf{P}_\mathbf{X}\mathbf{y} = \underbrace{(\mathbf{I} - \mathbf{P}_\mathbf{X})}_{\equiv \mathbf{M}_\mathbf{X}} \mathbf{y} \equiv \mathbf{M}_\mathbf{X}\mathbf{y}$$
$M_X$为**消灭矩阵**。其作用是将y向X的列空间col( X) 投影，然后得到此投影的残差。

消灭矩阵在线性代数的“施密特正交化”(Gram-Schmidt orthogonalization)中起着重要作用，因为施密特正交化本质上就是投影之后计算残差的运算。
$$\mathbf{M}_\mathbf{X} \equiv \mathbf{I} - \mathbf{P}_\mathbf{X} = \left[ \mathbf{I} - \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \right]$$

### 拟合优度

$\sum_{i=1}^{n} (y_i - \bar{y})^2$(平方和分解公式$SST=SSR+SSE$)
**当回归方程有常数项。**

证明：将$y_i - \bar{y}$分解成$y_i - \hat{y}_i+\hat{y}_i-\bar{y}=e_i+\hat{y}_i-\bar{y}$得
$$\begin{align*} \sum_{i = 1}^{n}(y_i-\bar{y})^2&=\sum_{i = 1}^{n}(y_i - \hat{y}_i+\hat{y}_i-\bar{y})^2=\sum_{i = 1}^{n}(e_i+\hat{y}_i-\bar{y})^2\\ &=\sum_{i = 1}^{n}e_i^2+\sum_{i = 1}^{n}(\hat{y}_i-\bar{y})^2+2\sum_{i = 1}^{n}e_i(\hat{y}_i-\bar{y}) \end{align*}  $$
因此，只要证明交叉项为0。

首先，根据 OLS 的正交性有$$\sum_{i = 1}^{n}e_i\hat{y}_i=\hat{\mathbf{y}}'\mathbf{e}=0$$
由上文，当回归方程中有常数项时有
$$(1\ 1\cdots1)\begin{pmatrix}e_1\\e_2\\\vdots\\e_n\end{pmatrix}=\sum_{i = 1}^{n}e_i = 0$$
则得证，平方和分解公式成立。

根据平方和分解公式，$y_i$的离差平方$\sum_{i = 1}^{n}(y_i-\bar{y})^2$可分解成不可解释部分$\sum_{i = 1}^{n}e_i^2$和可解释部分$\sum_{i = 1}^{n}(\hat{y}_i-\bar{y})^2$。
如果模型可解释的部分所占比重越大，则样本回归线的拟合程度越好。
**定义 拟合优度**$R^2$
$$0\leq R^2\equiv\frac{\sum_{i = 1}^{n}(\hat{y}_i-\bar{y})^2}{\sum_{i = 1}^{n}(y_i - \bar{y})^2}=1-\frac{\sum_{i = 1}^{n}e_i^2}{\sum_{i = 1}^{n}(y_i - \bar{y})^2}\leq1$$
也称为**可决系数**。

### 过拟合与泛化能力

线性模型可视为一阶泰勒近似。为此，可以考虑加入高次项。但如果加入太多高次项，则会引起过拟合(overfit)。我们使用“模拟数据”(simulated data)考察过拟合问题。

生成模拟数据，并画散点图与函数$f(x)=\sin(2\pi x)$

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.formula.api as smf

np.random.seed(42)
x = np.random.rand(10) # 随机10个生成（0，1）的随机数
y = np.sin(2* np.pi * x)+np.random.normal(0,0.3,10)
plt.scatter(x,y)
plt.xlabel('x')
plt.ylabel('y')
w = np.linspace(0,1,100)
plt.plot(w,np.sin(2*np.pi *w))
```

考虑使用以下多项式(polynomial)函数来拟合此数据
$$y = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_M x^M + \varepsilon$$
使用 for 循环分别进行 OLS 估计。

```python
fig,ax = plt.subplots(3, 3, sharex=True, sharey=True, subplot_kw=dict(yticks=[]))
fig.subplots_adjust(hspace= 0.1, wspace = 0.1,)
for m in range(1,10):
    ax = fig.add_subplot(3,3,m)
    ax.set_xlim([0,1])
    ax.set_ylim([-1.5,1.5])
    ax.set_yticks(np.arange(-1.5,2,0.5))
    ax.plot(w,np.sin(2* np.pi *w) , 'k' , linewidth = 1)
    sns.regplot(x=x, y=y, order=m, ci=None, scatter_kws={'s': 15}, line_kws={'linewidth': 1})
    ax.text(0.6,1,'M = ' +str(m))
plt.tight_layout()
```

其中，第 1 个命令设定画布(fig)
由3 $\times$ 3  的画轴(ax)所构成，参数“sharex=True”与“sharey=True”表示所有画轴将共享 x 轴与 y
轴的尺度，而参数“subplot_kw=dict(yticks=[])”将 y 轴的标记(ticks)设为空(避免重复标记)

![[QQ_1747057418884.png]]
![[QQ_1747057427409.png]]
![[QQ_1747057439711.png]]
3-6 次函数似乎拟合得最好，可视为“good fit” 、 “just fit”或“ideal fit” 。

6 次以上的多项式函数，其拟合效果则越来越过拟合(overfit)，即虽然样本内的拟合程度越来越好(比如R2越来越高，参见图 4.9)

在过拟合的情况下，虽然样本内的拟合优度很高，但模型在样本外的预测能力反而下降。然而，机器学习的目的恰恰是样本外的预测能力，即将模型运用于其未见过的数据时，所具有的推广预测能力，即模型的泛化能力。

下面继续用模拟数据考察多项式回归的泛化能力。使用同样的数据生成过程(但用不同的随机数种子)，随机生成 100 个原模型所未见过的新观测值，构成一个测试集(test data)，然后进行 1-9 阶多项式回归，并计算其测试误差(test error)：

```python
R2 = []
train_error = []
for m in range(1,9):
    stats = np.polyfit(x,y,m,full = True)
    ssr = stats[1]
    train_error.append((ssr/10)**0.5)
    r2 = 1-ssr/(sum((y-y.mean())**2))
    R2.append(r2)
R2.append(1)
train_error.append(0)
```

其中，第 1-2 个命令首先初始化 R2 与 train_error 为空的列表(emptylists)

在 for 循环中，使用 Numpy 的 polyfit()函数进行多项式回归，其中参数“m”为多项式的阶数，而“full=True”表示返回更多信息(默认仅返回回归系数)。

把 polyfit()函数的返回值记为 stats，并提取其第 1 个元素stats[1]为 ssr，即残差平方和(sum of squared residuals)。命令“train_error.append((ssr / 10) ** 0.5)”将残差平方
和 ssr 除以样本容量 10，再开平方，即为均方误差(RMSE)，然后加入列表train_error。类似地，计算拟合优度，并加入列表 R2。

此时的R2 为数组，需要改为标量值。

```python
print(R2)
import numpy as np

# 修正：确保所有元素都是标量值
R2 = [x.item() if isinstance(x, np.ndarray) else x for x in R2]
print(R2)
```

输出结果如下

```python
[array([0.44666296]), array([0.48692919]), array([0.9321818]), array([0.93256872]), array([0.9330259]), array([0.94350078]), array([0.94925029]), array([0.94948975]), 1]
[0.44666295521945165, 0.48692918660797735, 0.9321817990565615, 0.9325687156147496, 0.933025901649715, 0.9435007751739376, 0.9492502866735225, 0.9494897531177422, 1]
```

通过画图考察样本内拟合优度随着回归阶数的变化，可输入命令：

```python
plt.plot(range(1,10),R2,'o-')
plt.xlabel('Degree of Polynomial')
plt.ylabel('R2')
plt.title('In-sample Fit')
```

![[Pasted image 20250706152822.png]]
随着多项式的阶数越来越高，样本内的拟合优度R2越来越高，并于M=9(9 阶多项式)时达到 1
但多项式回归发现数据中真正信号的能力，当M =3之后却越来越低
直观上，数据可视为信号与噪音的组合，参见方程。当样本内的拟合越来越完美时，这意味着回归函数也拟合了大量的噪音(因为回归函数“跟”数据跟得太紧)；而噪音对于样本外的预测毫无意义。
在过拟合的情况下，虽然样本内的拟合优度很高，但模型在样本外的预测能力反而下降。
然而，机器学习的目的恰恰是样本外的预测能力，即将模型运用于其未见过的数据
(unseen data)时，所具有的推广预测能力，即模型的泛化能力。
下面继续用模拟数据考察多项式回归的泛化能力。使用同样的数据生成过程(但用不同的随机数种子)，随机生成 100 个原模型所未见过的新观测值，构成一个测试集(test data)，然后进行 1-9 阶多项式回归，并计算其测试误差(test error)。

```python
test_error = []
for m in range(1,10):
    coefs = np.polyfit(x,y,m)
    np.random.seed(123)
    x_new = np.random.rand(100) #生成[0,1]之间的随机数
    y_new = np.sin(2*np.pi*x_new)+np.random.normal(0,0.3,100) # 加入均值为 0，标准差为 0.3 的高斯噪声
    pred = np.polyval(coefs,x_new) 
    ssr = (sum((pred-y_new)**2)/100)**0.5 
    test_error.append(ssr)
```

此时的train_error 是已经处理成标量了的 步骤与R2类似 此处省略。
接下来作图

```python
plt.plot(range(1,10),train_error,'o--k',label = 'Training Error')
plt.plot(range(1,10),test_error,'o--b',label = 'Test Error')
plt.ylim(-0.05,1)
plt.xlabel('Degree of Polynomial')
plt.ylabel('Root Mean Squared Errors')
plt.legend()
plt.title('Training Error vs Test Error')
```

结果如下
![[Pasted image 20250706165852.png]]
作为对比，考察测试误差(Test Error)的具体取值。

```python
print(test_error)

[0.627292912034017, 0.6674828980959218, 0.3755499502639337, 0.37676901692557796, 0.3716217933651426, 0.3888762182609997, 1.8923251664241212, 13.382614784961655, 1124.9782332162392]
```

结果显示，样本外的测试误差在M = 5 时达到最低值，然后不断上升；而且当自由度几乎用尽时，测试误差急剧上升，以致于超出y轴的画图范围。

### 偏差与方差权衡

随着模型复杂程度的增加，测试误差一般呈现出 U型的曲线特征，即先下降而后上升。
为什么测试误差一般呈 U 型？事实上，测试误差受到两种不同力量的影响，即偏差与方差。

偏差(bias)指的是估计量是否有系统误差(比如，系统性高估或低估) 。给定X，则估计量 $\hat{f}(X)$。偏差是指 估计值的期望与真实值之间的差值。

$$
\text{Bias}(\hat{f}(\mathbf{x})) \equiv \mathbb{E}\left[\hat{f}(\mathbf{x})\right] - f(\mathbf{x})
$$
由上式可知，偏差度量的是在大量重复抽样过程中，$\hat{f}(X)$对于$f(X)$的平均偏差程度。

方差(Variance)则衡量在大量重复抽样过程中，估计量$\hat{f}(X)$本身围绕着其期望值$E \hat{f}(X)$
的波动幅度，其定义为
（对于估计值的方差定义式）

$$\text{Var}(\hat{f}(\mathbf{x})) \equiv \mathbb{E}\left[ \hat{f}(\mathbf{x}) - \mathbb{E}\hat{f}(\mathbf{x}) \right]^2
$$
给定X，估计量$\hat{f}(X)$的均方误差可作如下分解：
$$\begin{align*} \text{MSE}\bigl(\hat{f}(\mathbf{x})\bigr) &= \mathrm{E}\!\left[\, y - \hat{f}(\mathbf{x}) \,\right]^{\!2} \quad \text{(MSE定义)} \\ &= \mathrm{E}\!\left[\, f(\mathbf{x}) + \varepsilon - \hat{f}(\mathbf{x}) \,\right]^{\!2} \quad \text{(代入模型 } y = f(\mathbf{x}) + \varepsilon \text{)} \\ &= \mathrm{E}\!\left[\, f(\mathbf{x}) - \mathrm{E}\hat{f}(\mathbf{x}) + \mathrm{E}\hat{f}(\mathbf{x}) - \hat{f}(\mathbf{x}) + \varepsilon \,\right]^{\!2} \quad \text{加减 } \mathrm{E}\hat{f}(\mathbf{x}) \text{)} \\ &= \underbrace{\bigl[\, \mathrm{E}\hat{f}(\mathbf{x}) - f(\mathbf{x}) \,\bigr]^{\!2}}_{\text{Bias}^2} + \underbrace{\mathrm{E}\!\left[\, \hat{f}(\mathbf{x}) - \mathrm{E}\hat{f}(\mathbf{x}) \,\right]^{\!2}}_{\text{Variance}} + \underbrace{\mathrm{E}(\varepsilon^2)}_{\text{Var}(\varepsilon)} \\ &= \underbrace{\text{Bias}^2 + \text{Variance}}_{\text{reducible}} + \underbrace{\text{Var}(\varepsilon)}_{\text{irreducible}} \end{align*}$$
$\hat{f}(X) - E\hat{f}(X)$是常数
$E(\varepsilon)^2 = E(\varepsilon^2) -E(\varepsilon)^2 = E(\varepsilon^2)$             ($E(\varepsilon) = 0$)

其中，平方展开后的三个交叉项均为 0，证明如下:

第一项
$$\begin{align*} \mathbb{E}\!\left[ \left( f(\mathbf{x}) - \mathbb{E}\hat{f}(\mathbf{x}) \right) \left( \mathbb{E}\hat{f}(\mathbf{x}) - \hat{f}(\mathbf{x}) \right) \right] &\quad \text{([} f(\mathbf{x}) - \mathbb{E}\hat{f}(\mathbf{x}) \text{]为常数)} \\ &= \left( f(\mathbf{x}) - \mathbb{E}\hat{f}(\mathbf{x}) \right) \cdot \mathbb{E}\!\left[ \mathbb{E}\hat{f}(\mathbf{x}) - \hat{f}(\mathbf{x}) \right] \quad \text{(求期望为线性运算)} \\ &= \left( f(\mathbf{x}) - \mathbb{E}\hat{f}(\mathbf{x}) \right) \cdot \underbrace{\left( \mathbb{E}\hat{f}(\mathbf{x}) - \mathbb{E}\hat{f}(\mathbf{x}) \right)}_{=0} = 0 \end{align*}
$$
第二项
$$\begin{align*} \mathrm{E}\left[ \bigl( f(\mathbf{x}) - \mathrm{E}\hat{f}(\mathbf{x}) \bigr) \varepsilon \right] &= \mathrm{E}_{\mathbf{x}} \mathrm{E}\left[ \bigl( f(\mathbf{x}) - \mathrm{E}\hat{f}(\mathbf{x}) \bigr) \varepsilon \,\big|\, \mathbf{x} \right] \quad \text{(重期望律)} \\ &= \mathrm{E}_{\mathbf{x}} \left[ \bigl( f(\mathbf{x}) - \mathrm{E}\hat{f}(\mathbf{x}) \bigr) \cdot \mathrm{E}\bigl( \varepsilon \,\big|\, \mathbf{x} \bigr) \right] \quad \\ &= \mathrm{E}_{\mathbf{x}} \left[ \bigl( f(\mathbf{x}) - \mathrm{E}\hat{f}(\mathbf{x}) \bigr) \cdot 0 \right] = 0 \end{align*}$$
第三项
$$\begin{align*} \mathrm{E}\left[ \bigl( \mathrm{E}\hat{f}(\mathbf{x}) - \hat{f}(\mathbf{x}) \bigr) \varepsilon \right] &= \mathrm{E}_{\mathbf{x}} \mathrm{E}\left[ \bigl( \mathrm{E}\hat{f}(\mathbf{x}) - \hat{f}(\mathbf{x}) \bigr) \varepsilon \,\big|\, \mathbf{x} \right] \quad \text{(重期望律)} \\ &= \mathrm{E}_{\mathbf{x}} \left[ \bigl( \mathrm{E}\hat{f}(\mathbf{x}) - \hat{f}(\mathbf{x}) \bigr) \cdot \mathrm{E}\bigl( \varepsilon \,\big|\, \mathbf{x} \bigr) \right] \quad  \\ &= \mathrm{E}_{\mathbf{x}} \left[ \bigl( \mathrm{E}\hat{f}(\mathbf{x}) - \hat{f}(\mathbf{x}) \bigr) \cdot 0 \right] = 0 \end{align*}$$
因此，估计量$\hat{f}(X)$的均方误差可分解为偏差平方$(Bias^2)$、方差与扰动项方差$Var(\varepsilon)$之和。
![[Pasted image 20250706201108.png]]
**方差为估计值的方差；偏差为估计值的期望与真实值的差**。
靶心为真实的 y，而射中位置为预测的 ˆy。左上角的低偏差、低方差情形为最为理想的模型，其 ˆy总在 y的附近。
右上角的模型虽然平均而言系统偏差很小，但方差很大，故经常偏离靶心，存在“过拟合”(overfit)。
左下角的模型则正好相反，虽然方差很小，几乎总打在相同的地方，但遗憾的是此地并非靶心，故偏差较大，存在“欠拟合”(underfit)。
右下角的模型则偏差与方差都较大，不仅存在较大系统偏差，而且波动幅度大，故是最糟糕的模型。

另一方面，扰动项方差$Var(\varepsilon)$则“不可降低”(irreducible)，即使知道真实函数 f(x) ，但$Var(\varepsilon)$也依然存在。
本质上，扰动项的方差$Var(\varepsilon$)  决定了预测问题的困难程度。
如果扰动项方差$Var(\varepsilon)$很大，即使你所学得的函数$\hat{f}(X)$已经很接近于真实函数$f(X)$ ，预测准确度依然可能不高；比如对金融市场的预测。
另一方面，如果扰动项方差很小，则只要$\hat{f}(X)$足够接近$f(X)$ ，预测准确度就可以很高；比如人脸识别。

通常来说，偏差平方与方差之间存在着此消彼长的替代关系。

在选择模型复杂程度时，存在偏差与方差的权衡(bias-variance trade-off)，故须选择合适的模型复杂程度，使得模型的均方误差达到最小值。

### 模型评估的再抽样方法

训练误差可能是对测试误差的糟糕估计。为了纠正此偏误，统计学的传统方法是使用信息准则(information criterion)，对过于复杂的模型进行惩罚。比如，常用的 AIC(Akaike Information Criterion)信息准则为。

$$\min_{p} \ \text{AIC} \equiv \ln(\text{MSE}) + \frac{2}{n} p$$
其中，右边第一项为对模型拟合度的奖励(减少均方误差 MSE)，而第二项为对变量过多(模型过于复杂)的惩罚(为变量个数 p的增函数)。

p上升时，第一项下降而第二项上升。通过选择合适的变量个数 p，最小化 AIC，达到在模型拟合度与简洁性(parsimony)之间的权衡，以避免过拟合。

另一常用的信息准则为“贝叶斯信息准则”(Bayesian Information Criterion，简记 BIC)：

$$\min_{p} \ \text{BIC} \equiv \ln(\text{MSE}) + \frac{\ln n}{n} \, p
$$
BIC 准则对于变量过多的惩罚比 AIC 准则更为严厉，因此 BIC 准则更强调模型的简洁性。

但信息准则的原理依赖于大样本理论的一些假设，在有限样本中未必能选出预测能力最优的模型。
故机器学习一般并不使用信息准则来评估模型，而通过再抽样(resampling)或重采样的方法来度量模型的泛化能力，包括验证集法、重复验证集法、K 折交叉验证、重复K 折交叉验证与自助法等。

在使用验证集法(validation set approach)时，先将样本数据随机地一分为二，其中大部分(比如 70%)作为训练集，而将其余的小部分(比如 30%)作为“验证集”(validation set)或“保留集”(hold-out set)。

在估计模型时，仅使用训练集；然后在验证集中进行样本外预测，并计算“验证集误差”(validation set error)，以此估计测试误差(test error)。

验证集法虽然操作方便，但有两个缺点。

1. 在使用不同的随机数种子，将数据分为不同的训练集和验证集时，其验证集误差可能波动较大
2. 在使用验证集法时，仅使用训练集的部分数据来估计模型，浪费了验证集的那部分信息，估计效率较低，导致出现偏低。

改进验证集法的一种简单方法为重复验证集法(repeated training/test splits)，即重复使用验证集法，比如 100 次。
这意味着，每次均将数据集随机分组为训练集与验证集(分割比例保持不变)，然后将每次所得的验证集误差进行平均。

这种方法虽可使估计结果更为稳定，但依然无法解决验证集法的偏差问题(因为每次仅用 70%的数据进行训练)。

K 折交叉验证：其中K 一般为 5或 10。以 10 折交叉验证为例。
首先，将样本数据随机地分为大致相等的 10 个子集。
其次，每次均留出一折(约十分之一样本)作为验证集，而将其余九折(约十分之九样本)作为训练集，以训练集估计模型，然后在验证集中进行预测，并计算验证集均方误差。
最后，将所有验证集均方误差进行平均，即为“交叉验证误差”(cross-validation error)，可作为对测试误差的估计。
![[Pasted image 20250706204851.png]]
如果将留出第k 折所得的验证集均方误差记为MSEk，则K 折交叉验证误差为

$$CV_{(K)} \equiv \frac{1}{K} \sum_{k=1}^K \text{MSE}_k
$$

