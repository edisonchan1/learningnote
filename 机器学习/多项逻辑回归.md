---
title: 多项逻辑回归
date: 2025-07-23
updated: 2025-07-23
description:
---

## 多项逻辑回归

### 多项逻辑回归介绍

逻辑回归推广至“多项逻辑回归”(Multinomial Logit)，应用于多分类问题。
多分类问题也很常见。比如，在识别手写数字时，响应变量 $y \in \{0, 1, 2, \dots, 9\}$ ，分别为从 $0-9$ 的十个整数，共分为十类。

假设 $y$ 的取值可分为K类，即 $y \in \{0, 1, 2, \dots, k\}$
给定特征向量 $x_i$ ，假设事件“ $y_i = k$ ” $(k =1,\dots ,K)$ 的条件概率为

$$
\mathrm{P}(y_i = k \mid \mathbf{x}_i) = \frac{\exp\left(\mathbf{x}_i' \boldsymbol{\beta}_k\right)}{\sum_{l=1}^K \exp\left(\mathbf{x}_i' \boldsymbol{\beta}_l\right)} \quad (k = 1, \dots, K)
$$

这就是“多项逻辑回归”。其中，参数向量 $\beta_k$ 为对应于第 $k$ 类的回归系数， $k =1,2,\dots,k$
对所有 K 个类别的 “指数得分” 求和，起到 **归一化** 作用 —— 让所有类别概率之和为 1（满足概率公理）。将第k类的指数得分在总体指数得分的占比作为第k类的预测概率。

右边也称为**软极值函数**(softmax function)，广泛用于分类问题的神经网络模型。
各类别的条件概率之和为 1，即

$$
\sum_{k=1}^{K} \mathrm{P}(y_i = k \mid \mathbf{x}_i) = 1
$$

条件概率的方程中无法同时识别所有的系数 $\beta_l (l = 1,\dots,k)$
这是因为如果将 $\beta_l$ 变为 $\beta_l+\alpha$ ，方程的右边依然不变，并不影响此模型第 $l$ 类预测的拟合效果。

$$
\frac{\exp\left[x_i' \left( \beta_k + \alpha \right) \right]}{\sum_{l=1}^K \exp\left[ x_i' \left( \beta_l + \alpha \right) \right]} = \frac{\exp\left( x_i' \beta_k \right) \cdot \exp\left( x_i' \alpha \right)}{\exp\left( x_i' \alpha \right) \cdot \sum_{l=1}^K \exp\left( x_i' \beta_l \right)} = \frac{\exp\left( x_i' \beta_k \right)}{\sum_{l=1}^K \exp\left( x_i' \beta_l \right)}  
$$

为此，通常将某类(比如，第 1类)作为“参照类别”(base category)，然后令其相应系数 $\beta_1 = 0$
由此，类别k 的条件概率可写为

$$
\mathrm{P}(y_i = k \mid \mathbf{x}_i) = \begin{cases} \displaystyle \frac{1}{1 + \sum_{l=2}^K \exp(\mathbf{x}_i' \boldsymbol{\beta}_l)} & (k = 1) \\[6pt] \displaystyle \frac{\exp(\mathbf{x}_i' \boldsymbol{\beta}_k)}{1 + \sum_{l=2}^K \exp(\mathbf{x}_i' \boldsymbol{\beta}_l)} & (k = 2, \dots, K) \end{cases}  
$$

其中“ $k =1$ ”所对应的类别为参照类别，故 $\beta_1=0$
显然，当 $k =2$ 时，多项逻辑模型就是逻辑回归：

$$
\mathrm{P}(y_i = k \mid \mathbf{x}_i) = \begin{cases} \displaystyle \frac{1}{1 + \exp(\mathbf{x}_i' \boldsymbol{\beta}_2)} & (k = 1) \\[6pt] \displaystyle \frac{\exp(\mathbf{x}_i' \boldsymbol{\beta}_2)}{1 + \exp(\mathbf{x}_i' \boldsymbol{\beta}_2)} & (k = 2) \end{cases}  
$$

即上一章节的逻辑回归。

### 最大似然估计

假设样本数据为“独立同分布” (independently and identically distribution，简记 iid)，则第i个观测值的似然函数为

$$
L_i(\boldsymbol{\beta}_1, \dots, \boldsymbol{\beta}_K) = \prod_{k=1}^K \left[ \mathrm{P}(y_i = k \mid \mathbf{x}_i) \right]^{I(y_i = k)}  
$$

其中， $\displaystyle \prod_{k=1}^K \left[ \cdot \right]$ 表示连乘；而指数中的 $I(\cdot)$ 为特征函数(indicator function)，即当 $y_i = k$ 时， $I(y_i = k) = 1$ ，反之则为 0。
将上式取对数，可得第 $i$ 个观测值的对数似然函数

$$
\ln L_i(\boldsymbol{\beta}_1, \dots, \boldsymbol{\beta}_K) = \sum_{k=1}^K \left[ I(y_i = k) \cdot \ln \mathrm{P}(y_i = k \mid \mathbf{x}_i) \right]  
$$

将所有观测值的对数似然函数加总，即得到整个样本的对数似然函数

$$
\max_{\boldsymbol{\beta}_1, \dots, \boldsymbol{\beta}_K} \ln L(\boldsymbol{\beta}_1, \dots, \boldsymbol{\beta}_K) = \sum_{i=1}^n \sum_{k=1}^K \left[ I(y_i = k) \cdot \ln \mathrm{P}(y_i = k \mid \mathbf{x}_i) \right]  
$$

有n个观测值

最大化此目标函数，即可得到系数估计值 $\hat \beta_1,\dots,\hat \beta_k$ 。
对于多项逻辑模型，也可以根据对数似然函数定义准 $R^2$ 与残差偏离度。与logit 模型类似

### 多项逻辑回归的解释

如果响应变量 $y$ 分为 $K$ 类，则多项逻辑模型有 $K-1$ 个参数向量：由于将第一类作为参照类别，故 $\beta_0 = 0$ 。那么应如何解释这些参数向量 $\beta_2 \dots ,\beta_k$ 呢（这里的 $X_i$ 与 $\beta_i$ 都是向量）

对于系数 $\hat \beta_k$ 的是解释，依赖于参照方案的设定。

$$
\mathrm{P}(y_i = k \mid \mathbf{x}_i) = \begin{cases} \displaystyle \frac{1}{1 + \sum_{l=2}^K \exp(\mathbf{x}_i' \boldsymbol{\beta}_l)} & (k = 1) \\[6pt] \displaystyle \frac{\exp(\mathbf{x}_i' \boldsymbol{\beta}_k)}{1 + \sum_{l=2}^K \exp(\mathbf{x}_i' \boldsymbol{\beta}_l)} & (k = 2, \dots, K) \end{cases}  
$$

由上述方程，响应变量 $y$ 归属于第 $K(k = 2,\dots ,K)$ 的条件概率与 $y$ 归属第 $1$ 类的条件概率之比为

$$
\frac{\mathrm{P}(y_i = k \mid \mathbf{x}_i)}{\mathrm{P}(y_i = 1 \mid \mathbf{x}_i)} = \exp(\mathbf{x}_i' \boldsymbol{\beta}_k)  
$$

进一步，如果某变量 $x_j$ 为离散变量(比如，性别、子女数)，则可通过几率比来解释该变量对 $y$ 的作用
假设 $x_j$ 增加 1 单位，从 $x_j$ 变为 $x_j + 1$ ，记条件概率 $\mathrm{P}(y_i = 1 \mid \boldsymbol{x}_i)$ 与 $\mathrm{P}(y_i = k \mid \boldsymbol{x}_i)$ 的新值分别为 $\mathrm{P}^*(y_i = 1 \mid \boldsymbol{x}_i)$ 与 $\mathrm{P}^*(y_i = k \mid \boldsymbol{x}_i)$ 。
可计算新几率与原几率的比率，即“几率比”(odds ratio)，也称为相对风险比率(Relative Risk Ratio，简记 RRR)

$$
\text{RRR} \equiv \frac{ \dfrac{\text{P}(y_i = k \mid \boldsymbol{x}_i)}{\text{P}(y_i = 1 \mid \boldsymbol{x}_i)} }{ \dfrac{\text{P}(y_i = k \mid \boldsymbol{x}_i')}{\text{P}(y_i = 1 \mid \boldsymbol{x}_i')} } = \frac{ \exp\left[ \beta_1 x_1 + \cdots + \beta_j (x_j + 1) + \cdots + \beta_p x_p \right] }{ \exp\left( \beta_1 x_1 + \cdots + \beta_j x_j + \cdots + \beta_p x_p \right) } = \exp(\beta_j)
$$

$\text{若 } \beta_j = 0.12\text{则几率比}\exp(\beta_j) = e^{0.12} = 1.13。$
这意味着，当 $x_j$ 增加 1 单位时，则(相对于参照方案的)新几率变为原几率的 1.13 倍，即增加 13%

### 多项逻辑回归的 R 案例

```python
import pandas as pd
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import cohen_kappa_score
```

```python
Glass = pd.read_csv('glass.csv')
Glass.shape
```

![fix](/images/Pasted%20image%2020250710125135.png)

(214, 10)

```python
sns.displot(Glass.Type ,kde = False)
```

![fix](/images/Pasted%20image%2020250710125354.png)

```python
sns.boxplot(x = 'Type',y = 'Mg',data = Glass,paletta = 'Set2')
# 箱型线 图显示了不同类型玻璃中镁含量的分布情况。
# 异常值用点表示，箱体的中线表示中位数，箱体的上下边缘分别表示四分位数。
```

![fix](/images/Pasted%20image%2020250710125456.png)

```python
X = Glass.iloc[:,:-1]# 选择特征变量（除去最后一列Type）
y = Glass.iloc[:,-1]# 选择目标变量（最后一列Type）
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y, random_state=0)
model = LogisticRegression(multi_class='multinomial', solver='newton-cg', C = 1e10,max_iter=1000)
model.fit(X_train, y_train)
prob = model.predict_proba(X_test)
pred = model.predict(X_test)
table = confusion_matrix(y_test, pred)
table = pd.crosstab(y_test, pred, rownames=['Actual'], colnames=['Predicted'])
sns.heatmap(table, annot=True, fmt='d', cmap='Blues')# 热力图显示了混淆矩阵的可视化效果。每个单元格的颜色深浅表示预测类别与实际类别之间的关系。

```

![fix](/images/Pasted%20image%2020250710125651.png)
