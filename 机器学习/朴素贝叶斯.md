---
title: 朴素贝叶斯
date: 2025-07-23
updated: 2025-07-23
description:
---

## 朴素贝叶斯

### 朴素贝叶斯介绍

参考上文判别分析的贝叶斯决策理论
假设训练数据为 $\{ \boldsymbol{x}_i, y_i \}_{i=1}^n$，而 $y_i$的可能取值为 $y_i \in \{1,2,\cdots,K\}$，共分为 $K$类。 “贝叶斯最优决策”(Bayes optimal decision)通过最大化“后验概率”(posterior probability)来作预测： $$ \max_{k} \ P(y_i = k \mid \boldsymbol{x} = \boldsymbol{x}_i) $$进一步，使用贝叶斯公式计算后验概率：

其中 $\pi_k = P(y_i = k)$为先验概率，而 $f_k(X_i) = p(X= X_i | y_i = k)$为给定类别 $y_i = k$的条件概率密度，而这可能很难估计。

特征向量 $X_i$可以是连续、离散或混合型随机向量（同时包括连续性与离散型变量），故有时无法假设 $X_i$服从多维正态分布。

$X_i$的维度也可能很高。例如，在使用“词频”(word frequency)判定“正常邮件”(email)与“垃圾邮件”(spam)时，涉及的关键词可能成千上万，而所得数据矩阵一般为高维的稀疏矩阵(sparse matrix)，故不易估计其协方差矩阵。这也是“维度灾难”(curse of dimensionality)的一种表现形式。

为简化计算，朴素贝叶斯分类起对高维的条件概率 $f_k(X_i)$作了一个假定
假设在给定类别 $y_i = k$的情况下，$X_i$的各分量属性之间条件独立。
$$ \begin{align*} f_k(\boldsymbol{x}_i) &\equiv P(\boldsymbol{x} = \boldsymbol{x}_i \mid y_i = k) \\ &= P(x_{i1} \mid y_i = k) \cdot P(x_{i2} \mid y_i = k) \cdots P(x_{ip} \mid y_i = k) \\ &= \prod_{j=1}^p P(x_{ij} \mid y_i = k) \end{align*} $$
其中，$p$为$\boldsymbol{x}_i = (x_{i1} \cdots x_{ip})'$的维度，即特征变量的个数。 根据朴素贝叶斯的假定，事实上将高维问题降为一维问题，因为只要分别估计$p$个单变量的条件概率$P(x_{ij} \mid y_i = k)$，其中$j = 1, \cdots, p$，然后连乘在一起即可。

尽管朴素贝叶斯的假定不切实际，但在不少情况下却能得到较好的预测效果。
可能由于在有些情况下，属性之间的部分相关性可能互相抵消。
而且，我们关心的是对于类别的预测，并非准确地估计条件概率。

### 拉普拉斯修正

在应用朴素贝叶斯分类器时，有时还需进行拉普拉斯修正。

朴素贝叶斯的概率连乘形式具有一票否决的特点：当某个变量 $x_{ij}$在训练数据的第 $K$类中共有 $n_k$次取值为0，而取值为1的次数为0

此时$$\hat p(x_{ij}=1|y_i = k) = \frac{0}{n_k}=0$$
$$ \hat{P}(x_{i1},\cdots,x_{ij} = 1,\cdots,x_{ip} \mid y_i = k) = 0 $$

此时连乘导致整个$y_i = k$的条件概率密度为0，进而导致后验概率为0。于是一旦在未来的预测数据中出现$x_{ij}=1$，无论其他取值如何，都会自动排除$y_i = k$的可能性，从而导致偏差。

拉普拉斯修正的解决方案为，将$x_{ij}$的不同取值在第$k$类数据中出现的次数均加上1，从而有$n_k+1$次取值为0，而有1次取值为1

修正之后，在给定$y_i = k$的情况下，时间$x_{ij} =1$的后验概率估计值为

$$\hat p(x_{ij}=1|y_i =k)=\frac{1}{n_k+2}$$
事件$x_{ij}=0$的后验概率之估计值为
$$\hat P(x_{ij} =0|y_i =k) = \frac{n_k+1}{n_k+2}$$
更一般地，在做拉普拉斯修正时，也可将$x_{ij}$的不同取值在第$k$类数据中的出现次数均加上一个很小的正数$c > 0$，而不一定限制$c = 1$。 由于拉普拉斯修正将为0的后验概率修正为正数，起着平滑的作用，故也称为**拉普拉斯平滑**(Laplace smoothing)，而$c$为拉普拉斯平滑参数。

### 案例

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import CategoricalNB
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import RocCurveDisplay
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV

spam = pd.read_csv('spam.csv')
spam.shape
pd.options.display.max_columns = 60 #展示的最大列数为60
spam.head(1) # 查看数据集的前1行
spam.spam.value_counts() # 查看spam标签的分布情况
spam.spam.value_counts(normalize=True)
```

条形图查看分布情况

```python
spam.iloc[:, :5].plot.hist(subplots=True, bins=100)
spam.iloc[:, -4:].plot.hist(subplots=True, bins=100)
```

![fix](/images/Pasted%20image%2020250720083107.png)

![fix](/images/Pasted%20image%2020250720083112.png)

 “贝叶斯最优决策”(Bayes optimal decision)通过最大化“后验概率”(posterior probability)来作预测： $$ \max_{k} \ P(y_i = k \mid \boldsymbol{x} = \boldsymbol{x}_i) $$
 $$ P(y_i = k \mid \mathbf{x} = \mathbf{x}_i) = \frac{P(\mathbf{x} = \mathbf{x}_i \mid y_i = k) P(y_i = k)}{P(\mathbf{x} = \mathbf{x}_i)} \equiv \frac{f_k(\mathbf{x}_i) \pi_k}{P(\mathbf{x} = \mathbf{x}_i)} $$

其中 $\pi_k = P(y_i = k)$为先验概率，而 $f_k(X_i) = p(X= X_i | y_i = k)$为给定类别 $y_i = k$的条件概率密度，而这可能很难估计。

特征向量 $X_i$可以是连续、离散或混合型随机向量（同时包括连续性与离散型变量），故有时无法假设 $X_i$服从多维正态分布。

$X_i$的维度也可能很高。例如，在使用“词频”(word frequency)判定“正常邮件”(email)与“垃圾邮件”(spam)时，涉及的关键词可能成千上万，而所得数据矩阵一般为高维的稀疏矩阵(sparse matrix)，故不易估计其协方差矩阵。这也是“维度灾难”(curse of dimensionality)的一种表现形式。

为简化计算，朴素贝叶斯分类起对高维的条件概率$f_k(X_i)$作了一个假定
假设在给定类别$y_i = k$的情况下，$X_i$的各分量属性之间条件独立。
$$ \begin{align*} f_k(\boldsymbol{x}_i) &\equiv P(\boldsymbol{x} = \boldsymbol{x}_i \mid y_i = k) \\ &= P(x_{i1} \mid y_i = k) \cdot P(x_{i2} \mid y_i = k) \cdots P(x_{ip} \mid y_i = k) \\ &= \prod_{j=1}^p P(x_{ij} \mid y_i = k) \end{align*} $$
其中，$p$为$\boldsymbol{x}_i = (x_{i1} \cdots x_{ip})'$的维度，即特征变量的个数。 根据朴素贝叶斯的假定，事实上将高维问题降为一维问题，因为只要分别估计$p$个单变量的条件概率$P(x_{ij} \mid y_i = k)$，其中$j = 1, \cdots, p$，然后连乘在一起即可。

```python
X = spam.iloc[:,:-1]
y = spam.iloc[:,-1]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=0)
```

**模型1 高斯朴素贝叶斯**
这里假设$X_i$服从高斯分布，即可得到高斯朴素贝叶斯。

```python
model1 = GaussianNB()
model1.fit(X_train, y_train)
model1.score(X_test, y_test) #准确度Accuracy
0.8298334540188269
```

**模型2 多项式朴素贝叶斯**
计算先验概率 统计每个类别在训练数据中出现的频率，得到先验概率 $P(C)$。假设训练数据集中有 $N$ 个样本，其中属于类别 $C_k$ 的样本有 $N_k$ 个，那么： $$P(C_k) = \frac{N_k}{N}$$计算条件概率 对于每个类别 $C_k$ 和每个特征 $X_i$，计算在该类别下特征出现的条件概率 $P(X_i \mid C_k)$ 。假设在类别 $C_k$ 中，特征 $X_i$ 出现的次数为 $n_{ik}$，类别 $C_k$ 中所有特征出现的总次数为 $\sum_i n_{ik}$，则 $$P(X_i \mid C_k) = \frac{n_{ik} + \alpha}{\sum_i n_{ik} + \alpha \cdot V}$$ 其中，$\alpha$ 是平滑参数（常用值为 1，即拉普拉斯平滑 ），$V$ 是特征的总数。添加平滑参数是为了避免当某个特征在某个类别中从未出现时，条件概率为 0 的情况。

```python
model2_1 = MultinomialNB(alpha=0)
model2_1.fit(X_train, y_train)
print(model2_1.score(X_test, y_test)) #准确度Accuracy 
0.8081100651701666

model2_2 = MultinomialNB(alpha=1)
model2_2.fit(X_train, y_train)
print(model2_2.score(X_test, y_test)) #准确度Accuracy
0.8052136133236785
```

**模型3 伯努利朴素贝叶斯**
**伯努利分布假设** 伯努利朴素贝叶斯的独特之处在于，它假设**每个特征都是二值变量**（0 或 1），且服从伯努利分布。例如： - 在文本分类中，"单词是否出现"（而非出现次数）可作为二值特征。 - 在医学诊断中，"症状是否存在"可作为二值特征。 对于类别 $C$ 和特征 $X_i$，条件概率计算为： $$ P(X_i|C) = P(X_i=1|C)^{x_i} \cdot (1-P(X_i=1|C))^{1-x_i} $$ 其中：

- $x_i$ 是样本 $X$ 的第 $i$ 个特征值（0 或 1）。
- $P(X_i=1|C)$ 是类别 $C$ 中特征 $X_i$ 出现的概率。

`alpha=1` - **作用**：平滑参数，用于防止概率计算中出现零概率问题（当某个特征在训练数据的某个类别中从未出现时）。 - **数学原理**： 在计算条件概率时，原本的公式为： $$P(X_i = 1 \mid C) = \frac{\text{类别 } C \text{ 中 } X_i=1 \text{ 的样本数}}{\text{类别 } C \text{ 的总样本数}}$$ 添加拉普拉斯平滑后变为： $$P(X_i = 1 \mid C) = \frac{\text{类别 } C \text{ 中 } X_i=1 \text{ 的样本数} + \alpha}{\text{类别 } C \text{ 的总样本数} + 2\alpha}$$ 当 `alpha=1` 时，称为**拉普拉斯平滑**；当 $0 < \alpha < 1$ 时，称为 **Lidstone 平滑**。
`binarize=0.1`

- **作用**：将连续特征自动二值化的阈值。
- 若特征值 $\leq 0.1$，则转换为 `0`。 - 若特征值 $> 0.1$，则转换为 `1`。
- 其默认值为0

```python
model3_1 = BernoulliNB(alpha=1)
model3_1.fit(X_train, y_train)
print(model3_1.score(X_test, y_test))#准确度Accuracy
model3_2 = BernoulliNB(alpha=1,binarize=0.1)
model3_2.fit(X_train, y_train)
print(model3_2.score(X_test, y_test))#准确度Accuracy
0.8834178131788559 0.9087617668356264
```

**模型4 补集朴素贝叶斯**
补集朴素贝叶斯通过计算 **“样本不属于某类别” 的概率**
来进行分类，而非直接计算 “属于某类别” 的概率。
其核心思想是：对每个类别 c，计算样本属于其他类别的条件概率之和（即 “补集” 概率）。
选择补集概率最小的类别作为预测结果（即样本最不可能属于的类别作为预测）。
补集朴素贝叶斯是处理不平衡数据和文本分类的强大工具，通过反转视角（关注 “非目标类别”）提升了对少数类的识别能力。

```python
model4 = ComplementNB()
model4.fit(X_train, y_train)
model4.score(X_test, y_test) #准确度Accuracy
0.7994207096307024
```

伯努利朴素贝叶斯参数 **网格搜索**

```python
best_score = 0
for binarize in np.arange(0.1,1.1,0.1):
 for alpha in np.arange(0.1,1.1,0.1):
  model = BernoulliNB(binarize = binarize , alpha = alpha)
  model.fit(X_train , y_train)
  score = model.score(X_test , y_test)
  if score > best_score:
   best_score = score
   best_parameters = {'binarize': binarize, 'alpha': alpha}

print(f'Best score: {best_score}, Best parameters: {best_parameters}')

Best score: 0.9131064446053584, Best parameters: {'binarize': 0.2, 'alpha': 0.2}
```

再次划分数据

```python
X_trainval,X_test,y_trainval,y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=0)

X_train,X_val,y_train,y_val = train_test_split(X_trainval,y_trainval,test_size=0.25,stratify=y_trainval,random_state=0)
```

两次抽样的核心作用是**严格分离训练、调优、评估的流程**：

- 第一次拆分测试集 → 保护最终评估的客观性。
- 第二次拆分验证集 → 为超参数调优提供无偏依据。
这是机器学习中**避免过拟合、确保模型泛化能力**的关键实践，尤其在数据类别不平衡时不可或缺。

```python
best_val_score = 0

for binarize in np.arange(0.1, 1.1, 0.1):
 for alpha in np.arange(0.1, 1.1, 0.1):
  model = BernoulliNB(binarize=binarize, alpha=alpha)
  model.fit(X_train, y_train)
  val_score = model.score(X_val, y_val)
  if val_score > best_val_score:
   best_val_score = val_score
   best_val_parameters = {'binarize': binarize, 'alpha': alpha}

print(f'Best validation score: {best_val_score}, Best parameters: {best_val_parameters}')


Best validation score: 0.9043478260869565, Best parameters: {'binarize': 0.1, 'alpha': 0.1}
```

```python
model = BernoulliNB(**best_parameters)
model.fit(X_trainval, y_trainval)
model.score(X_test, y_test) #最终测试集的准确度Accuracy

0.9153094462540716
```

K折

```python
best_score = 0
kfold = StratifiedKFold(n_splits = 10,shuffle = True , random_state = 1)
for binarize in np.arange(0.1,1.1,0.1):
 for alpha in np.arange(0.1,1.1,0.1):
  model = BernoulliNB(binarize = binarize , alpha = alpha)
  scores = cross_val_score(model,X_trainval,y_trainval,cv - kfold , scoring = 'accuracy')
  mean_score = scores.mean()
  if mean_score>best_score:
   best_score = mean_score
   best_parameters = {'binarize':binarize , 'alpha': alpha}
   
print(best_score)
print(best_parameters)
0.8991847826086957 {'binarize': 0.1, 'alpha': 0.1}
```

```python
model = BernoulliNB(**best_parameters)
model.fit(X_trainval, y_trainval)
model.score(X_test, y_test)
```

快速实现：

```python
param_grid = {
'binarize': np.arange(0.1, 1.1, 0.1),
'alpha': np.arange(0.1, 1.1, 0.1)
}

model = GridSearchCV(BernoulliNB(),param_grid,cv = kfold)
model.fit(X_trainval, y_trainval)
model.best_params_
{'alpha': 0.1, 'binarize': 0.1}
```

可视化：
