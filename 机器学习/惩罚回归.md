## 惩罚回归

### 高维回归的挑战

大数据的一种表现形式为“高维数据”，即特征向量 $X_i$ 的维度 $p$ 大于样本容量 $n$ ，也称为数据丰富的环境。
比如某研究收集了100为病人的信息，其中每位病人均有两万条基因（即两万个变量）的数据，需要研究哪些基因导致了某种病毒。假设受成本限制，样本容量 $n=100$ 难以再扩大，而变量个数 $p$ 远大于样本容量。

首先考虑传统的线性回归模型：

$$
y_i = \boldsymbol{x}_i'\boldsymbol{\beta} + \varepsilon_i = \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} + \varepsilon_i \quad (i = 1, \cdots, n)  
$$

更简洁地，写为矩阵的形式：

$$
\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}  
$$

其中，响应向量 $\boldsymbol{y} \equiv (y_1\ y_2\ \cdots\ y_n)'$ ，残差向量 $\boldsymbol{\varepsilon} \equiv (\varepsilon_1\ \varepsilon_2\ \cdots\ \varepsilon_n)'$ ，而 $n \times p$ 数据矩阵(data matrix) $\boldsymbol{X}$ 包含所有特征向量 $\boldsymbol{x}_i$ 的信息：

$$
\boldsymbol{X} \equiv \begin{pmatrix} \boldsymbol{x}_1' \\ \boldsymbol{x}_2' \\ \vdots \\ \boldsymbol{x}_n' \end{pmatrix} = \begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1p} \\ x_{21} & x_{22} & \cdots & x_{2p} \\ \vdots & \vdots & \ddots & \vdots \\ x_{n1} & x_{n2} & \cdots & x_{np} \end{pmatrix}
$$

对于高维数据，由于 $n<p$ ，故矩阵 $X$ 不满列秩（存在严格多重共线性），
因为

$$
\boldsymbol{X}的列秩 = \boldsymbol{X}的行秩 \leq n < p  
$$

故逆矩阵 $(\boldsymbol{X}'\boldsymbol{X})^{-1}$ 不存在，故 OLS 不存在唯一解，无法进行 OLS 回归。
直观上，对于 $n<p$ 的高维数据，可用来解释 $y_i$ 的特征变量 $(x_{i1},x_{i2},\dots,x_{ip})$ 很多
如果使用传统的OLS回归，虽可得到完美的样本内拟合，但外推预测的效果则可能很差。

例 假设 $n = p = 100$ 。假定这 100 个特征变量 $\boldsymbol{x}$ 与响应变量 $y$ 毫无关系（比如，相互独立），但将 $y$ 对 $\boldsymbol{x}$ 作 OLS 回归，也能得到拟合优度 $R^2 = 1$ 的完美拟合。 在特征向量 $\boldsymbol{x}$ 所存在的 100 维空间中，最多只可能有 100 个线性无关的向量，而加入 $\boldsymbol{y} \equiv (y_1\ y_2\ \cdots\ y_n)'$ 之后，必然导致线性相关，即 $\boldsymbol{y}$ 可由这 100 个特征变量所线性表出，故残差为 0，而 $R^2 = 1$ 。

根据此样本数据估计的回归函数，将毫无外推预测的价值；因为 $y$ 与 $x$ 事实上相互独立。
这种拟合显然过度了，故名“过拟合”(overfit)，因为它不仅拟合了数据中的信号(signal)，而且拟合了数据中的很多噪音(noise)。
在此极端例子中，由于数据中全是噪音而毫无信号，故 OLS 完美地拟合了数据中的噪音，自然毫无意义

对于低维数据，样本容量大于变量个数（ $n>p$ )，一般很少出现严格多重共线性。即使偶然出现，只要将多余变量去掉就行。

对于 $n<p$ 的高维数据，则严格多重共线性成为常态。
比如，对于n维数据，在任意（ $n+1$ ）个变量之间，一般就存在严格多重共线性。
此时，简单地丢掉导致多重共线性的变量将无济于事，因为需要扔掉很多变量。
比如，假设样本为 100 个病人，但有 2 万个基因的变量，如果通过去掉变量消除严格多重共线性，则难免将婴儿与洗澡水一起倒掉。

什么是严格多重共线性？数学表达为：存在一组不全为 0 的系数 $\lambda_0, \lambda_1, \dots, \lambda_p$ ，使得 $\lambda_0 + \lambda_1 x_{i1} + \lambda_2 x_{i2} + \dots + \lambda_p x_{ip} = 0 \quad (i = 1,2,\dots,n)$
通俗说，就是某特征可被其他特征 “完美线性表示”，破坏了 OLS 回归对 “特征线性无关” 的假设（OLS要求特征向量线性无关）。

### 岭回归

作为高维回归的方法之一，岭回归(Ridge Regression)最早由 Hoerl and Kennard (1970)提出，其出发点正是为了解决多重共线性。

当时还几乎没有高维数据。在传统的低维回归(low-dimensional regression)，虽然严格多重共线性很少见，但近似(不完全)的多重共线性却不时出现，即特征变量（ $x_1 \dots x_p$ ）之间高度相关，比如相关系数大于 0.9。

矩阵 $\mathbf{X}'\mathbf{X}$ 变得“几乎”不可逆(类似于 1 除以非常小的数)，导致 $(\mathbf{X}'\mathbf{X})^{-1}$ 变得很大，使得 OLS 估计量 $\hat{\boldsymbol{\beta}}_{OLS} \equiv (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}$ 的方差也变得很大。
Hoerl and Kennard (1970)的解决方案为，在矩阵 $\mathbf{X}'\mathbf{X}$ 的主对角线上都加上某常数 $\lambda > 0$ ，以缓解多重共线性，使所得矩阵 $(\mathbf{X}'\mathbf{X} + \lambda\mathbf{I})$ 变得“正常”。由此可得岭回归估计量：

$$
\hat{\boldsymbol{\beta}}_{ridge} \equiv (\mathbf{X}'\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}'\mathbf{y} \tag{9.5}  
$$

岭回归只是在 OLS 表达式中加入“山岭” $\lambda\mathbf{I}$ ，故名“岭回归”(ridge regression)。 其中，参数 $\lambda$ 称为调节参数(tuning parameter)。

由于 OLS 估计量是无偏的(unbiased)（线性回归的两个假设 扰动项的期望 $\epsilon$ ）
即 $\mathrm{E}(\hat{\boldsymbol{\beta}}_{OLS}) = \boldsymbol{\beta}$ ，故凭空加上此 “山岭” $\lambda\mathbf{I}$ 后，岭回归估计量其实是有偏的(biased)，其偏差为

$$
\text{Bias}(\hat{\boldsymbol{\beta}}_{\text{ridge}}) \equiv \text{E}(\hat{\boldsymbol{\beta}}_{\text{ridge}}) - \boldsymbol{\beta} \neq \boldsymbol{0}  
$$

我们的目标是最小化均方误差(Mean Squared Errors，简记 MSE)，并不强求无偏估计。 对于任意估计量 $\hat{\boldsymbol{\beta}}$ ，其均方误差可分解为方差与偏差平方之和(参见附录 A9.1)：
**偏差表达式**： $\text{Bias}(\hat{\boldsymbol{\beta}}_{\text{ridge}}) = \mathrm{E}(\hat{\boldsymbol{\beta}}_{\text{ridge}}) - \boldsymbol{\beta} = \left[ (\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{X} - \mathbf{I} \right] \boldsymbol{\beta}.$ 当 $\lambda > 0$ 时，矩阵 $(\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{X} \neq \mathbf{I}$ ，因此 $\text{Bias} \neq \boldsymbol{0}$ 。

我们的目标是最小化均方误差(Mean Squared Errors，简记 MSE)，并不强求无偏估计。

对于任意估计量 $\hat{\boldsymbol{\beta}}$ ，其均方误差可分解为方差与偏差平方之和

$$
\begin{align*} \text{MSE}(\hat{\beta}) &\equiv \mathrm{E}(\hat{\beta} - \beta)^2 = \mathrm{E}\left[ \hat{\beta} - \mathrm{E}(\hat{\beta}) + \mathrm{E}(\hat{\beta}) - \beta \right]^2 \quad \text{(加、减} \mathrm{E}(\hat{\beta}) \text{)} \\ &= \mathrm{E}\left[ \hat{\beta} - \mathrm{E}(\hat{\beta}) \right]^2 + \mathrm{E}\left[ \mathrm{E}(\hat{\beta}) - \beta \right]^2 + 2\mathrm{E}\left\{ \left[ \hat{\beta} - \mathrm{E}(\hat{\beta}) \right] \left[ \mathrm{E}(\hat{\beta}) - \beta \right] \right\} \\ &= \text{Var}(\hat{\beta}) + \left[ \text{Bias}(\hat{\beta}) \right]^2 + \underbrace{2\mathrm{E}\left\{ \left[ \hat{\beta} - \mathrm{E}(\hat{\beta}) \right] \left[ \mathrm{E}(\hat{\beta}) - \beta \right] \right\}}_{=0} \end{align*}  $$ 其中，上式的交叉项为 0：  $$ \mathrm{E}\left\{ \left[ \hat{\beta} - \mathrm{E}(\hat{\beta}) \right] \left[ \mathrm{E}(\hat{\beta}) - \beta \right] \right\} = \left[ \mathrm{E}(\hat{\beta}) - \beta \right] \cdot \mathrm{E}\left[ \hat{\beta} - \mathrm{E}(\hat{\beta}) \right] = \left[ \mathrm{E}(\hat{\beta}) - \beta \right] \cdot 0 = 0  
$$

在一维情况下，此分解公式可简化为

$$
\text{MSE}(\hat{\beta}) = \text{Var}(\hat{\beta}) + \left[ \text{Bias}(\hat{\beta}) \right]^2 \tag{9.8}  
$$

使均方误差最小化，可视为在方差与偏差之间进行权衡(trade-off)。比如，一个无偏估计量(偏差为 0)，如果方差很大，则可能不如一个虽然有偏差但方差却很小的估计量。

使均方误差最小化，可视为在方差与偏差之间进行权衡(trade-off)。比如，一个无偏估计量(偏差为 0)，如果方差很大，则可能不如一个虽然有偏差但方差却很小的估计量。![fix](/images/Pasted%20/images/image%2020250718083659.png)

在(严格)多重共线性的情况下，虽然 OLS 估计量无偏，但其方差太大(无穷大)，而岭回归虽有少量偏差，但可大幅减少方差，使得岭回归估计量的均方误差(MSE)可能比 OLS 更小

岭回归的理论基础是什么？  
在严格多重共线性的情况下，不存在唯一的 OLS 估计量 $\hat{\boldsymbol{\beta}}_{OLS}$ 。  
这意味着有许多 $\hat{\boldsymbol{\beta}}_{OLS}$ ，都能使残差平方和等于 0，而拟合优度 $R^2 = 1$ 。  
为得到参数向量 $\boldsymbol{\beta}$ 的唯一解，须对其取值范围有所限制，进行所谓“正则化”(加调节参数）。  

为此，考虑在损失函数(loss function)中加入“惩罚项”(penalty term)，进行惩罚回归(penalized regression)：

$$
\min_{\boldsymbol{\beta}} \ L(\boldsymbol{\beta}) = \underbrace{(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})}_{\text{SSR}} + \underbrace{\lambda \|\boldsymbol{\beta}\|_2^2}_{\text{penalty}} \tag{9.9}  
$$

其中，损失函数的第 1 项依然为残差平方和(SSR)。 第 2 项为惩罚项，也称为“正则项”。 $\lambda \geq 0$ 为“调节参数”(tuning parameter)，控制惩罚的力度。
$\|\boldsymbol{\beta}\|_2$ 为参数向量 $\boldsymbol{\beta}$ 的长度，即 $\boldsymbol{\beta}$ 到原点的欧氏距离，也称为 “2-范数”（ $L_2$ norm）：  
$$
\|\boldsymbol{\beta}\|_2 \equiv \sqrt{\beta_1^2 + \cdots + \beta_p^2}
$$
将惩罚项写为 $\lambda\|\boldsymbol{\beta}\|_2^2 = \lambda(\beta_1^2 + \cdots + \beta_p^2) = \lambda\boldsymbol{\beta}'\boldsymbol{\beta}$ ，则损失函数可写为  
$$
\min_{\boldsymbol{\beta}} \ L(\boldsymbol{\beta}) = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) + \lambda\boldsymbol{\beta}'\boldsymbol{\beta}
$$
由于惩罚项 $\lambda\boldsymbol{\beta}'\boldsymbol{\beta}$ 只是简单的二次型，使用向量微分规则可得一阶条件：  

$$
\frac{\partial L(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = -2\mathbf{X}'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) + 2\lambda\boldsymbol{\beta} = \boldsymbol{0}  $$ 经移项整理可得：  $$ (\mathbf{X}'\mathbf{X} + \lambda\mathbf{I})\boldsymbol{\beta} = \mathbf{X}'\mathbf{y}  $$ 所得最优解正是岭回归估计量：  $$ \hat{\boldsymbol{\beta}}_{\text{ridge}}(\lambda) \equiv (\mathbf{X}'\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}'\mathbf{y}  
$$

### 岭回归的计算

由于单位可能带来很大影响，在进行岭回归(或其他形式的惩罚回归)时，一般先将每个变量 $x_j \ (j=1,\cdots,p)$ 标准化，即减去其均值 $\bar{x}_j$ ，再除以标准差 $\text{sd}(x_j)$ ，然后使用标准化的变量 $\frac{(x_j - \bar{x}_j)}{\text{sd}(x_j)}$ 进行岭回归。 在标准化之后，特征变量的样本均值为 0。

在标准化之后，特征变量的样本均值为 0。

对响应变量 $\boldsymbol{y}$ 进行中心化，即变换为 $(\boldsymbol{y} - \overline{\boldsymbol{y}})$ ，故响应变量的样本均值也为 0。 可以证明，如果不惩罚常数项，则对于常数项的岭回归估计就是响应变量 $\boldsymbol{y}$ 的样本均值（中心化之前），故常数项不必放入岭回归方程中。中心化操作已经在一定程度上分离了常数项所代表的信息，使得常数项的估计值就是原始响应变量的均值 。如果对常数项进行惩罚，会破坏这种原本合理的对应关系，导致模型无法准确拟合数据的基线水平。

以一元回归为例：（这里的 $x_i$ 是经过标准化处理后的）

$$
y_i = \alpha + \beta x_i + \varepsilon_i  
$$

岭回归的目标函数为

$$
\min_{\alpha, \beta} \ L(\alpha, \beta) = \sum_{i=1}^n (y_i - \alpha - \beta x_i)^2 + \lambda \beta^2  
$$

其中， $\lambda \geq 0$ ，且不惩罚常数项 $\alpha$ 。对 $\alpha$ 求偏导数可得一阶条件：

$$
\begin{align*} \frac{\partial L}{\partial \alpha} &= \frac{\partial \sum_{i=1}^n (y_i - \alpha - \beta x_i)^2}{\partial \alpha} + \underbrace{\frac{\partial (\lambda \beta^2)}{\partial \alpha}}_{=0} \\ &= -2 \sum_{i=1}^n (y_i - \alpha - \beta x_i) = 0 \end{align*}  
$$

在上式两边同除（ $-2n$ ），并移项可得：

$$
0 = \overline{y} = \hat{\alpha} + \hat{\beta} \underbrace{\overline{x}}_{=0} = \hat{\alpha}  
$$

在上式中，由于将变量标准化或中心化， $\overline{x} = 0$ ， $\overline{y} = 0$ ，故在不惩罚常数项的情况下，对常数项的岭回归估计量为 $\hat{\alpha} = 0$ 。

这意味着，无须在岭回归中放入常数项。

### 岭回归的几何解释

由于岭回归的目标函数包含对过大参数的惩罚项，故岭回归为收缩估计量

调节参数 $\lambda$ 也称收缩参数(shrinkage parameter)。与 OLS 估计量相比，岭回归估计量更为向原点收缩。
可以这么理解：要求一个适当的 $\beta$ 使损失函数尽可能小，若参数过大，惩罚项会 “惩罚” 它（总损失上升 ）； 若参数太小，可能拟合不好数据（残差平方和上升 ）。
这就使得模型在优化时，不仅要考虑最小化残差平方和，还要兼顾让参数值尽可能小。相比无惩罚的 OLS，参数会被 “拉小”（向 0 收缩 ）。

岭回归的目标函数可等价地写为如下有约束的极值问题：

$$
\min_{\boldsymbol{\beta}} \ (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})   $$$$ \text{s.t. } \|\boldsymbol{\beta}\|_2^2 \leq t  $$ 其中， $t \geq 0$ 为某常数。 对于此约束极值问题，可引入拉格朗日乘子函数，并以 $\lambda$ 作为其乘子：  $$ \min_{\boldsymbol{\beta}, \lambda} \ \tilde{L}(\boldsymbol{\beta}) = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) + \lambda\left( \|\boldsymbol{\beta}\|_2^2 - t \right)
$$

下面从几何角度考察约束极值问题(9.19)。其目标函数为残差平方和， OLS 相同。
在 $\boldsymbol{\beta}$ 的参数空间(parameter space)，约束条件 $\|\boldsymbol{\beta}\|_2^2 \leq t$ 对应于以 $\sqrt{t}$ 为半径的圆球之内部。 以二元回归为例，则 $\boldsymbol{\beta} = (\beta_1\ \beta_2)'$ ，约束条件 $\|\boldsymbol{\beta}\|_2^2 \leq t$ 可写为 $\beta_1^2 + \beta_2^2 \leq t$ 。如图所示

![fix](/images/Pasted%20/images/image%2020250718232742.png)

满足约束条件 $\beta_1^2 + \beta_2^2 \leq t$ 的可行解(feasible solutions)，在以 $\sqrt{t}$ 为半径的圆球之内部。
图中黑点为OLS估计量 $\hat{\boldsymbol{\beta}}_{OLS}$ (如OLS估计量不唯一，则构成一个集合)。
围绕 $\hat{\boldsymbol{\beta}}_{OLS}$ 的一圈圈椭圆为残差平方和的“等值线”(contour)或“水平集”(level set)。
直观上，可将 $\hat{\boldsymbol{\beta}}_{OLS}$ 想象为“山谷”的最低点，而越靠近 $\hat{\boldsymbol{\beta}}_{OLS}$ 的等值线，其残差平方和越低。
在可行集中最小化残差平方和，其最优解必然发生于等值线与圆周相切位置，如图中蓝点所示。

### 套索估计值( LASSO)

在进行高维回归时，有时希望从大量的特征变量中，筛选出真正对 y有影响的少数变量。
希望从 2 万个基因中，找到真正影响疾病的少数基因。
需要一个估计量，能挑选出那些真正有影响的(基因)变量，而使其他无影响或影响微弱的(基因)变量的回归系数变为 0。

LASSO将岭回归惩罚项的 2-范数改为 1-范数：

$$
\min _{\boldsymbol{\beta}} L(\boldsymbol{\beta})=\underbrace{(\mathbf{y}-\mathbf{X} \boldsymbol{\beta})^{\prime}(\mathbf{y}-\mathbf{X} \boldsymbol{\beta})}_{S S R}+\underbrace{\lambda\|\boldsymbol{\beta}\|_{1}}_{\text {penalty }}  
$$

其中， $\|\boldsymbol{\beta}\|_{1}=\sum_{j=1}^{p}\left|\beta_{j}\right|$ 为参数向量 $\boldsymbol{\beta}$ 的 1-范数( $L_1$ norm)，即 $\boldsymbol{\beta}$ 各分量的绝对值之和。
此损失函数包括惩罚项，故Lasso也是收缩估计量，类似岭回归的绝对值收敛。

Lasso 最小化问题也可等价地写为如下约束极值问题。

$$
\min _{\boldsymbol{\beta}} (\mathbf{y}-\mathbf{X} \boldsymbol{\beta})^{\prime}(\mathbf{y}-\mathbf{X} \boldsymbol{\beta}) \\ \text { s.t. }\|\boldsymbol{\beta}\|_{1} \leq t  
$$

其中， $t \geq 0$ 为某常数。
此约束极值问题的约束集不再是圆球，而是菱形(钻石形)或高维的菱状体。
以二元回归为例，则 $\beta = (\beta_1,\beta_2)$

约束条件 $\|\boldsymbol{\beta}\|_1 \leq t$ 可写为 $\vert \beta_1 \vert + \vert \beta_2 \vert \leq t$ 为菱形。
![fix](/images/Pasted%20/images/image%2020250718233744.png)
类似上文岭回归，最优解必然发生与等值线与菱形相切位置。如蓝点所示。

Lasso 与岭回归孰优孰劣？
从预测的角度，如果真实模型(或数据生成过程)确实是稀疏的，则 Lasso一般更优。
但如果真实模型并不稀疏，则岭回归的预测效果可能优于 Lasso。

从模型易于解释(interpretability)的角度，则 Lasso 显然是赢家，因为岭回归一般只是收缩回归系数，并不具备变量筛选的功能。

### 套索估计量的计算

绝对值函数并不光滑，使得Lasso的目标函数不可微，故一般情况下不存在解析解。
目前最有效的Lasso算法为坐标下降法：依次沿着一个坐标轴的方向进行最优化，使得损失函数下降，直至最低点。假设损失函数为

$$
L(\boldsymbol{\beta}) = f(\beta_1, \beta_2, \dots, \beta_p)
$$

假定在迭代过程中， $\boldsymbol{\beta}$ 的当前取值为 $(\beta_1^*, \beta_2^*, \dots, \beta_p^*)$ 。
首先，给定 $(\beta_2^*, \dots, \beta_p^*)$ ，将函数 $f(\beta_1, \beta_2^*, \dots, \beta_p^*)$ 针对 $\beta_1$ 进行一元最小化，得到最优解 $\beta_1^{**}$ 。
其次，给定 $(\beta_1^{**}, \beta_3^*, \dots, \beta_p^*)$ ，将函数 $f(\beta_1^{**}, \beta_2, \beta_3^*, \dots, \beta_p^*)$ 针对 $\beta_2$ 进行一元最小化，得到最优解 $\beta_2^{**}$ 。
（坐标下降法示意图）

![fix](/images/Pasted%20/images/image%2020250719122316.png)
对于一维的 Lasso 问题，它实际上是一个单变量的优化问题，在这种情况下可以推导出解析解。

考虑一维（单变量）Lasso问题

$$
\min _{\beta} \ L(\beta)=\sum_{i=1}^{n}\left(y_{i}-\beta x_{i}\right)^{2}+\lambda|\beta|  
$$

虽然无法对上式中的绝对值 $|\beta|$ 求导数，但可求其“次导数”(subderivative)，进而得到“次微分”(subdifferential)。 由此可证明，一元 Lasso 问题的最优解可写为 OLS 估计量 $\hat{\beta}_{OLS}$ 的函数：

$$
\hat{\beta}_{lasso} = sign(\hat{\beta}_{OLS})\cdot\left(\left|\hat{\beta}_{OLS}\right| - \lambda/2\right)_+ \tag{9.26}  $$其中， $\boldsymbol{sign(\cdot)}$ 为“符号函数”(sign function)，即  $$ sign(x)= \begin{cases} 1 & if\ x>0 \\ 0 & if\ x=0 \\ -1 & if\ x<0 \end{cases}  $$ 而“ $(\cdot)_+$ ”为取“正部”(positive part)的运算，即  $$ (x)_+ = \begin{cases} x & if\ x \geq 0 \\ 0 & if\ x < 0 \end{cases}  
$$

在机器学习中，函数“ $(\cdot)_+$ ”也称为修正线性单元(Rectified Linear Unit, 简记 ReLU)或线性整流函数，广泛用于人工神经网络![fix](/images/Pasted%20/images/image%2020250719152241.png)
于是 Lasso 的最优解 $\hat{\beta}_{Lasso}$ 写为

$$
\hat{\beta}_{Lasso} = \begin{cases} \hat{\beta}_{OLS} - \lambda/2 & if\ \hat{\beta}_{OLS} > \lambda/2 \\ 0 & if\ \vert \hat{\beta}_{OLS} \vert \leq \lambda/2 \\ \hat{\beta}_{OLS} + \lambda/2 & if\ \hat{\beta}_{OLS} < -\lambda/2 \end{cases} \tag{9.29}  
$$

上式将 Lasso 估计量 $\hat{\beta}_{Lasso}$ 表示为 OLS 估计量 $\hat{\beta}_{OLS}$ 的分段函数
如果 $\left|\hat{\beta}_{OLS}\right| \leq \lambda/2$ ，则 Lasso 直接将 OLS 估计量 $\hat{\beta}_{OLS}$ 收缩至 0。 如果 $\hat{\beta}_{OLS} > \lambda/2$ ，则 Lasso 使 OLS 估计量向原点收缩 $\lambda/2$ ，即

$$
\hat{\beta}_{lasso} = \hat{\beta}_{OLS} - \lambda/2 $$ 如果 $\hat{\beta}_{OLS} < -\lambda/2$ ，则 Lasso 使 OLS 估计量向原点收缩 $\lambda/2$ ，即  $$\hat{\beta}_{lasso} = \hat{\beta}_{OLS} + \lambda/2
$$

故 Lasso 也称为软门限算子。
![fix](/images/Pasted%20/images/image%2020250719152510.png)

### 调节变量的选择

无论岭回归，还是 Lasso 估计量，其最优解都是调节参数 $\lambda$ 的函数，可写为 $\hat{\boldsymbol{\beta}}_{\text{ridge}}(\lambda)$ 或 $\hat{\boldsymbol{\beta}}_{\text{lasso}}(\lambda)$ 。

变动调节参数 $\lambda$ (调节惩罚力度), 即可得到整条“解的路径”(solution path)或“系数路径” (coefficient path)。
也可将惩罚回归的最优解写为 $L_2$ 或 $L_1$ 范数 $t$ 的函数，即 $\hat{\boldsymbol{\beta}}_{\text{ridge}}(t)$ 或 $\hat{\boldsymbol{\beta}}_{\text{lasso}}(t)$ ，参见约束极值问题。

选择最优 $\lambda$ 的常见方法为**K折交叉验证**：将全样本随机分为大致相等的 10 个子样本，即 10 折。然后，以其中的 9 折作为训练集，进行惩罚回归(岭回归或 Lasso)，并以所得模型预测作为验证集的其余 1 折，得到该折的均方误差。
得到每一折的均方误差：

$$
\text{MSE}_k(\lambda) \equiv \frac{1}{n_k} \sum_{i \in \text{fold}_k} \left( y_i - \hat{y}_i(\lambda) \right)^2, \quad k = 1, \dots, K  
$$

其中， $n_k$ 为第 $k$ 折的样本容量。 $\hat{y}_i(\lambda)$ 是9折训练集进行惩罚回归的预测值，而 $y_i$ 是1折验证集的数据。

将这10折的均方误差进行平均，即可得到交叉验证误差.

$$
\text{CV}(\lambda) \equiv \overline{\text{MSE}}(\lambda) \equiv \frac{1}{K} \sum_{k=1}^{K} \text{MSE}_k(\lambda)  
$$

这是 $\lambda$ 的函数，故可选择最优 $\lambda$ ，使结果最小化。

$$
\hat{\lambda} = \underset{\lambda}{\text{argmin}} \ \text{CV}(\lambda)  
$$

在实践中，可以对调节变量 $\lambda$ 进行网格化处理。

首先，由于 $\lambda \geq 0$ ，故 $\lambda$ 的最小值为 0。  

其次，如果 $\lambda$ 足够大，则对于 $\|\boldsymbol{\beta}\|_2^2$ 或 $\|\boldsymbol{\beta}\|_1$ 的惩罚非常严厉，可使 $\hat{\boldsymbol{\beta}}_{\text{ridge}}$ 或 $\hat{\boldsymbol{\beta}}_{\text{lasso}}$ 变为 0。记刚好能使 $\hat{\boldsymbol{\beta}}_{\text{ridge}}$ 或 $\hat{\boldsymbol{\beta}}_{\text{lasso}}$ 变为 0 的调节参数取值为 $\lambda_{\text{max}}$ 。  

故只要将区间 $\big[0, \lambda_{\text{max}}\big]$ 进行网格等分(比如 100 等分)，然后在每个等分点上计算 $\text{CV}(\lambda)$ ，即可求得最优的 $\hat{\lambda}$ 。  

$\text{CV}(\lambda)$ 一般为凸函数， $\lambda$ 太小或太大均不利于最小化，故最优的 $\hat{\lambda}$ 位于中间区域。
![fix](/images/Pasted%20/images/image%2020250719154838.png)
进一步，由于在 $\lambda$ 的每个网格点，均计算了 $\text{MSE}_1(\lambda), \dots, \text{MSE}_{10}(\lambda)$ ，共有 10 个数，故可计算相应的样本标准差，记为 $sd_{\text{MSE}}(\lambda)$ ：

$$
sd_{\text{MSE}}(\lambda) \equiv \sqrt{ \frac{1}{9} \sum_{k=1}^{10} \big[ \text{MSE}_k(\lambda) - \overline{\text{MSE}}(\lambda) \big]^2 }
$$

### 弹性网估计量

Lasso 虽然具有筛选变量的功能，但此功能并不完美。比如，当几个变量高度相关时，Lasso 可能随意选择其中一个。

弹性网估计量：将 Lasso 与岭回归相结合
在弹性网估计量的损失函数中，同时包含 $L_1$ 与 $L_2$ 惩罚项：

$$
\min_{\boldsymbol{\beta}} \ (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) + \lambda_1 \|\boldsymbol{\beta}\|_1 + \lambda_2 \|\boldsymbol{\beta}\|_2^2  
$$

其中， $\lambda_1 \geq 0$ 与 $\lambda_2 \geq 0$ 都是调节参数。

由于 $\lambda_1$ 与 $\lambda_2$ 的取值范围均为无穷，不便于使用交叉验证选择其最优值。为此，定义 $\lambda \equiv \lambda_1 + \lambda_2$ ， $\alpha \equiv \lambda_1 / \lambda$ ，可将损失函数等价写为

$$
\min_{\boldsymbol{\beta}} \ (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) + \lambda \big[ \alpha \|\boldsymbol{\beta}\|_1 + (1 - \alpha) \|\boldsymbol{\beta}\|_2^2 \big]
$$

其中， $\lambda \geq 0$ 与 $0 \leq \alpha \leq 1$ 为调节参数。 由于调节参数 $\alpha$ 的取值局限于区间 $\big[0, 1\big]$ ，故便于通过交叉验证选择其最优值。

如果 $\alpha = 0$ ，则弹性网退化为岭回归。如果 $\alpha = 1$ ，则弹性网退化为 Lasso。故岭回归与 Lasso 都是弹性网的特例。 如果 $0 < \alpha < 1$ ，则弹性网为岭回归与 Lasso 之间的折衷。 上式可等价地写为以下约束极值问题：

$$
\begin{cases} \min_{\boldsymbol{\beta}} & (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})'(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) \\ \text{s.t.} & \alpha \|\boldsymbol{\beta}\|_1 + (1 - \alpha) \|\boldsymbol{\beta}\|_2^2 \leq t \end{cases}
$$

其中， $t \geq 0$ 为调节参数。

仍以二元回归为例， $\boldsymbol{\beta} = (\beta_1\ \beta_2)'$ ，则弹性网估计量的约束集为

$$
\alpha(|\beta_1| + |\beta_2|) + (1 - \alpha)(\beta_1^2 + \beta_2^2) \leq t  
$$

![fix](/images/Pasted%20/images/image%2020250719181258.png)

### 案例
