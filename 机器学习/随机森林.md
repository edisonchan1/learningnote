---
title: 随机森林
date: 2025-07-25
updated: 2025-07-25
description:
---

### 集成学习

集成学习的基本问题：如果有一些预测效果一般的“弱学习器”，能否以某种方式将它们组合起来，构成一个预测效果优良的“强学习器”？

用于集成学习的弱学习器也称为“基学习器”。决策树是最常见的基学习器。

给定数据与算法，则监督学习的估计效果是基本确定的（可能由于随机分组的不同而略有差异），这可表示为：数据+算法= 结果。

要得到不同的结果，一种方法是给定数据，使用不同的算法，然后将所得的不同结果组合在一起：比如加权平均，而权重则以交叉验证来确定。

以回归问题为例，假设共有三种不同的算法（比如线性回归、KNN与决策树），其预测结果分别为 $\hat f_1(X)$，$\hat f_2(X)$，$\hat f_3(X)$；则继承学习的最终预测结果为：

$$
\hat f(X) = \alpha_1 \hat f_1(X) +\alpha_2 \hat f_2(X) +(1-\alpha_1 -\alpha_2)\hat f_3(X)
$$

其中，$\alpha_1 \geq0$，$\alpha_2 \geq0$，且 $\alpha_1 + \alpha_2 \leq 1$。最优调节参数 $\alpha_1$ 与 $\alpha_2$ 通过交叉验证来确定。

由于算法 $\hat f_1(X)$，$\hat f_2(X)$ 与 $\hat f_3(X)$ 皆为集成算法 $\hat f(X)$ 的特例，故 $\hat f(X)$ 的预测效果肯定优于这三种单独的算法。

对于分类问题，采用 “多数投票法”（多个基学习器的预测结果中，出现次数最多的类别为最终结果）；对于回归问题，采用 “均值法”（多个基学习器的预测值的平均值为最终结果）。

集成学习的另一方式为，给定算法(比如以决策树的 CART 算法为基学习器)，然后“搅动数据”，得到不同的决策树模型，再组合在一起，即所谓“Perturb + Combine”的模式。

### 装袋法

Breiman(1996)提出“装袋法”(bootstrap aggregating，简记 bagging)。

装袋法的具体步骤如下：

首先，对训练数据进行又放回的再抽样，得到B个自助样本，比如 $B = 500$。其中，第 $b$ 个自助样本可写为

$$
\left\{ \mathbf{x}_i^{*b}, y_i^{*b} \right\}_{i=1}^n, \quad b = 1, \cdots, B
$$

其中，上标星号表示自助样本。每个自助样本的样本容量均为 $n$（样本容量不变）。

其次，根据自助样本 $\left\{ \mathbf{x}_i^{*b}, y_i^{*b} \right\}_{i=1}^B$ 估计 $B$ 课不同的决策树，不进行修枝。记起预测结果为

$$
\left\{ \hat{f}^{*b}(\mathbf{x}) \right\}, \quad b = 1, \cdots, B
$$

最后，对于回归树，将 $B$ 课决策树的预测结果进行平均：

$$
\hat{f}_{\text{bag}}(\mathbf{x}) = \frac{1}{B} \sum_{b=1}^B \hat{f}^{*b}(\mathbf{x})
$$

对于分类树，装袋法的预测结果为：

$$
\hat{f}_{\text{bag}}(\mathbf{x}) = \underset{y \in \{1, \cdots, K\}}{\arg\max} \sum_{b=1}^B I\left( y = \hat{f}^{*b}(\mathbf{x}) \right)
$$

其中，假设 $y \in \{1, \cdots, K\}$，共分为 $K$ 类。$I(\cdot)$ 为示性函数（indicator function），判断是否 “$y = \hat{f}^{*b}(\mathbf{x})$”；如果是，则记为 1，否则记为 0。

公式解释：
该公式表示装袋法（Bagging）在分类问题中的集成预测方式。对于输入 $\mathbf{x}$，最终的预测类别 $\hat{f}_{\text{bag}}(\mathbf{x})$ 是使得指示函数 $I(y = \hat{f}^{*b}(\mathbf{x}))$ 之和最大的类别 $y$。

$y \in {1, \cdots, K}$，共分为 $K$ 类。
$I(\cdot)$ 为示性函数（indicator function），当括号内条件成立时取值为 1，否则为 0。
$\hat{f}^{*b}(\mathbf{x})$ 表示第 $b$ 棵树对 $\mathbf{x}$ 的预测结果。
$\sum_{b=1}^B$ 表示对 $B$ 棵树的投票计数。
$\arg\max$ 表示取使得投票数最多的类别。

通过多数票，将许多弱分类器进行组合。由于装袋法把很多自助样本的估计结果汇总，故名bagging。

### 装袋法的原理

由于bagging将很多决策树进行平均，故可降低估计量的方差，从而提高模型的预测准确率。

例如，假设随机变量 $\{z_1, \cdots, z_n\}$ 为独立同分布（iid），而方差为 $\sigma^2$，则样本均值 $\overline{z} = \frac{1}{n} \sum_{i=1}^n z_i$ 的方差可缩小 $n$ 倍：

$$
\mathrm{Var}(\overline{z}) = \mathrm{Var}\left( \frac{1}{n} \sum_{i=1}^n z_i \right) = \frac{1}{n^2} \sum_{i=1}^n \mathrm{Var}(z_i) = \frac{\sigma^2}{n}
$$

其中，由于 $\{z_1, \cdots, z_n\}$ 相互独立，故上述式中的协方差均为 0，而所有 $\mathrm{Var}(z_i) = \sigma^2$（同分布）。

另一方面，由于装袋法并不修枝，而让决策树尽情生长，故可降低每棵决策树的偏差，然后通过bagging来控制方差。

在一定条件下，装袋法可以同时降低偏差和方差，故可减小均方误差或总误差。

应该在什么时候使用bagging？

Bagging特别适合于方差较大的不稳定估计量，比如决策树、线性回归的变量子集选择等。

对于**不稳定估计量，当样本数据轻微扰动时，则可能引起估计结果较大的变化**。

比如，当数据轻微变化时，所得决策树结构可能发生较大的改变。

又比如，当用线性回归使用变量子集选择时，所选变量也对样本数据比较敏感，并不稳健。

而Bagging对于方差较小的稳定估计量，比如KNN，则作用不大。对于KNN而言，即使搅动数据，一般也不会对附近K近邻的平均结果产生明显影响。

由于单棵决策树为分段常值函数，故并不连续。

将很多决策树进行平均，则可得到更为连续的回归函数或决策边界，从而提高预测性能。

### 袋外误差

Bagging还提供了估计测试误差的简便方法，可以不用测试集或交叉验证。

由于进行有放回的再抽样，每棵树都有些观测值未使用，称为“套外观测值”，可作为测试集。

例如：对于任意观测值 $X_i$，它未出现于大约 $\frac{B}{3}$ 的决策树，故对于这些决策树而言是套外观测值。

将 $X_i$ 代入这些决策树的预测结果进行平均（或多数投票），记为对第 $i$ 个观测值的袋外预测值 $\hat y_i$。由此可得对所有观测值的袋外预测值由此可得对所有观测值的袋外预测值 $\left\{ \hat{y}_{i, \mathrm{OOB}} \right\}_{i=1}^n$。

将袋外观测值 $\hat y_i$ 与实际观测值 $y_i$ 对比，即可得到**袋外误差**。

对于回归问题，袋外误差为**袋外均方误差**：

$$
\mathrm{MSE}_{\mathrm{OOB}} \equiv \frac{1}{n} \sum_{i=1}^n \left( \hat{y}_{i, \mathrm{OOB}} - y_i \right)^2
$$

对于分类问题，袋外误差为**袋外错误率**：

$$
\mathrm{Err}_{\mathrm{OOB}} \equiv \frac{1}{n} \sum_{i=1}^n I\left( \hat{y}_{i, \mathrm{OOB}} \neq y_i \right)
$$

如果自助样本的数目 $B$ 足够大，则袋外误差与“留一交叉验证误差”几乎等价。对于样本容量很大或变量很多的大数据，尤其方便实用袋外误差（可节省运算事件），作为对测试误差的估计。

对于回归问题，还可以根据袋外均方误差计算准 $R^2$，即在响应变量 $y$ 的样本方差中，有很大比例可由模型解释：

$$
\text{Pseudo } R^2 \equiv \frac{\widehat{\mathrm{Var}}(y) - \mathrm{MSE}_{\mathrm{OOB}}}{\widehat{\mathrm{Var}}(y)} = 1 - \frac{\mathrm{MSE}_{\mathrm{OOB}}}{\widehat{\mathrm{Var}}(y)}
$$

其中，$\widehat{\mathrm{Var}}(y)$ 为响应变量 $y$ 的样本方差。

### 随机森林

如果 $\{z_1, \cdots, z_n\}$ 为 iid，则其样本均值 $\overline{z}$ 的方差可缩小 $n$ 倍。

但如果 $\{z_1, \cdots, z_n\}$ 之间相关，则样本均值 $\overline{z}$ 的方差一般不会缩小 $n$ 倍。

所以希望这些决策树互不相关，才更有互补性。

在使用套袋法时，由于所用算法完全相同，而每棵决策树所用自助样本都来自同样的原始数据，故存在较高的相关性。

如果想进一步提高Bagging的预测性能，必须降低这些决策树之间的相关性。

这里引入随机森林：在Bagging的基础上，在决策树的每个节点进行分裂时，仅**随机选取部分变量**（比如m个变量）作为候选的分裂变量。

比如，在一个节点随机算则 $m$ 个变量作为候选的分裂变量其余 $p-m$ 个变量则不用，而在下一个节点再次随机选择可能不相同的 $m$ 个变量作为候选的分裂变量，以此类推；然后对随机森林中的每棵决策树都如此操作。这样，即使数据还是取自原样本，但每棵树的相关性就大大减小了。

对于回归树，一般建议随机选取 $m = \frac{p}{3}$ 个变量。

对于分类树，则一般建议 $m = \sqrt{p}$

这种做法称为随机特征选择，其目的是为了降低决策树之间的相关性。

随机森林作预测的方式与装袋法相同，也是对所有决策树的预测结果进行平均(回归问题)，或多数投票(分类问题)。

在偏差与方差的权衡中，随机森林以牺牲少量偏差为代价，可以换取方差的更大幅下降，从而降低均方误差(或总误差)。

- 增大方差：若数据中存在一个高度相关的特征（如 “收入” 对 “消费能力” 的预测），所有决策树可能都优先用这个特征分裂，导致基学习器高度相似（相关性 $\rho$ 高）。当减少特征数量（如仅随机选择 $\sqrt{d}$ 个特征，$d$ 为总特征数）时，每个决策树被迫在不同特征子空间中学习。部分树可能用到 “强特征”，部分树可能仅用 “弱特征”，甚至不同树用到的特征完全不重叠，这种方法显著降低了基学习器之间的相关性。
- 增大偏差：决策树的分裂质量依赖于 “可选特征的丰富度”。若节点分裂时只能从少量特征中选择，可能错过 “最优分裂特征”（例如，真实数据中对目标变量最关键的特征未被选入当前子集）。此时，决策树无法充分捕捉数据中的规律，导致单个树的预测结果与真实值的偏差增大（即单个基学习器的偏差升高）。

装袋法为随机森林的特例。对于随机森林，如果令 $m = p$，则为套袋法。此时使用所有特征变量进行分裂，故偏差较小，但不同决策树之间的相关性较强，导致方差较大。

在另一个极端，当 $m$ 很小时，虽然决策树之间很不相关，可降低方差，但由于每次分裂的变量太少，导致偏差较大。

随机森林最重要的调节参数就是 $m$。

调节参数 $m$ 的选择，涉及偏差与方差的权衡，可通过套外误差或交叉验证来确定。选择一个最优参数 $m$，使得袋外误差或交叉验证误差最小化。

对于随机森林，自助样本的数目 $B$ 则不太重要。通常选择 $B = 500$ 或更多。

由于随机森林使用 $B$ 棵决策树的预测结果进行平均或多数投票，故增加 $B$ 并不会导致过拟合(由大数定律所保证)。

### 案例

首先我们来看相比于单一的决策树分类器，Bagging对模型训练拟合的效果如何。（以摩托车撞击实验数据集 mcycle 为例）

代码理解难度较小，部分代码之前都有出现。有些代码加#️⃣注解过。

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import BaggingClassifier, BaggingRegressor
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.datasets import load_iris
```

第一步还是导入相关的包

```python
# 导入数据
mcyle = pd.read_csv('mcycle.csv')
X = np.array(mcycle.times).reshape(-1,1) # 转换为二维数组
y = mcycle.accel
```

先看单一树模型，这里通过交叉验证选择后剪枝的复杂度参数。

关于后剪枝这里回归上一章决策树内容：

---

后剪枝：先建立完决策树后再进行剪枝。

先让决策树尽情生长，即最大的树为 $T_{max}$，再进行“修枝”，以得到一个“子树” $T$。

对于任意子树 $T \subseteq T_{max}$，定义其“复杂性”为子树 $T$ 的终节点数目，记为 $\vert T \vert$。

为避免过拟合，不希望决策树过于复杂，故惩罚其规模 $\vert T \vert$

$$
\min_{T} \underbrace{R(T)}_{\text{cost}} + \lambda \cdot \underbrace{|T|}_{\text{complexity}}
$$

其中，$R(T)$ 为原来的损失函数，比如 $0-1$ 损失函数，即在训练样本中，如果预测正确，即损失为0，若预测错误则损失为1.

$\lambda \geq 0$ 为调节参数，也称为“复杂性参数”。

$\lambda$ 控制对决策树规模 $\vert T \vert$ 的惩罚力度，可通过交叉验证确定，这种修枝方法称为**成本复杂性修枝**，即在成本与复杂性之间进行最优的权衡。

将 $0-1$ 损失函数代入表达式可得：

$$
\min_{T} \underbrace{\sum_{m=1}^{|T|} \sum_{\mathbf{x}_i \in R_m} I(y_i \neq \hat{y}_{R_m})}_{\text{cost}} + \lambda \cdot \underbrace{|T|}_{\text{complexity}}
$$

其中，$R_m$ 为第 $m$ 个终节点，$\hat y_{R_m}$ 为该终节点的预测值（该终节点观测值的众树），而 $I(y_i \neq \hat{y}_{R_m})$ 为示性函数。

$\sum _{X_i \in R_m} I(y_i \neq \hat{y}_{R_m})$ 表示在第 $m$ 个终节点的损失，然后对所有终节点 $m = 1, \dots ,\vert T \vert$ 进行加总。

如果 $\lambda = 0$，则没有复杂性惩罚项，一定选择最大的树 $T_{max}$，导致过拟合。

当 $\lambda$ 增大时，则会得到一个相互嵌套的子树序列，然后从中选择最优的子树。

---

```python
# 单一树模型 通过交叉验证选择后剪枝的复杂度参数
model = DecisionTreeRegressor(random_state=123)
path = model.cost_complexity_pruning_path(X, y) # 获取复杂度参数的路径 查看决策树后剪枝相关内容
param_grid = {'ccp_alpha': path.ccp_alphas} # 复杂度参数
kfold = KFold(n_splits=5,shuffle = True,random_state = 1)
model = GridSearchCV(DecisionTreeRegressor(random_state=123),param_grid ,cv = kfold)
pred_tree = model.fit(X,y).predict(X)

#查看拟合效果图
sns.scatterplot(x = 'times',y = 'accel',data = mcycle)
plt.plot(X,pred_tree,color = 'red')
plt.show()
```

![fix](/images/imageRF.png)

```python
#再看看使用套袋法，这里将分类器作为决策树，并且生成500个分类器
model = BaggingRegressor(estimator=DecisionTreeRegressor(random_state=123), n_estimators=500, random_state=0) # 分类器使用决策树
pred_bag = model.fit(X, y).predict(X)

#依旧查看效果图
sns.scatterplot(x='times',y='accel',data=mcycle)
plt.plot(X,pred_bag,color = 'red')
plt.title('Bagging estimation')

#这里提一嘴：使用所有特征作为随机特征参数的随机森林模型的拟合效果与套袋法一模一样，上文理论有讲解
#以下代码仅展示
model = RandomForestRegressor(n_estimators=500, random_state=0, max_features=None)
model.fit(X, y)
model.score(X,y)
pred = model.predict(X)
sns.scatterplot(x='times',y='accel',data=mcycle)
plt.plot(X,pred,'red')
plt.title('RF Estimation')
```

![fix](/images/imageRF-1.png)

#### 波士顿房价回归预测

```python
Boston = pd.read_csv('Boston_house_prices.csv')
X = Boston.drop('MEDV', axis=1)  # 假设'MEDV'为目标变量列名
y = Boston['MEDV']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
```

```python
from sklearn.metrics import mean_squared_error

model = BaggingRegressor(estimator=DecisionTreeRegressor(random_state=123), n_estimators=500, oob_score=True, random_state=0) #oob_score=True表示使用袋外样本进行评估
model.fit(X_train, y_train)
pred_oob = model.oob_prediction_
print(mean_squared_error(y_train, pred_oob))  # 输出袋外预测的均方误差
print(model.oob_score_)  # 输出袋外得分
print(model.score(X_test, y_test))  # 输出测试集得分
```

结果如下：

```python
11.323470116238905
0.8605296124474909
0.9054552734857662
```

为什么袋外得分不如模型测试集得分呢？

- 袋外样本是指每棵树训练时未被抽到的训练集样本，对这些样本的预测相当于交叉验证，评估更严格，更真实反映模型泛化能力。
- 测试集划分存在偶然性随机性，某些划分下模型可能刚好对测试集拟合较好，得分偏高。

我们来查看一般线性回归的预测得分

```python
model = LinearRegression().fit(X_train,y_train)
model.score(X_tesdt,y_test)

0.7836295385076276
```

可以看到套袋法的结果要相比普通线性回归好一些的。

根据袋外均方误差的大小来选择最优分类器数目呢？

```python
oob_errors = []
for n_estimators in range(100,301):
    model = BaggingRegressor(estimator = DecisionTreeRegressor(random_state=123), n_estimators=n_estimators, oob_score=True, random_state=0) # n_jobs = -1: 使用所有可用的 CPU 核心进行并行计算，加快模型训练速度
    model.fit(X_train, y_train)
    pred_oob = model.oob_prediction_
    oob_errors.append(mean_squared_error(y_train, pred_oob))

plt.xlabel('Number of Estimators')
plt.ylabel('OOB Error')
plt.title('OOB Error vs Number of Estimators')
plt.show()
```

![fix](/images/imageRF-2.png)

可以看出随着分类器数目增大，袋外误差有减小趋势。

接下来看随机森林的训练效果：

```python
max_features = int(X_train.shape[1]/3) # 默认的回归最优参数数目为总特征数目的三分之一。上文有介绍
model = RandomForestRegressor(n_estimators=500,max_features=max_features, random_state=0)
model.fit(X_train, y_train)
model.score(X_test,y_test)

0.8959042792611034
```

```python
pred = model.predict(X_test)

plt.scatter(pred, y_test, alpha=0.6)
w = np.linspace(min(pred), max(pred), 100)
plt.plot(w,w)
plt.xlabel('Predicted MEDV')
plt.ylabel('Actual MEDV')
plt.title('Random Forest Predictions vs Actual Values')
```

![fix](/images/imageRF-3.png)

接下来看特征的重要性。

```python
model.feature_importances_
sorted_index = model.feature_importances_.argsort() # 对特征向量按照重要性进行排序

# 生成特征重要性水平条形图
plt.barh(range(X.shape[1]), model.feature_importances_[sorted_index])
plt.yticks(np.arange(X.shape[1]), X.columns[sorted_index])
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('Random Forest')
plt.tight_layout()
```

![fix](/images/imageRF-4.png)

上文是用默认的最优参数值，我们来看看通过遍历选择模型最优得分的随机特征数目：

```python
# 通过测试集选择最好的随机特征数
# 测试集法只用一部分数据（测试集）来评估模型，最优参数高度依赖于这部分数据的分布和偶然性，容易受偶然波动影响。
# 将随机种子更改可能会导致结果的变化，因为测试集的分布可能会有所不同。
scores = []
for max_features in range(1,X.shape[1]+1):
    model = RandomForestRegressor(max_features=max_features, random_state=123,n_estimators = 500)
    model.fit(X_train, y_train)
    scores.append(model.score(X_test,y_test))

index = np.argmax(scores) # 返回最大值的索引
print(range(1,X.shape[1]+1)[index]) # 返回最大值对应的特征数

plt.plot(range(1,X.shape[1]+1),scores)
plt.xlabel('max_features')
plt.ylabel('R^2 Score ')
plt.title('choose max_features via Test set')

print(scores)

9
[0.8437131192861476, 0.8739752162372, 0.8933972149960787, 0.8945791364193233, 0.9023805203648395, 0.9064183313773803, 0.9047043224433458, 0.9052402402199933, 0.9095729082813647, 0.9065784479617953, 0.9059220121673773, 0.9049744409525852, 0.9046686863266287]
```

![fix](/images/imageRF-5.png)

将随机森林的测试集误差对比单一树模型 和套袋法 的测试集误差

随机森林：

```python
scores_rf = []
for n_estimators in range(1,301):
    model = RandomForestRegressor(max_features=5, n_estimators=n_estimators, random_state=123)
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    mse = mean_squared_error(y_test, pred)
    scores_rf.append(mse)
```

套袋法：

```python
scores_bag = []
for n_estimators in range(1,301):
    mdoel = BaggingRegressor(estimator=DecisionTreeRegressor(random_state=123),n_estimators=n_estimators, random_state=123)
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    mse = mean_squared_error(y_test, pred)
    scores_bag.append(mse)
```

单一树模型：

```python
# 单一树模型 这里依旧使用了后剪枝法进行调参

model = DecisionTreeRegressor()
path = model.cost_complexity_pruning_path(X_train, y_train)
param_grid = {'ccp_alpha': path.ccp_alphas}
kfold = KFold(n_splits=10, shuffle=True, random_state=1)
model = GridSearchCV(DecisionTreeRegressor(random_state=123),param_grid, cv=kfold)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
scores_tree = [mse]*300
```

查看最终效果：

```python
plt.plot(range(1, 301), scores_tree, 'k--', label='Single Tree')
plt.plot(range(1, 301), scores_bag, 'k-', label='Bagging')
plt.plot(range(1, 301), scores_rf, 'b-', label='Random Forest')
plt.xlabel('Number of Trees')
plt.ylabel('MSE')
plt.title('Test Error')
plt.legend()
```

![fix](/images/imageRF-6.png)

接下来对随机森林模型使用一种比测试集更可靠的调参方法：交叉验证法：

```python
max_features = range(1, X.shape[1] + 1)                   
param_grid = {'max_features': max_features }
kfold = KFold(n_splits=10, shuffle=True, random_state=1)
model = GridSearchCV(RandomForestRegressor(n_estimators=300, random_state=1), 
                     param_grid, cv=kfold, scoring='neg_mean_squared_error', return_train_score=True)

model.fit(X_train, y_train)
model.best_params_
cv_mse = -model.cv_results_['mean_test_score']

plt.plot(max_features, cv_mse, 'o-')
plt.axvline(max_features[np.argmin(cv_mse)], linestyle='--', color='k', linewidth=1)
plt.xlabel('max_features')
plt.ylabel('MSE')
plt.title('CV Error for Random Forest')

```

![fix](/images/imageRF-7.png)

通过交叉验证选择最好的随机特征数 比测试集法更稳健。因为交叉验证使用了更多的数据进行训练和验证，减少了偶然性对结果的影响。

#### Sonar 分类模型

```python
# Classification
Sonar = pd.read_csv('sonar.csv')
Sonar.shape

X = Sonar.iloc[:,:-1]
y = Sonar.iloc[:,-1]
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=1)
```

生成一个以颜色深浅直观展示变量间相关程度的矩阵图，常用于探索数据集中特征之间的关系。

```python
# 生成一个以颜色深浅直观展示变量间相关程度的矩阵图，常用于探索数据集中特征之间的关系。
sns.heatmap(X.corr(), cmap='Blues')
plt.title('Correlation Matrix')
plt.tight_layout()
```

单一树模型

```python
# 单一树模型
model = DecisionTreeClassifier()
path = model.cost_complexity_pruning_path(X, y)  # 获取复杂度参数的路径 查看决策树后剪枝相关内容
param_grid = {'ccp_alpha': path.ccp_alphas}  
kfold = KFold(n_splits=10, shuffle=True, random_state=1) #shuffle=True 对数据进行随机打乱
model = GridSearchCV(DecisionTreeClassifier(random_state=123), param_grid, cv=kfold)
model.fit(X_train, y_train)
model.score(X_test, y_test)

0.6825396825396826
```

逻辑回归

```python
# 逻辑回归
model = LogisticRegression(C=1e10, max_iter=500) # max_iter=500 迭代500次 确保模型收敛
model.fit(X_train, y_train)   
model.score(X_test, y_test)
```

随机森林

```python
# 随机森林
model = RandomForestClassifier(n_estimators=500, max_features=None, random_state=0)
model.fit(X_train, y_train)
model.score(X_test, y_test)
```

交叉验证调参随机森林

```python
y_train_dummy = pd.get_dummies(y_train)
y_train_dummy = y_train_dummy.iloc[:, 1] # 将目标变量转换为二进制形式，适用于二分类问题

y_test_dummy = pd.get_dummies(y_test)
y_test = y_test_dummy.iloc[:, 1]

param_grid = {'max_features': range(1, 11) }
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)
model = GridSearchCV(RandomForestClassifier(n_estimators=500, random_state=123),
                     param_grid, cv=kfold)
model.fit(X_train, y_train_dummy)
model.score(X_test, y_test)

0.8095238095238095
```

重要性参考

```python
model.best_estimator_.feature_importances_
sorted_index = model.best_estimator_.feature_importances_.argsort() # 对特征向量按照重要性进行排序
plt.barh(range(X.shape[1]), model.best_estimator_.feature_importances_[sorted_index])
plt.yticks(np.arange(X.shape[1]), X.columns[sorted_index],fontsize=3.5)
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('Random Forest Feature Importance')
```
