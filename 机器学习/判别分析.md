## 判别分析

### 贝叶斯决策理论

假设训练数据为$\{\boldsymbol{x}_i, y_i\}_{i=1}^n$，而$y_i$的可能取值为$y_i \in \{1, 2, \cdots, K\}$，共分为$K$类($K$ classes)。
如果知道条件概率$P(y_i = k \mid \mathbf{x} = \mathbf{x}_i)$，其中$k = 1, 2, \cdots, K$；则最优预测应最大化此条件概率，也称为“后验概率”(posterior probability)：
$$ \max_{k} \ P(y_i = k \mid \mathbf{x} = \mathbf{x}_i) $$

上式表明，对于 $y_i$ 的预测，应选择 $\hat{y}_i = k$，使得后验概率 $P(y_i = k \mid \mathbf{x} = \mathbf{x}_i)$ 最大化。
这种决策方式称为**贝叶斯最优决策**(Bayes optimal decision)。由此所得的决策边界，称为**贝叶斯决策边界**。

使用贝叶斯最优决策，所能达到的错误率，称为贝叶斯错误率(Bayes error rate) 或“贝叶斯风险”(Bayes risk)。贝叶斯错误率为可能的最低错误率，故也称为最优贝叶斯错误率(optimal Bayes rate)。

使用贝叶斯公式，在给定$\boldsymbol{x} = \boldsymbol{x}_i$的情况下，事件“$y_i = k$”的后验概率可写为
$$ P(y_i = k \mid \mathbf{x} = \mathbf{x}_i) = \frac{P(\mathbf{x} = \mathbf{x}_i \mid y_i = k) P(y_i = k)}{P(\mathbf{x} = \mathbf{x}_i)} \equiv \frac{f_k(\mathbf{x}_i) \pi_k}{P(\mathbf{x} = \mathbf{x}_i)} $$
其中，$\pi_k \equiv P(y_i = k)$为看到数据“$\mathbf{x} = \mathbf{x}_i$”之前的“先验概率”
$f_k(\mathbf{x}_i) \equiv P(\mathbf{x} = \mathbf{x}_i \mid y_i = k)$为给定类别“$y_i = k$”的条件概率密度

给定$\mathbf{x} = \mathbf{x}_i$，为比较第$k$类与第$l$类($k \neq l$)的后验概率，将二者相除，可得“**后验几率**”(posterior odds)，也称为“似然比”(likelihood ratio)：
$$ \frac{P(y_i = k \mid \mathbf{x} = \mathbf{x}_i)}{P(y_i = l \mid \mathbf{x} = \mathbf{x}_i)} = \frac{\dfrac{f_k(\mathbf{x}_i)\pi_k}{P(\mathbf{x} = \mathbf{x}_i)}}{\dfrac{f_l(\mathbf{x}_i)\pi_l}{P(\mathbf{x} = \mathbf{x}_i)}} = \frac{f_k(\mathbf{x}_i)\pi_k}{f_l(\mathbf{x}_i)\pi_l} $$
只要后验几率$\dfrac{P(y_i = k \mid \mathbf{x} = \mathbf{x}_i)}{P(y_i = l \mid \mathbf{x} = \mathbf{x}_i)} > 1$，即可预测为第$k$类。

而“$\dfrac{P(y_i = k \mid \mathbf{x} = \mathbf{x}_i)}{P(y_i = l \mid \mathbf{x} = \mathbf{x}_i)} = 1$”，则为贝叶斯决策边界(Bayes decision boundary)。
对应的样本特征 $\boldsymbol{x_i}$ 构成的边界，是分类任务中两类决策的临界条件。

**例** 对于二分类问题$y \in \{0,1\}$，假设只有一个特征变量$x$。对于$y = 0$的数据，$x$的条件分布为$x \mid y = 0 \sim N(-2,1)$；而对于$y = 1$的数据，$x$的条件分布为$x \mid y = 1 \sim N(2,1)$

若假设先验概率相等，即$\pi_0 = P(y = 0) = P(y = 1) = \pi_1$，则
$$ \frac{P(y_i = 1 \mid \mathbf{x} = \mathbf{x}_i)}{P(y_i = 0 \mid \mathbf{x} = \mathbf{x}_i)} = \frac{f_1(\mathbf{x}_i)\pi_1}{f_0(\mathbf{x}_i)\pi_0} = \frac{f_1(\mathbf{x}_i)}{f_0(\mathbf{x}_i)} $$
后验几率取决于两类数据的条件概率密度$f_1(\mathbf{x}_i)$与$f_0(\mathbf{x}_i)$之比。

![fix](/images/Pasted%20image%2020250713144133.png)

### 线性判别分析（正态分布的后验几率）

承接上文：在给定类别“$y_i = k$”的情况下，$\mathbf{x}_i$服从$p$维多元正态分布$N(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$，其概率密度可写为

$$ f_k(\mathbf{x}_i) = \frac{1}{(2\pi)^{p/2} \left| \boldsymbol{\Sigma}_k \right|^{1/2}} \exp \left\{ -\frac{1}{2} (\mathbf{x}_i - \boldsymbol{\mu}_k)' \boldsymbol{\Sigma}_k^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_k) \right\} $$
其中，$\boldsymbol{\mu}_k$为期望，$\boldsymbol{\Sigma}_k$为协方差矩阵，而$\left| \boldsymbol{\Sigma}_k \right|$为$\boldsymbol{\Sigma}_k$的行列式。
为简化计算，进一步假设所有类别的协方差矩阵均相等，即$\boldsymbol{\Sigma}_k = \boldsymbol{\Sigma},\ \forall\ k$。该假设称为“同方差假定”(homoskedastic assumption)。

则有（这里假设两个分类的协方差矩阵相同，可直接约分）
$$ \frac{P(y_i = k \mid \mathbf{x} = \mathbf{x}_i)}{P(y_i = l \mid \mathbf{x} = \mathbf{x}_i)} = \frac{\pi_k \exp \left\{ -\frac{1}{2} (\mathbf{x}_i - \boldsymbol{\mu}_k)' \boldsymbol{\Sigma}^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_k) \right\}}{\pi_l \exp \left\{ -\frac{1}{2} (\mathbf{x}_i - \boldsymbol{\mu}_l)' \boldsymbol{\Sigma}^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_l) \right\}} $$
将上式取对数，即可得到“**对数后验几率**”

$$ \begin{align*} \ln \frac{P(y_i = k \mid \mathbf{x} = \mathbf{x}_i)}{P(y_i = l \mid \mathbf{x} = \mathbf{x}_i)} &= \ln \frac{\pi_k}{\pi_l} - \frac{1}{2} (\mathbf{x}_i - \boldsymbol{\mu}_k)' \boldsymbol{\Sigma}^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_k) + \frac{1}{2} (\mathbf{x}_i - \boldsymbol{\mu}_l)' \boldsymbol{\Sigma}^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_l) \\ &= \underbrace{\left[ \ln \frac{\pi_k}{\pi_l} - \frac{1}{2} \boldsymbol{\mu}_k' \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_k + \frac{1}{2} \boldsymbol{\mu}_l' \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_l \right]}_{\text{constant}} + \underbrace{\mathbf{x}_i' \boldsymbol{\Sigma}^{-1} (\boldsymbol{\mu}_k - \boldsymbol{\mu}_l)}_{\text{linear}} \end{align*} $$
若令对数几率等于 0，即得到在第k 类与第l 类之间的“决策边界”
$$ \ln \frac{P(y_i = k \mid \mathbf{x} = \mathbf{x}_i)}{P(y_i = l \mid \mathbf{x} = \mathbf{x}_i)} = \underbrace{\left[ \ln \frac{\pi_k}{\pi_l} - \frac{1}{2} \boldsymbol{\mu}_k' \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_k + \frac{1}{2} \boldsymbol{\mu}_l' \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_l \right]}_{\text{constant}} + \underbrace{\mathbf{x}_i' \boldsymbol{\Sigma}^{-1} (\boldsymbol{\mu}_k - \boldsymbol{\mu}_l)}_{\text{linear}} = 0 $$
由于此决策边界为线性函数，故名线性判别分析(Linear Discriminant Analysis，简记 LDA)。

进一步，将对数几率按照类别k 与 l，合并同类项可得：
$$ \begin{align*} \ln \frac{P(y_i = k \mid \mathbf{x} = \mathbf{x}_i)}{P(y_i = l \mid \mathbf{x} = \mathbf{x}_i)} &= \underbrace{\left[ \ln \pi_k - \frac{1}{2} \boldsymbol{\mu}_k' \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_k + \mathbf{x}_i' \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_k \right]}_{\equiv \delta_k(\mathbf{x}_i)} - \underbrace{\left[ \ln \pi_l - \frac{1}{2} \boldsymbol{\mu}_l' \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_l + \mathbf{x}_i' \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_l \right]}_{\equiv \delta_l(\mathbf{x}_i)} \\ &\equiv \delta_k(\mathbf{x}_i) - \delta_l(\mathbf{x}_i) \end{align*} $$
其中，$\delta_k(\mathbf{x}_i) \equiv \ln \pi_k - \frac{1}{2} \boldsymbol{\mu}_k' \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_k + \mathbf{x}_i' \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_k$ 称为线性判别函数。

最优决策规则为选择类别k ，使得线性判别函数最大化：

$$ \max_k \ \delta_k(\mathbf{x}_i) $$
在实践中，我们需要估计线性判别函数$\delta_k(\mathbf{x}_i)$中的未知总体参数$\pi_k$，$\boldsymbol{\mu}_k$与$\boldsymbol{\Sigma}$。根据训练数据$\{ \mathbf{x}_i, y_i \}_{i=1}^n$，只要计算这些总体参数的相应样本估计量即可。

先验概率$\pi_k$的估计量为 $$ \hat{\pi}_k = \frac{n_k}{n} \quad (k = 1, \cdots, K) $$ 其中，$n_k$为训练样本中第$k$类数据的样例数。

每类数据的期望值$\boldsymbol{\mu}_k$之估计量为样本均值$\hat{\boldsymbol{\mu}}_k$： $$ \hat{\boldsymbol{\mu}}_k = \frac{1}{n_k} \sum_{y_i=k} \mathbf{x}_i \quad (k = 1, \cdots, K) $$
第$k$类数据的协方差矩阵$\boldsymbol{\Sigma}_k$之估计量为样本协方差矩阵$\hat{\boldsymbol{\Sigma}}_k$： $$ \hat{\boldsymbol{\Sigma}}_k = \frac{1}{n_k - 1} \sum_{y_i=k} (\mathbf{x}_i - \hat{\boldsymbol{\mu}}_k)(\mathbf{x}_i - \hat{\boldsymbol{\mu}}_k)' \equiv \frac{\mathbf{S}_k}{n_k - 1}  $$
其中，$\mathbf{S}_k \equiv \sum_{y_i=k} (\mathbf{x}_i - \hat{\boldsymbol{\mu}}_k)(\mathbf{x}_i - \hat{\boldsymbol{\mu}}_k)'$ 称为第 $k$ 类数据的“散度矩阵”(scatter matrix)，而$(n_k - 1)$为自由度(在估计$\hat{\boldsymbol{\mu}}_k$时损失了一个自由度)。

在每类数据的协方差矩阵均相等的假设下(即$\boldsymbol{\Sigma}_k = \boldsymbol{\Sigma}$)，可用每类数据的样本协方差矩阵$\hat{\boldsymbol{\Sigma}}_k$之加权平均(权重为每类数据在样本中的比重，经过自由度调整)，来估计整个样本的协方差矩阵： $$ \hat{\boldsymbol{\Sigma}} = \sum_{k=1}^K \hat{\boldsymbol{\Sigma}}_k \cdot \underbrace{\left( \frac{n_k - 1}{n - K} \right)}_{\text{权重}} = \sum_{k=1}^K \frac{\mathbf{S}_k}{n_k - 1} \cdot \left( \frac{n_k - 1}{n - K} \right) = \frac{1}{n - K} \sum_{k=1}^K \mathbf{S}_k $$
K为类别数，即所有损失的自由度。权重表示某一类样本自由度在总体样本自由度中的占比

### 二次判别分析

线性判别分析假设所有类别的协方差矩阵均相同，即$\sum_K = \sum$。该假设可能与显示数据相悖。
仍可考虑第$k$ 类与第$l$ 类（$k\neq l$）之间的对数几率：

$$ \ln \frac{P(y_i = k \mid \mathbf{x} = \mathbf{x}_i)}{P(y_i = l \mid \mathbf{x} = \mathbf{x}_i)} = \ln \frac{\pi_k}{\pi_l} - \frac{1}{2} \ln \left( \frac{|\boldsymbol{\Sigma}_k|}{|\boldsymbol{\Sigma}_l|} \right) - \frac{1}{2} (\mathbf{x}_i - \boldsymbol{\mu}_k)' \boldsymbol{\Sigma}_k^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_k) + \frac{1}{2} (\mathbf{x}_i - \boldsymbol{\mu}_l)' \boldsymbol{\Sigma}_l^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_l) $$

由方程 “$\ln \dfrac{P(y_i = k \mid \mathbf{x} = \mathbf{x}_i)}{P(y_i = l \mid \mathbf{x} = \mathbf{x}_i)} = 0$” 所决定的决策边界为二次(型)函数，故称为**二次判别分析**(Quadratic Discriminant Analysis，简记 QDA)。进一步，可将对数几率的表达式，按照类别$k$ 与 $l$合并同类项
$$ \begin{align*} \ln \frac{P(y_i = k \mid \mathbf{x} = \mathbf{x}_i)}{P(y_i = l \mid \mathbf{x} = \mathbf{x}_i)} &= \underbrace{\left[ \ln \pi_k - \frac{1}{2} \ln |\boldsymbol{\Sigma}_k| - \frac{1}{2} (\mathbf{x}_i - \boldsymbol{\mu}_k)' \boldsymbol{\Sigma}_k^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_k) \right]}_{\equiv \delta_k(\mathbf{x}_i)} \\ &\quad - \underbrace{\left[ \ln \pi_l - \frac{1}{2} \ln |\boldsymbol{\Sigma}_l| - \frac{1}{2} (\mathbf{x}_i - \boldsymbol{\mu}_l)' \boldsymbol{\Sigma}_l^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_l) \right]}_{\equiv \delta_l(\mathbf{x}_i)} \\ &\equiv \delta_k(\mathbf{x}_i) - \delta_l(\mathbf{x}_i) \end{align*} $$
其中，$\delta_k(\mathbf{x}_i) \equiv \ln \pi_k - \frac{1}{2} \ln |\boldsymbol{\Sigma}_k| - \frac{1}{2} (\mathbf{x}_i - \boldsymbol{\mu}_k)' \boldsymbol{\Sigma}_k^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_k)$ 称为**二次判别函数**(quadratic discriminant function)。

最优决策规则为选择类别$k$，使得二次判别函数$\delta_k(\mathbf{x}_i)$最大化。在实践中，可使用$\hat{\boldsymbol{\Sigma}}_k = \dfrac{1}{n_k - 1} \sum_{y_i=k} (\mathbf{x}_i - \hat{\boldsymbol{\mu}}_k)(\mathbf{x}_i - \hat{\boldsymbol{\mu}}_k)'$估计$\boldsymbol{\Sigma}_k$。

### 费雪线性判别分析

Fisher(1936)从数据降维的角度来考虑线性判别问题，称为费雪线性判别法。其基本思想是：能否将特征向量$X_i$作适当的线性组合$W'X_i$，使得数据变得更容易分离？

首先考虑二分类问题：
假设训练样本为$\{ \mathbf{x}_i, y_i \}_{i=1}^n$，其中$\mathbf{x}_i = (x_{i1} \cdots x_{ip})'$为$p$维特征向量，而响应变量$y_i \in \{1, 2\}$分为两类。 由于$p$可能很大，为高维数据，故考虑特征向量的线性组合$z_i = \mathbf{w}' \mathbf{x}_i$，其中$\mathbf{w} = (w_1 \cdots w_p)'$为此线性组合的“权重”(weights)。通过一维标量$z_i$来进行样本分类
$$ \boldsymbol{w}'\boldsymbol{x}_i = w_1 x_{i1} + w_2 x_{i2} + \dots + w_p x_{ip} $$
![fix](/images/Pasted%20image%2020250716122539.png)
如图所示，需要找到一个合适的投影，使得组内方差最小，组间方差最大。即使得类内尽可能仅：同一类别内部的投影点尽量集中，对应样本投影到$w$之后，类内方差小；而类间尽可能远：不同类别的投影中心间距大，对应$\bar{z}_1 - \bar{z}_2 = \boldsymbol{w}'(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2)$最大化。

分别记第 1 类与第 2 类数据之特征向量的样本均值为$\hat \mu_1$和$\hat \mu_2$
$$\hat\mu_1 = \frac{1}{n_1}\sum_{y_i = 1} X_i，\hat \mu_2 = \frac{1}{n_2}\sum_{y_i=2}X_i$$
其中，$n_1$为第 1 类数据的样例数，而$n_2$为第 2 类数据的样例数。

两类数据的中心位置经过投影变换之后，分别变为$\bar{z}_1 \equiv \mathbf{w}' \hat{\boldsymbol{\mu}}_1$与$\bar{z}_2 \equiv \mathbf{w}' \hat{\boldsymbol{\mu}}_2$。因此，投影之后两类样本中心位置的差距为 $$ \bar{z}_1 - \bar{z}_2 = \mathbf{w}' (\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2) $$
而组间方差为
$$ (\bar{z}_1 - \bar{z}_2)^2 = \boldsymbol{w}'(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2) \left[ \boldsymbol{w}'(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2) \right]' = \boldsymbol{w}' \underbrace{(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2)(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2)'}_{\equiv \boldsymbol{S}_B} \boldsymbol{w} \equiv \boldsymbol{w}' \boldsymbol{S}_B \boldsymbol{w} $$
在上式中，虽然组间方差$(\bar{z}_1 - \bar{z}_2)^2$为标量，但在形式上依然可写为二次型（为后面推导方便），其中二次型矩阵$\boldsymbol{S}_B \equiv (\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2)(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2)'$称为“组间散度矩阵”(between - class scatter matrix)。

示例：（类内散度矩阵）
![fix](/images/Pasted%20image%2020250716123553.png)
$\boldsymbol{S}_B$ 是对称矩阵，反映两类均值向量的 “离散程度”：值越大，说明两类中心越远。

对于第 1 类数据，其投影之后的“**组内方差**”(within-class variance)可写为

$$ \begin{align*} \hat{s}_1^2 &= \sum_{y_i=1} (z_i - \bar{z}_1)^2 = \sum_{y_i=1} (\boldsymbol{w}'\boldsymbol{x}_i - \boldsymbol{w}'\hat{\boldsymbol{\mu}}_1)^2 \\ &= \sum_{y_i=1} \boldsymbol{w}'(\boldsymbol{x}_i - \hat{\boldsymbol{\mu}}_1)(\boldsymbol{x}_i - \hat{\boldsymbol{\mu}}_1)' \boldsymbol{w} \\ &= \boldsymbol{w}' \left[ \sum_{y_i=1} (\boldsymbol{x}_i - \hat{\boldsymbol{\mu}}_1)(\boldsymbol{x}_i - \hat{\boldsymbol{\mu}}_1)' \right] \boldsymbol{w} \\ &= \boldsymbol{w}' \boldsymbol{S}_1 \boldsymbol{w} \end{align*} $$
其中，$\boldsymbol{S}_1 \equiv \sum\limits_{y_i = 1} (\boldsymbol{x}_i - \hat{\boldsymbol{\mu}}_1)(\boldsymbol{x}_i - \hat{\boldsymbol{\mu}}_1)'$为第 1 类数据在特征空间的散度矩阵
类似地，第 2 类数据投影之后的组内方差可写为$\hat{s}_2 = \sum_{y_i=2} (z_i - \overline{z}_2)^2 = \mathbf{w}' \left[ \sum_{y_i=2} (\mathbf{x}_i - \hat{\boldsymbol{\mu}}_2)(\mathbf{x}_i - \hat{\boldsymbol{\mu}}_2)' \right] \mathbf{w} = \mathbf{w}' \mathbf{S}_2 \mathbf{w}$

其中，$\boldsymbol{S}_2 \equiv \sum\limits_{y_i = 2} (\boldsymbol{x}_i - \hat{\boldsymbol{\mu}}_2)(\boldsymbol{x}_i - \hat{\boldsymbol{\mu}}_2)'$为第 2 类数据在特征空间的散度矩阵。两类数据投影之后的组内方差之和为 $$\hat{s}_1 + \hat{s}_2 = \boldsymbol{w}'\boldsymbol{S}_1\boldsymbol{w} + \boldsymbol{w}'\boldsymbol{S}_2\boldsymbol{w} = \boldsymbol{w}'(\boldsymbol{S}_1 + \boldsymbol{S}_2)\boldsymbol{w} \equiv \boldsymbol{w}'\boldsymbol{S}_W\boldsymbol{w} $$ 二次型矩阵$\boldsymbol{S}_W \equiv \boldsymbol{S}_1 + \boldsymbol{S}_2$为“组内散度矩阵”(within - class scatter matrix)。
费雪线性判别的最优化目标为，在给定组内方差的情况下，最大化组间方差。故可将最优化问题的目标函数写为
$$ \max_{\boldsymbol{w}} \ J(\boldsymbol{w}) = \frac{(\overline{z}_1 - \overline{z}_2)^2}{\hat{s}_1 + \hat{s}_2} = \frac{\boldsymbol{w}'\boldsymbol{S}_B\boldsymbol{w}}{\boldsymbol{w}'\boldsymbol{S}_W\boldsymbol{w}} $$

此目标函数也称为“费雪准则”(Fisher criterion)。
最优解$\hat{\boldsymbol{w}}$与其长度无关。如果$\hat{\boldsymbol{w}}$是最优解，则对于任意$\alpha \neq 0$，$\alpha\hat{\boldsymbol{w}}$也是最优解(可在上式的分子与分母同时消去$\alpha^2$)。 不失一般性，令分母$\boldsymbol{w}'\boldsymbol{S}_W\boldsymbol{w} = 1$。
将上述无约束的最大化问题等价地写为有约束的最大化问题：
$$ \begin{align*} \max_{\boldsymbol{w}} &\ \ \boldsymbol{w}'\boldsymbol{S}_B\boldsymbol{w} \\ s.t. &\ \ \boldsymbol{w}'\boldsymbol{S}_W\boldsymbol{w} = 1 \end{align*} $$
为求解此约束极值问题，引入拉格朗日乘子函数：$$ L(\boldsymbol{w}, \lambda) = \boldsymbol{w}'\boldsymbol{S}_B\boldsymbol{w} + \lambda(1 - \boldsymbol{w}'\boldsymbol{S}_W\boldsymbol{w})  $$ 将上式对$\boldsymbol{w}$求偏导数，根据二次型的向量微分规则，并注意到$\boldsymbol{S}_B$与$\boldsymbol{S}_W$均为对称矩阵，可得一阶条件：（条件极值的求解 最值时满足偏导数为0）$$ \frac{\partial L(\boldsymbol{w}, \lambda)}{\partial \boldsymbol{w}} = 2\boldsymbol{S}_B\boldsymbol{w} - 2\lambda\boldsymbol{S}_W\boldsymbol{w} = \boldsymbol{0}  $$ 经移项整理可得： $$ \boldsymbol{S}_B\boldsymbol{w} = \lambda\boldsymbol{S}_W\boldsymbol{w} \tag{important}$$ 在上式两边同时左乘$\boldsymbol{S}_W^{-1}$可得： $$ \underbrace{(\boldsymbol{S}_W^{-1}\boldsymbol{S}_B)}_{\boldsymbol{A}}\boldsymbol{w} = \lambda\boldsymbol{w} $$
式可视为 “$\boldsymbol{A}\boldsymbol{w} = \lambda\boldsymbol{w}$”，正是线性代数的 “特征值问题”(eigenvalue problem)，其中$\lambda$为特征值，$\boldsymbol{w}$为相应的特征向量(eigenvector)，而 $\boldsymbol{A} = \boldsymbol{S}_W^{-1}\boldsymbol{S}_B$。故最优解为矩阵$\boldsymbol{S}_W^{-1}\boldsymbol{S}_B$的特征值与特征向量。

矩阵$\boldsymbol{S}_W^{-1}\boldsymbol{S}_B$的特征向量通常不止一个。最优解究竟是哪个特征值及其相应的特征向量呢？将方程代入目标函数可得： $$ \max_{\boldsymbol{w}} \ \boldsymbol{w}'\boldsymbol{S}_B\boldsymbol{w} = \boldsymbol{w}'\lambda\boldsymbol{S}_W\boldsymbol{w} = \lambda \underbrace{\boldsymbol{w}'\boldsymbol{S}_W\boldsymbol{w}}_{=1} = \lambda  $$ 其中，根据约束条件$\boldsymbol{w}'\boldsymbol{S}_W\boldsymbol{w} = 1$。
目标函数的最大值正是矩阵$\boldsymbol{S}_W^{-1}\boldsymbol{S}_B$的特征值$\lambda$。为了最大化此目标函数，应该选择最大的特征值，记为$\lambda_1$，并记其特征向量为$\boldsymbol{a}_1$(须将特征向量$\boldsymbol{a}_1$标准化，使得$\boldsymbol{a}_1'\boldsymbol{S}_W\boldsymbol{a}_1 = 1$)。

由此可得，最优投影方向为$\hat{\boldsymbol{w}} = \boldsymbol{a}_1'$，并称$z_i = \hat{\boldsymbol{w}}'\boldsymbol{x}_i$为线性判别变量，简称线性判元(linear discriminant)。

对于此具体问题，还有更简洁的解法。将组间散度矩阵$\boldsymbol{S}_B$的表达式 $\boldsymbol{S}_B \equiv (\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2)(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2)'$代入方程可得， $$ \lambda\boldsymbol{S}_W\boldsymbol{w} = \boldsymbol{S}_B\boldsymbol{w} = (\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2)\underbrace{(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2)'\boldsymbol{w}}_{= c \in \mathbb{R}} \equiv c(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2)  $$ 其中，记$(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2)'\boldsymbol{w}$为某常数$c \in \mathbb{R}$，其本身为标量。在上式两边同时左乘$\dfrac{1}{\lambda}\boldsymbol{S}_W^{-1}$可得，
$$ \boldsymbol{w} = \frac{c}{\lambda}\boldsymbol{S}_W^{-1}(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2) $$
常数$\dfrac{c}{\lambda}$并不影响向量$\boldsymbol{w}$的方向，而我们只在乎$\boldsymbol{w}$的方向，故最优解也可写为 $$ \hat{\boldsymbol{w}} = \boldsymbol{S}_W^{-1}(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2)$$ 严格来说，仍应将$\hat{\boldsymbol{w}}$标准化，使得$\hat{\boldsymbol{w}}'\boldsymbol{S}_W\hat{\boldsymbol{w}} = 1$。
由此所得的最佳投影$z_i = \hat{\boldsymbol{w}}'\boldsymbol{x}_i$，即为线性判元(linear discriminant)或线性判别得分(linear discriminant score)。

实际分类时，以两类均值投影的中点为阈值
对于一个新的样本点$\boldsymbol{x}_0$，可根据其线性判别得分$z_0 = \hat{\boldsymbol{w}}'\boldsymbol{x}_0$与两类数据投影的中心位置$\overline{z}_1 = \hat{\boldsymbol{w}}'\hat{\boldsymbol{\mu}}_1$与$\overline{z}_2 = \hat{\boldsymbol{w}}'\hat{\boldsymbol{\mu}}_2$的距离远近进行分类，即归入距离更近的那一类。
在具体操作上，当第一类的均值投影大于第一类时，如果$z_0$比$(\hat{\boldsymbol{\mu}}_1 + \hat{\boldsymbol{\mu}}_2)/2$的投影更大，则可将$\boldsymbol{x}_0$归入第 1 类，即 $$ z_0 = \hat{\boldsymbol{w}}'\boldsymbol{x}_0 \geq \hat{\boldsymbol{w}}'(\hat{\boldsymbol{\mu}}_1 + \hat{\boldsymbol{\mu}}_2)/2  $$
反之则归入第二类。

由此定义线性分类函数$\boldsymbol{x}'\hat{\boldsymbol{w}} - \frac{1}{2}(\hat{\boldsymbol{\mu}}_1 + \hat{\boldsymbol{\mu}}_2)'\hat{\boldsymbol{w}}$，将其中的$\hat w$都用上文的$\hat{\boldsymbol{w}} = \boldsymbol{S}_W^{-1}(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2）$代替，则可得：$$ \underbrace{-\frac{1}{2}(\hat{\boldsymbol{\mu}}_1 + \hat{\boldsymbol{\mu}}_2)'\boldsymbol{S}_W^{-1}(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2)}_{\text{constant}} + \underbrace{\boldsymbol{x}'\boldsymbol{S}_W^{-1}(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2)}_{\text{linear}} $$
然后，根据线性分类函数的取值正负对$X$进行分类。

### 费雪线性判别与基于正态的线性判别之关系

在进行基于正态的线性判别分析时，如果假设先验概率相等，则等价于费雪的线性判别分析。
下面证明，费雪判别分析的线性分类函数，等价于正态判别分析的对数后验几率函数。

即$\boldsymbol{x}'\hat{\boldsymbol{w}} - \frac{1}{2}(\hat{\boldsymbol{\mu}}_1 + \hat{\boldsymbol{\mu}}_2)'\hat{\boldsymbol{w}}$等价于正态判别分析的对数后验几率函数。

在进行基于正态的线性判别分析时，第 1 类与第 2 类数据的“对数后验几率”(log posterior odds)的样本估计值为：其中，假设先验概率$\pi_1 = \pi_2$，故$\ln \dfrac{\pi_1}{\pi_2} = 0$。

$$ \begin{align*} \ln \frac{P(y=1|\boldsymbol{x})}{P(y=2|\boldsymbol{x})} &= \left( \ln \frac{\pi_1}{\pi_2} - \frac{1}{2}\hat{\boldsymbol{\mu}}_1'\hat{\boldsymbol{\Sigma}}^{-1}\hat{\boldsymbol{\mu}}_1 + \frac{1}{2}\hat{\boldsymbol{\mu}}_2'\hat{\boldsymbol{\Sigma}}^{-1}\hat{\boldsymbol{\mu}}_2 \right) + \boldsymbol{x}'\hat{\boldsymbol{\Sigma}}^{-1}(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2) \\ &= \left( -\frac{1}{2}\hat{\boldsymbol{\mu}}_1'\hat{\boldsymbol{\Sigma}}^{-1}\hat{\boldsymbol{\mu}}_1 + \frac{1}{2}\hat{\boldsymbol{\mu}}_2'\hat{\boldsymbol{\Sigma}}^{-1}\hat{\boldsymbol{\mu}}_2 \right) + \boldsymbol{x}'\hat{\boldsymbol{\Sigma}}^{-1}(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2) \end{align*} $$
将上式与费雪判别法的线性分类函数对比。
当两类数据的协方差矩阵相等的情况下，第一项括号内可用平方差公式合成一项。
又有 $\hat{\boldsymbol{\Sigma}}_k = \frac{1}{n_k - 1} \sum_{y_i=k} (\mathbf{x}_i - \hat{\boldsymbol{\mu}}_k)(\mathbf{x}_i - \hat{\boldsymbol{\mu}}_k)' \equiv \frac{\mathbf{S}_k}{n_k - 1}$  
$\hat{\boldsymbol{\Sigma}} = \sum_{k=1}^K \hat{\boldsymbol{\Sigma}}_k \cdot \underbrace{\left( \frac{n_k - 1}{n - K} \right)}_{\text{权重}} = \sum_{k=1}^K \frac{\mathbf{S}_k}{n_k - 1} \cdot \left( \frac{n_k - 1}{n - K} \right) = \frac{1}{n - K} \sum_{k=1}^K \mathbf{S}_k$
则$\hat{\boldsymbol{\Sigma}}^{-1}$与$\boldsymbol{S}_W^{-1}$仅相差$(n-K)$倍，不妨令$\boldsymbol{S}_W^{-1} = \hat{\boldsymbol{\Sigma}}^{-1}$，则上式的一次项与线性分类函数(7.36)的一次项相等，即 $$ \boldsymbol{x}'\hat{\boldsymbol{\Sigma}}^{-1}(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2) = \boldsymbol{x}'\boldsymbol{S}_W^{-1}(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_2) $$
由此可知，如果假设先验概率相等，则费雪线性判别与基于正态的线性判别等价，使用二者所得的决策边界完全相同。

### 多分类问题的费雪判别分析

将二分类问题的费雪判别分析推广到多分类问题。对于二分类问题只有一个最佳投影方向$W$，对于$K$分类问题，记$y_i \in \{1,\cdots,K\}$，则一般可以有$(K-1)$个最佳投影方向$\{\boldsymbol{w}_1, \cdots, \boldsymbol{w}_{K-1}\}$，以及相应的$(K-1)$个线性判元$\{\boldsymbol{w}_1'\boldsymbol{x}, \cdots, \boldsymbol{w}_{K-1}'\boldsymbol{x}\}$。
但线性判元的个数也受到属性个数 p (即特征向量x的维度)的限制。

### 案例

以iris数据为例 （150个数据 4个特征 3个类别）

```python
import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.datasets import load_iris

from sklearn.model_selection import train_test_split

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

from sklearn.metrics import confusion_matrix

from sklearn.metrics import classification_report

from sklearn.metrics import cohen_kappa_score

iris = load_iris()

dir(iris) #描述

```

'DESCR', 'data', 'data_module', 'feature_names', 'filename', 'frame', 'target', 'target_names']

```python
iris.feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']#修改特征的标签名称 去掉单位

X = pd.DataFrame(iris.data,columns= iris.feature_names)

sns.heatmap(X.corr() , cmap = "Blues",annot=True) #相关性热力图 标注出具体数值
```

![fix](/images/Pasted%20image%2020250719210440.png)
模型建立：

```python
y = iris.target 
model = LinearDiscriminantAnalysis()# LDA
model.fit(X,y)
model.score(X,y)
```

0.98

```python
model.priors_ #先验概率 array([0.33333333, 0.33333333, 0.33333333])
model.means_ #三个类别的各特征的均值 array([[5.006, 3.428, 1.462, 0.246], [5.936, 2.77 , 4.26 , 1.326], [6.588, 2.974, 5.552, 2.026]])
model.explained_variance_ratio_ #解释方差比率
model.scalings_ #投影方向的缩放系数
```

由上文的理论（费雪准则）：二分类
$$ \max_{\boldsymbol{w}} \ J(\boldsymbol{w}) = \frac{(\overline{z}_1 - \overline{z}_2)^2}{\hat{s}_1 + \hat{s}_2} = \frac{\boldsymbol{w}'\boldsymbol{S}_B\boldsymbol{w}}{\boldsymbol{w}'\boldsymbol{S}_W\boldsymbol{w}} $$
引申出多分类
数学基础：类间散度与类内散度
定义符号：
数据集包含 $K$ 个类别，第 $k$ 类的样本数为 $n_k$，总样本数 $n = \sum_{k=1}^K n_k$。
第 $k$ 类的样本均值向量：$\mathbf{m}_k = \frac{1}{n_k} \sum_{\mathbf{x} \in C_k} \mathbf{x}$
全局样本均值向量：$\mathbf{m} = \frac{1}{n} \sum_{i=1}^n \mathbf{x}_i$
求一个适当的$w$值使费雪准则最大

类内散度矩阵（Within-Class Scatter） $$\mathbf{S}_W = \sum_{k=1}^K \sum_{\mathbf{x} \in C_k} (\mathbf{x} - \mathbf{m}_k)(\mathbf{x} - \mathbf{m}_k)^T$$ - 衡量每个类别内部样本的分散程度。

类间散度矩阵（Between-Class Scatter） $$\mathbf{S}_B = \sum_{k=1}^K n_k (\mathbf{m}_k - \mathbf{m})(\mathbf{m}_k - \mathbf{m})^T$$ - 衡量不同类别之间的中心差异。

由此可得类间散度矩阵的自由度为$K-1$，故费雪准则得出的$W$也是两个自由度。
2 个类别(二分类)只需1个超平面
3 个类别需2个超平面(2个判别成分)，形成三维空间中的三角区域。
这就解释了`model.scalings`是4\*2的

```python
model.scalings_ #线性判别分析的缩放系数
#array([[ 0.82937764, -0.02410215], [ 1.53447307, -2.16452123], [-2.20121166, 0.93192121], [-2.81046031, -2.83918785]])
```

将两组线性判别分析的缩放系数作为新的LDA数据：

```python
ida_lodings = pd.DataFrame(model.scalings_, index=iris.feature_names, columns=['LD1', 'LD2']) #以iris.feature_names为列索引

print(ida_lodings)#

```

```python
LD1       LD2
sepal_length  0.829378 -0.024102
sepal_width   1.534473 -2.164521
petal_length -2.201212  0.931921
petal_width  -2.810460 -2.839188
```

转换数据 先训练再将特征值X与得到的系数矩阵相乘

```python
lda_scores = model.fit(X, y).transform(X)

#先训练模型， 再转换数据

lda_scores.shape  #(150, 2)

lda_scores[:5,:] #array([[ 8.06179978, -0.30042062], [ 7.12868772, 0.78666043], [ 7.48982797, 0.26538449], [ 6.81320057, 0.67063107], [ 8.13230933, -0.51446253]])

LDA_scores = pd.DataFrame(lda_scores , columns = ['LD1','LD2'])
LDA_scores['Species'] = iris.target
LDA_scores.head()
```

```python
|LD1|LD2|Species|
|---|---|---|
|0|8.061800|-0.300421|0|
|1|7.128688|0.786660|0|
|2|7.489828|0.265384|0|
|3|6.813201|0.670631|0|
|4|8.132309|-0.514463|0|
```

将类别0 1 2 转换成标签

```python
d = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}# 类别标签映射

LDA_scores['Species'] = LDA_scores['Species'].map(d)

LDA_scores.head()

```

```python
|LD1|LD2|Species|
|---|---|---|
|0|8.061800|-0.300421|setosa|
|1|7.128688|0.786660|setosa|
|2|7.489828|0.265384|setosa|
|3|6.813201|0.670631|setosa|
|4|8.132309|-0.514463|setosa|
```

以LD1为x LD2为y画散点图：

```python
sns.scatterplot(data=LDA_scores, x='LD1', y='LD2', hue='Species', palette='Set1')
```

`hue = 'Species'`根据`Species`列的类别标签对数据点着色，不同类别用不同颜色区分，便于观察类别间的分离程度。

`palette='Set1'`指定配色方案为`Set1`（Seaborn 内置的高对比度配色，适合区分多个类别）。
![fix](/images/Pasted%20image%2020250719225032.png)
将第二第三列单独提出

```python
X2 = X.iloc[:,2:4]
model = LinearDiscriminantAnalysis()
model.fit(X2,y)
model.score(X2,y)
model.explained_variance_ratio_
```

画图

```python
from mlxtend.plotting import plot_decision_regions
plot_decision_regions(np.array(X2),y,model)
plt.xlabel('Petal Length')
plt.ylabel('Petal Width')
plt.title('LDA Decision Boundary')
```

![fix](/images/Pasted%20image%2020250719232614.png)
为什么这里不使用最初的LD1和LD2呢？
**要求输入原始特征空间**（即模型训练时的特征维度），因为它需要在 **原始特征维度** 上生成网格并计算决策边界
函数会误以为 `lda_scores` 是原始特征，在错误的维度范围生成网格，导致边界绘制异常。

**对训练集测试集的LDA**

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y, random_state=123)

model = LinearDiscriminantAnalysis()

model.fit(X_train, y_train)

model.score(X_test, y_test) #模型评分 0.9777777777777777


```

熟悉的模型预测与评估

```python
prob = model.predict_proba(X_test)

pred = model.predict(X_test)

print(pred[:5])

confusion_matrix(y_test, pred) #混淆矩阵

print(classification_report(y_test, pred)) #分类报告
```

![fix](/images/Pasted%20image%2020250719233309.png)
二次型判别分析：

```python
model = QuadraticDiscriminantAnalysis()

model.fit(X_train, y_train)

model.score(X_test, y_test) #准确度Accuracy

prob = model.predict_proba(X_test)

pred = model.predict(X_test)

confusion_matrix(y_test, pred) #混淆矩阵

print(classification_report(y_test, pred)) #分类报告
```

![fix](/images/Pasted%20image%2020250719233506.png)
类似可求二次判别分析的决策边界

```python
X2 = X.iloc[:, 2:4]

model2 = QuadraticDiscriminantAnalysis()#二次判别分析

model2.fit(X2, y)

model2.score(X2, y) #模型评分

plot_decision_regions(np.array(X2), y, model2)

plt.xlabel('petal_length')

plt.ylabel('petal_width')

plt.title('Decision Boundary for QDA')
```

![fix](/images/Pasted%20image%2020250719233706.png)