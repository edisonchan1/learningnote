---
title: 人工神经网络
---

人的大脑是由大约一千亿个神经元相连而构成的神经网络

Rosenblatt(1958)提出“感知机”(perceptron)，通过引入“学习”(learning)，使得感知机具备将事物分类的能力。但这种单层的神经网络无法得到非线性的决策边界，直到出现多层感知机，即多层神经网络。随着算法日益改进，演变成炙手可热的深度学习。

### 人工神经网络的思想

在人工智能领域，主要有两大派系：一个派系为符号主义，又称逻辑主义，主张用公理和逻辑体系搭建一套人工智能系统。

符号主义者认为：人工智能应模仿人类的逻辑方式获取知识。

另一派系则是连接主义，也称仿生学派，主张模仿人类的神经元，用神经网络的连接机制实现人工智能。

连接主义者奉行大数据和学习来获取知识。

**人工神经网络ANN**是连接主义的代表作。人类的大脑是由大量神经细胞为基本单位而组成的神经网络。

神经细胞也称神经元

![fix](/images/imageANN.png)

一个神经元通过左边的树突从其他神经元的轴突及轴突末梢获取电子或化学信号，两个神经元之间的连接部位称为神经突触。连接在一起的神经元，可以共同兴奋。

从树突获得不同的信号后，神经元的细胞体将这些信号进行夹总处理。

如果这些信号的总量超过某个阈值，则神经元会兴奋起来，并通过轴突向外传输信号，通过神经突触而为其他神经元的树突所接收。

将生物神经元简化成一个数学模型，简称为**M-P 神经元模型**

![fix](/images/imageANN-1.png)

将神经元视为一个计算单位，它首先从树突输入信号 $X=(x_1,\dots,x_p)'$，在细胞体进行加权求和 $\sum ^p_{i =1}w_ix_i$，其中，$W = (w_1,\dots,w_p)'$ 为权重。

通过求和之后的总数，超过某个阈值，则神经元兴奋起来，通过轴突向外传输信号；反之，则神经元处于抑制状态。

$$
I\left( \sum_{i=1}^{p} w_i x_i + b > 0 \right) =
\begin{cases}
1 & \text{if } \sum_{i=1}^{p} w_i x_i > -b \\
0 & \text{if } \sum_{i=1}^{p} w_i x_i \leq -b
\end{cases}
$$

其中，参数 $b$ 表示阈值。称为**偏置**。示性函数 $I(\cdot)$ 称为激活函数。

M-P 神经元模型本质上只是一个纯数学模型，其中的参数 $w$ 和 $b$ 需要认为指定，而无法通过训练样本进行学习。

### 感知机

后续提出的**感知机**，使得M-P 神经元模型具备学习能力，称为神经网络模型的先驱。

通过之前SVM的学习我们知道：对于二分类问题，考虑分离超平面 $b+w'X =0$ 进行分类，而响应变量 $y \in \{1,-1\}$。

如果 $b+w'X >0$，则预测 $y = 1$。如果 $b+w'X <0$，则预测 $y=-1$。如果 $b+w'X = 0$，可随意预测。

正确分类要求 $y_i(b + \mathbf{w}'\mathbf{x}_i) > 0$。若 $y_i(b + \mathbf{w}'\mathbf{x}_i) < 0$，则错误分类。

从某个初始值 $(w_0,b_0)$ 出发，感知机希望通过调整参数 $(w,b)$，使得模型的错误分类最少。

感知机的目标函数为最小化所有分类错误观测值的错误程度之和：

$$
\min_{\mathbf{w}, b} \ L(\mathbf{w}, b) = -\sum_{i \in \mathcal{M}} y_i (b + \mathbf{w}' \mathbf{x}_i)
$$

其中，$\mathcal{M}$ 为所有错误分类(misclassified)的个体下标之集合。  

假定 $\mathcal{M}$ 不变(若 $\mathcal{M}$ 有变，在迭代过程中更新即可)，则此目标函数的梯度向量为：

$$
\begin{align*}
\frac{\partial L(\mathbf{w}, b)}{\partial \mathbf{w}} &= -\sum_{i \in \mathcal{M}} y_i \mathbf{x}_i \\
\frac{\partial L(\mathbf{w}, b)}{\partial b} &= -\sum_{i \in \mathcal{M}} y_i
\end{align*}
$$

使用梯度下降法，沿着负梯度方向更新，则参数的更新规则为

$$
\begin{align*}
\mathbf{w} &\leftarrow \mathbf{w} + \eta \sum_{i \in \mathcal{M}} y_i \mathbf{x}_i \\
b &\leftarrow b + \eta \sum_{i \in \mathcal{M}} y_i
\end{align*}
$$

其中 $\eta$ 为学习率，也称为步长。

通过迭代，可使损失函数 $L(w,b)$ 不断减小，直到变为0为止。

感知机算法的直观解释：当一个样本点被错误分类，即出现分离超平面的错误一端，则调整参数 $(w,b)$，使得分离超平面向该误分类点的一侧移动，来减少此误分类点与超平面的距离，知道正确分类位置。

参考SVM的内容可知，对于线性可分的数据，感知机一定会收敛。

这表明，只要给予足够的数据，感知机具备学得参数 $(w,b)$ 的能力，仿佛拥有感知世界的能力，故名感知机。

对于线性可分的数据，感知机虽然一定会收敛，但从不同的初始值出发，一般会得到不同的分离超平面，无法得到唯一解。

由于所得超平面未必是**最优分离超平面**，故感知机的泛化能力也没有保证。

如果数据为线性不可分，则感知机的算法不会收敛。

感知机最严重的缺陷是，它的决策边界依然是线性函数。可将感知机的预测函数写为

$$
f(X) = sign(w'X+b)
$$

其中，$sign(\cdot)$ 为符号函数，满足

$$
\text{sign}(z) =
\begin{cases}
1 & \text{if } z \geq 0 \\
-1 & \text{if } z < 0
\end{cases}
$$

虽然符号函数 $sign(\cdot)$ 为非线性，但感知机的决策边界为 $w'X+b = 0$，依然为线性函数。

感知机无法适用于决策边界为非线性的数据。

例 感知机无法识别“异或函数”。在逻辑学中，有几个常见的逻辑运算，包括“与”(AND)、“或”(OR)、“与非”(NOT AND)、“异或”(Exclusive Or，简记 XOR)。

“异或”是一种排他性(exclusive)的“或”，即当二者取值不同时为“真”(TRUE)，而当二者取值相同时即为“假”(FALSE)。

![fix](/images/imageANN-2.png)

逻辑判断 TRUE 记为 1(以蓝点表示)，而 FALSE 记为 0(以黑点表示)。

对于XOR的运算，由于TRUE和FALSE分别分布在两个对角上，故无法找到线性的决策边界，存在非线性的决策边界。感知机连基本的异或函数都无法区分，功能十分有限。

当时学界普遍认为感知机无发展前途，使人工神经网络研究陷入低谷。

### 神经网络的模型

事实上，在感知机的基础上，并不难得到非线性的决策边界。

只要引入多层神经元，经过两个及以上的非线性激活函数迭代之后，即可得到非线性的决策边界。

非线性激活函数是关键：如果使用线性的激活函数，则无论叠加或嵌套多少次（相当于复合函数），所得的结果还是线性函数。

首先，考虑具有多个输出结果的感知机：

![fix](/images/imageANN-3.png)

图中共有两个输出变量 $\hat y_1$ 何 $\hat y_2$。

其中，$z_1 \equiv b_1 + \sum_{i=1}^{p} w_{i1} x_i$ 与 $z_2 \equiv b_2 + \sum_{i=1}^{p} w_{i2} x_i$，均为在施加激活函数之前的加总值；而 $f(\cdot)$ 为激活函数。

其次，对于上图中的两个输出结果，可重新作为输入变量，经过加权求和后，再次施以激活函数。

![fix](/images/imageANN-4.png)

在图中，最终输出结果为  

$$
\hat{y} = f\left( b^{(2)} + w_1^{(2)} f(z_1) + w_2^{(2)} f(z_2) \right)
$$  

即对 $f(z_1)$ 与 $f(z_2)$ 再次加权求和，然后再施加激活函数 $f(\cdot)$。  

函数所对应的决策边界为非线性的。

在图中，最左边为输入层，中间为隐藏层，而最右边为输出层。

之所以将中间层称为隐藏层，因为该层的计算在算法内部进行，从外面并不可见。

隐藏层也可有多个，输出层也可以有多个输出结果。

![fix](/images/imageANN-5.png)

![fix](/images/imageANN-6.png)

这种标准的神经网络，称为前馈神经网络，因为输入从左向右不断前馈。也称为全连接神经网络，因为相邻层的所有神经元都相互连接。

针对特殊的数据类型。可能还需要特别的网络结构，比如卷积神经网络（适用于图像识别）、循环神经网络（适用于自然语言等时间序列）等。

如果神经网络的隐藏层很多，则称为**深度神经网络**，简称**深度学习**

### 神经网络的激活函数

感知机使用符号函数作为激活函数，是不连续的**阶梯函数**，不便于进行最优化。

激活函数必须为非线性函数，因为这个世界的本质是非线性的。神经网络模型中常用的激活函数包括：

1. S型函数（sigmoid Function）。狭义的S型函数就是逻辑分布的累积分布函数，其表达式为：
    $$
    \Lambda(z) \equiv \frac{1}{1 + e^{-z}}
    $$

    ![fix](/images/imageANN-7.png)

    此函数也用于逻辑回归，在机器学习中有时记 $\sigma(z)$，也称为逻辑函数。

    Sigmoid函数可视为一种挤压函数，即把输入的任何实数都积压到（0，1）区间。

    输入值 $z$ 在0附近时，sigmoid函数近似为线性函数；而当输入值 $z$ 靠近两端，则对输入进行抑制。

    输入越小，则输出越接近0；输入越大，则输出越接近1；此特点与生物神经元类似。

    由于sigmoid函数的输出值介于0与1之间，故可将其解释为概率分布。

    与感知机所用的阶梯激活函数相比，sigmoid函数为连续可导，其数学性质更好。

    但当输入靠近两端时，sigmoid函数的导数趋于0，故在训练神经网络时，可能导致梯度消失的问题，使得梯度下降法失效。

2. 双曲正切函数。即上图右边的函数图像。

    双曲正切函数是一种广义的S型函数，因为它的形状也类似于拉长的英文大写字母S，其表达式为：

    $$
    \tanh(z) \equiv \frac{e^z - e^{-z}}{e^z + e^{-z}}
    $$

    Tanh函数可看作是将Logistic函数进一步拉伸到（-1，1）区间，二者有如下关系：

    $$
    \tanh(z) = 2\Lambda(2z) - 1
    $$

    其中，上式右边可写为

    $$
    \begin{align*}
    2\Lambda(2z) - 1 &= 2 \cdot \frac{1}{1 + e^{-2z}} - 1 = \frac{1 - e^{-2z}}{1 + e^{-2z}} \\
    &= \frac{e^z - e^{-z}}{e^z + e^{-z}} = \tanh(z)
    \end{align*}
    $$

    Tanh函数的输出是零中心化的，而Logistic函数的输出是一定大于0。非零中心化的输出，会使得下一层的神经元输入发生偏置转移，使得梯度下降的收敛速度变慢。

    Tanh函数也是两端饱和的，依然可能发生梯度消失的问题。

3. 修正线性单元，也称线性整流函数。

    为了解决Logistic函数与Tanh函数的两端饱和问题提出了ReLU函数，称为目前深度神经网络中经常使用的激活函数。

    $$
    \text{ReLU}(z) \equiv \max(0, z) =
    \begin{cases}
    z & \text{if } z \geq 0 \\
    0 & \text{if } z < 0
    \end{cases}
    $$

    ReLU实际上是一个斜坡函数。当输入 $z\geq 0$，其输出也是 $z$，即所谓线性单元。而当输入 $z <0$ 时，则将输出修正为0.

    ![fix](/images/imageANN-8.png)

    以ReLU函数作为激活函数，其计算非常方便。相比于S型函数的两端饱和，ReLU函数为左饱和函数，即当 $z \to -\infty$ 时，ReLU函数的导数趋向于0。

    当 $z>0$ 时，ReLU函数的导数恒等于1，这可在一定程度上缓解神经网络训练中的梯度消失问题，加快梯度下降的收敛速度。

    ReLU函数被认为具有生物学上的解释，比如单侧抑制、宽兴奋边界。

    在生物神经网络中，同时处于兴奋状态的神经元一般很稀疏。S型激活函数会导致非稀疏的神经网络，而ReLU激活函数可导致较好的稀疏性。

    但当 $z<0$ 时，ReLU函数的导数恒等于0，这导致神经元在训练时可能死亡，称为死亡ReLU问题。

    所谓神经元死亡，就是无论该神经元输入什么，其输出永远是0，故无法更新其输入的权重。

4. 泄漏ReLU（Leaky ReLU）如上图右边的函数。

    解决死亡ReLU问题的一种方式是，当输入 $z<0$ 时，依然保持一个很小的梯度 $\gamma>0$。折使得当神经元处于非激活状态时，也有一个非零梯度可更新参数，避免永远不能被激活。泄漏ReLU函数定义为

    $$
    \text{LReLU}(z) \equiv
    \begin{cases}
    z & \text{if } z \geq 0 \\
    \gamma z & \text{if } z < 0
    \end{cases}
    $$

    其中 $\gamma$ 是一个很小的正数，比如0.01。当 $\gamma < 1$ 时，泄漏ReLU可写为

    $$
    \text{LReLU}(z) = \max(z, \gamma z)
    $$

5. 软加函数

    ReLU函数并不光滑，而且在 $z < 0 $ 时，导数一直为0.

    软加函数可视为 ReLU 函数的光滑版本，正好弥补ReLU的这些缺点。

    Softplus函数可定义为：

    $$
    \text{Softplus}(z) \equiv \ln(1 + e^z)
    $$

    Softplus 函数也具有单侧抑制、宽兴奋边界的特性。

    但没有 ReLU 函数的稀疏激活性(因为 Softplus 函数的导数永远为正)

    ![fix](/images/imageANN-9.png)

### 通用函数近似器

前馈神经网络具有很强的函数拟合能力。

在一定程度上，神经网络可作为一种通用近似器来使用：包含单一隐藏层的前馈神经网络模型，只要其神经元数目足够多，则可以任意精度逼近任何一个在有解闭集上定义的连续函数。

首先，包含单隐藏层的前馈神经网络所代表的函数可写为

$$
G(\mathbf{x}) = \sum_{i=1}^{m} \alpha_i f(\mathbf{w}_i' \mathbf{x} + b_i)
$$

其中 $(w_i,b_i)$ 为第 $i$ 个神经元的权重与偏置参数，$f(\cdot)$ 为激活函数，$\alpha_i$ 为连接隐藏层与输出层的参数，而 $m$ 为神经元的数目。

通用近似定理表明，任意有界闭集上的连续函数，都可找到上述函数，使二者的距离任意接近。

**通用近似定理**令 $f(\cdot)$ 为一个合适的激活函数(详见下文)，$\mathcal{I}_p$ 是一个 $p$ 维的单位超立方体(unit hypercube) $[0, 1]^p$，而 $C(\mathcal{I}_p)$ 是定义在 $\mathcal{I}_p$ 上的所有连续函数之集合。  

对于任意一个函数 $g \in C(\mathcal{I}_p)$，给定任意小的正数 $\varepsilon > 0$，则存在一个正整数 $m$(即神经元数目)，一组实数 $(\alpha_i, b_i)$，以及实数向量 $\mathbf{w}_i \in \mathbb{R}^p$，$i = 1, \cdots, m$，使得所定义的函数 $G(\mathbf{x})$，可以任意地接近 $g(\mathbf{x})$，即  

$$
\big| G(\mathbf{x}) - g(\mathbf{x}) \big| < \varepsilon, \quad \forall \mathbf{x} \in \mathcal{I}_p \tag{15.20}
$$

通用近似定理在任意 $p$ 维实数空间 $R^p$ 的有界闭集上依然成立。

通用近似定理的激活函数可采取不同形式的非线性函数，既包括非常数(nonconstant)、有界(bounded)且单调递增的连续函数(例如 S 型函数、双曲正切函数)，也包括无界(unbounded)且单调递增的连续函数(例如ReLU)，甚至允许不连续函数(例如阶梯函数)。

通用近似定理表明，神经网络可作为“万能”函数来使用

但通用近似定理只是说明，对于任意有界闭集上的连续函数，都存在与它非常接近的单隐藏层前馈神经网络。而并未给出找到此神经网络的方法，也不知道究竟需要多少神经元才能达到既定的接近程度 $\varepsilon$。

在实际应用中，一般并不知道真实函数 $g(X)$，而我们更关心神经网络 $G(X)$ 的泛化能力。

由于神经网络强大的拟合能力，容易在训练集上过拟合，需要避免过拟合以降低测试误差。

### 神经网络的损失函数

未经训练的神经网络就像空白的大脑，并不具备预测与分类的能力。

“训练”意味着估计神经网络模型的诸多参数。对于神经网络而言，知识就储存在这些参数中。

神经网络的通常训练方法为，在参数空间使用梯度下降法，使损失函数最小化。神经网络的损失函数之一般形式可写为：

$$
\mathbf{W}^* = \mathop{\text{argmin}}\limits_{\mathbf{w}} \frac{1}{n} \sum_{i=1}^n L\left(y_i, G\left(\mathbf{x}_i; \mathbf{W}\right)\right)
$$

其中，参数矩阵 $W$ 包含神经网络模型的所有参数（包括偏置），其每一列对应于神经网络每一层的参数。

$W^*$ 为 $W$ 的最优值，$G(X_i;W)$ 为神经网络对观测值 $X_i$ 所作的预测，即 $\hat y_i$，而 $L(y_i,\hat y_i)$ 为损失函数。

整个样本的损失函数为每个观测值的损失 $L\left( y_i, G(\mathbf{x}_i; \mathbf{W}) \right)$ 的平均值。

对于响应变量为连续的回归问题，一般使用平方损失函数，最小化训练集的均方误差：

$$
\mathbf{W}^* = \mathop{\text{argmin}}\limits_{\mathbf{w}} \frac{1}{n} \sum_{i=1}^n \big( y_i - G(\mathbf{x}_i; \mathbf{W}) \big)^2
$$

对于响应变量为离散的分类问题，则一般使用交叉熵损失函数，即多项逻辑回归的对数似然函数的负数。

对于二分类问题，一般使用交叉熵损失函数，即逻辑回归的对数似然函数的负数。

$$
\mathbf{W}^* = \mathop{\text{argmin}}\limits_{\mathbf{w}} -\frac{1}{n} \sum_{i=1}^n \Big[ y_i \ln G(\mathbf{x}_i; \mathbf{W}) + (1 - y_i) \ln \big( 1 - G(\mathbf{x}_i; \mathbf{W}) \big) \Big]
$$

### 神经网络的算法

由于神经网络通常包含很多参数，且涉及较多非线性激活函数，故一般不便于求二阶导数，无法使用牛顿法。

常使用梯度下降法训练神经网络，故需要计算神经网络 $G(X_i;W)$ 的梯度向量。计算梯度向量的枉法为反向传播算法，简称BP。

对于多层的神经网络，越靠近网络右边的参数，其导数越容易计算，因为他们离输出层更近。

反向传播算法就是使用微积分的链式法则，将靠左边的参数的导数，递归地表示为靠右边的参数的导数的函数。

![fix](/images/imageANN-10.png)

参考上图：对于一个简单的感知机，所有的数据都已经算出来，那么该如何优化调整 $w$ 何 $b$ 这两个参数来使损失函数最小化呢？

根据之前的感知机算法中的梯度下降算法，我们需要计算损失函数 $L$ 对 $w$ 和 $b$ 的梯度值，即偏导数，再沿着偏导数的方向更新两个参数。

为了更容易计算损失函数 $L$ 对 $w$ 和 $b$ 的偏导，我们计算一个中间量，即 $l$ 对 $y$ 的偏导，这样 $L$ 对 $w$ 和 $b$ 的偏导就可以写成如下形式：

![fix](/images/imageANN-11.png)

这便是求导的链式法则。通过这种链式求导就可以求出参数梯度的解析式。这种从后向前计算参数梯度值的方法就是反向传播算法。

接下来看有两个隐藏层的情况：

![fix](/images/imageANN-12.png)

还是求出损失函数对于每一个参数的梯度值。

在计算梯度向量时，需要知道每一层所有神经元的净输出 $Z_j^{(l)}$ 与激活值 $a_j^{(l)}$。

故首先需要将每个观测值 $(X_i,y_i)$ 输入神经网络，从左向右进行正向传播，得到每一层所有神经元的 $Z_j^{(l)}$ 与 $a_j^{(l)}$。

然后通过反向传播，计算每一层的误差 $\delta_j^{(l)}$；再根据链式法则方程计算每一层参数的偏导数，并通过梯度下降法更新参数。

在训练神经网络之前，一般建议将全部特征变量归一化，即最小值变为0，而最大值变为1。或标准化：均值变为0，标准差变为1。

这时因为如果特征变量的取值范围差别较大，则会影响神经网络的权重参数，不利于神经网络的训练。

对于回归问题，若对特征变量进行归一化处理，则所有特征变量 $x\in[0,1]$，此时建议也将响应变量作归一化处理，便于模型的训练与预测。

在选择参数矩阵 $W$ 的初始值 $W_0$ 时，通常从标准正态分布 $N(0,1)$ 或取值介于 $[-0.7,0.7]$ 的均匀分布中随机抽样，这样有利于不同神经元之间的分化，避免趋同。

### 神经网络的小批量训练

对于神经网络的训练，考虑最小化损失函数：

$$
\min_{\mathbf{w}} \frac{1}{n} \sum_{i=1}^n L\left( y_i, G(\mathbf{x}_i; \mathbf{W}) \right)
$$

其中 $G(X_i;W)$ 为一个前馈神经网络模型。由于 $G(X_i;W)$ 通常时一个高度非线性的函数，故上式中的求和式无法进一步简化。

如果样本容量为 100 万，则目标函数中共有 100 万项相加(对应于 100万个观测值的损失之和)

在求损失函数的梯队向量时，需要对每个观测值的损失 $L(y_i,G(X_i;W))$ 分别求梯度向量，然后将这一百万个梯度向量加总。

如果样本容量很大，则通常的梯度下降法过于费时，并不可行。

一种解决方法是，每次**无放回地**(without replacement)随机抽取一个观测值 $(\mathbf{x}_i, y_i)$，计算该观测值的梯度向量 $\frac{\partial L\big(y_i, G(\mathbf{x}_i; \mathbf{W})\big)}{\partial \mathbf{W}}$，然后沿着负梯度方向，使用合适的学习率 $\eta$，进行参数更新：  

$$
\mathbf{W} \leftarrow \mathbf{W} - \eta \frac{\partial L\big(y_i, G(\mathbf{x}_i; \mathbf{W})\big)}{\partial \mathbf{W}}
$$

这种方法称为**随机梯度下降SGD**，SGD的计算速度大大加快，因为每次仅需计算一个观测值的梯度向量。

但单个观测值的负梯度方向并不一定与整个样本的负梯度方向一致或类似，这导致随机梯度下降的过程充满噪音，有时反而会使损失函数上升。

经过不断迭代后，SGD的长期趋势依然指向损失函数的最小值。

为了克服随机梯度下降的不稳定与噪音，**小批量梯度下降**应运而生：每次五方回的随机抽取部分观测值，比如 $B$ 个观测值，计算这 $B$ 个观测值的梯度向量，再作平均，然后进行参数更新：

$$
\mathbf{W} \leftarrow \mathbf{W} - \eta \frac{1}{B} \sum_{i=1}^B \frac{\partial L\big(y_i, G(\mathbf{x}_i; \mathbf{W})\big)}{\partial \mathbf{W}}
$$

![fix](/images/imageANN-13.png)

由于 $B$ 通常不大，故小批量梯度下降依然计算较快。

经过对 $B$ 个观测值的平均之后，可得到对于全样本的真实梯度向量更为准确的估计，故小批量梯度下降的过程更为稳定。

传统的梯度下降法，在计算梯度向量时，同时考虑所有观测值，故称为批量梯度下降。

以上三种梯度下降的方法，主要区别在于批量规模，对于随机梯度下降，批量规模 $B$ 为1

对于批量梯度下降，则 $B$ 为 $n$

另一个相关概念为**轮**：在训练模型时，将所有样本数据都用了一遍，即为一轮，进过一轮之后，所有观测值都有机会影响参数更新。

对于批量梯度下降，每次迭代都用全部样本计算梯度向量，故一次迭代就是一轮。

对于随机梯度，每次仅用一个观测值计算梯度向量，故 $n$ 次迭代才算一轮。

### 神经网络的正则化

包含多个隐藏层的深度神经网络是表达能力能强的模型，可学习输入与输出之间非常复杂的函数关系。

如果进行很多轮的训练，则容易导致过拟合。

需要对神经网络模型进行正则化处理。常见的正则化方法包括：

1. **早停**。这意味着，提前停止训练，则不必等到神经网络达到损失函数或训练误差的最小值。

    如何知道在何时停止训练呢？

    一般建议将全样本随机的一分为三，即训练集、验证集与测试集。

    首先，在训练集中进行训练，并同时将学得的神经网络模型同步地在验证集中作预测，并计算验证误差。

    其次，当验证误差开始上升时，即停止训练。

    最后，将所得的最终模型在测试集中进行预测，并计算测试误差。

    ![fix](/images/imageANN-14.png)

2. **丢包**。为避免过拟合，提出在训练样本时，随机地让某些神经元的激活值取值为0，即让某些神经元死亡，而不再影响神经网络。

    通常随机地丢弃 $50%$ 的神经元，这样可以迫使神经网络不过分依赖于某些神经元而导致过拟合。

    ![fix](/images/imageANN-15.png)

3. **惩罚**。在神经网络模型的目标函数中，可引入类似于岭回归的 $L_2$ 惩罚项，进行正则化。

    $$
    \min_{\mathbf{w}} \frac{1}{n} \sum_{i=1}^n L\left( y_i, G(\mathbf{x}_i; \mathbf{W}) \right) + \lambda \|\mathbf{W}\|_2^2
    $$

    其中 $||W||_2$ 为矩阵 $W$ 的二范数，即 $W$ 所有元素的平方和开根号；而 $\lambda>0$ 为调节参数，可通过交叉验证或验证集法确定。

    这是一个收缩估计量，在神经网络中称为**权重衰减**

### 卷积神经网络

在计算机视觉领域，如果使用前馈神经网络进行图像识别，则会导致参数太多、丢失空间信息等问题。

1. **参数太多**：假设输入图像大小为 $100\times 100 \times 3$ 的张量，即共度为100的像素，宽度为100像素，且包含3个颜色通道（RGB）。可将此张量排列成一个30000维的长向量。

    第1个隐藏层的每个神经元，到输入层都有3万个相互独立的连接。

    随着隐藏层神经元的数目增加，参数的规模也急剧上升。这导致神经网络的训练效率很低，易出现过拟合。

2. **丢失空间信息**：在上述例子中，在将 $100\times 100 \times 3$ 的张量扁平化为3万维长向量时，事实上丢失了原来图像中空间响铃的信息。

    对于自然图像中的物体，一般都有局部不变性的特点，比如在平移、旋转与尺度缩放等操作下不影响其内在信息。

    由于全连接神经网络失去空间相邻信息，故很难提取这种局部不变特征，这导致神经网络的预测能力下降。

**卷积神经网络**，简记CNN，是收到生物学中**感受野**的机制启发而提出。

所谓感受野，主要是指视觉、听觉等神经系统汇中一些神经元的特性，即神经元只接收其所支配的刺激区域的信号。

相邻神经元的感受野有部分重叠。

如果神经元的感受野不受限制，则每个神经元都会太忙，这相当于全连接网络中的每个隐藏层神经元均需接受巨量的输入信息。

在数学上，对于感受野机制的抽象，就是所谓**卷积运算**，使用卷积运算的神经网络，称为卷积神经网络。

作为卷积运算的示例，假设图像为简单的 $5\times5$ 像素矩阵，我们相拥一个 $3\times3$ 的过滤器，也称**卷积核**，对此图像进行卷积运算

![fix](/images/imageANN-16.png)

卷积核通过训练得出，相当于前馈神经网络中的 $w$ 参数。

将卷积核在输入图像上滑动，并将相应元素相乘，再把结果加总。

先讲过滤器与图像左上角的 $3\times3$ 矩阵按元素相乘，然后再加总，结果为5。

![fix](/images/imageANN-17.png)

作为卷积运算的第二步，将过滤器向右移动，重复上一步操作，再加总。

继续重复以上步骤，直到将过滤器在图像上全部滑动一遍，所得结果如下所示。

![fix](/images/imageANN-18.png)

卷积的意义何在？

在此例中，过滤器与图像左上角的卷积运算结果为最高值5，这表明图像左上角的局部区域与过滤器的形状最为相似。

过滤器与图像上方中间的卷积运算结果为最低值0，这说明图像上方中间的局部区域与过滤器的形状差别最大。

其他区域的卷积运算结果则介于二者之间。说明这些区域与过滤器的形状有着不同的相似度。

卷积运算的结果相当于度量了图像中每个局部区域与过滤器的相关程度，故卷积运算也称为互相关。

卷积运算考研起到中图像中抽取特征的作用，而卷积的结果则称为特征映射。

在使用过滤器与整个图像进行卷积运算时，过滤器的权重参数并不改变，故卷积神经网络具有权值共享的特点，科节省参数。

过滤器中的参数本身，也是神经网络学习的结果。

由于一个卷积核智能提取一种局部特征，故需要使用多个卷积核来抽取图像的不同特征，由此构成卷积核。

经过卷积层运算之后，所得的特征映射的维度依然很高。为此需要通过一个**汇聚层**，也称池化层，进一步汇聚与压缩信息。

所谓汇聚，也称为下采样或子采样，一般有以下两种常用的实现方式。

1. **最大汇聚**，即在特征映射中，抽取每个局部区域的最大值。

   ![fix](/images/imageANN-19.png)

2. **平均汇聚**，即在特征映射中，抽取每个局部区域的平均值。

    ![fix](/images/imageANN-20.png)

    经过汇聚层之后，图像的局部特征已被抽取并压缩，得到为数较少的特征变量，然后可连接到通常的前馈神经网络。

对于分类问题，卷积神经的最后一层一般使用多项逻辑的软极值函数作为激活函数。其输出结果就是个类别的题哦啊贱概率，并以条件概率最大者作为预测类别。

使用卷积神经网络作为图像识别的基本网络结果，参加如下图：

![fix](/images/imageANN-21.png)

最左边为输入层，之后为卷积层，接着是汇聚层，然后将汇聚层的输出结果，接入全连接层进行分类识别。

对于比较复杂的图像识别任务，仅使用一个卷积层和一个汇聚层还不够，因为这只是提取了图像中的初级特征，比如边缘与黑点。

在此之后，再加上一个卷积层与一个汇聚层，以提取中级特征，比如眼、耳、鼻。最后，还需再加上一个卷积层与一个汇聚层，才能提取高级特征吗比如面部结构。

对于卷积神经网络的训练，依然可使用反向传播算法。汇聚层中并没有需要估计的参数。对于卷积层来说，本质上可视为部分连接的隐藏层，而未连接的权重参数则预先设为0.
