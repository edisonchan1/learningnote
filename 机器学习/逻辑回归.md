## 逻辑回归

### 逻辑回归

在监督学习中，存在大量关于“是与否”的二分类问题，在各行业有着广泛的应用。

以过滤垃圾邮件为例，假设响应变量只有两种可能取值，即$y = 1$（垃圾邮件），$y = 0$（正常邮件）。
这种0-1变量称为虚拟变量或者"哑变量"。

记特征向量$\boldsymbol{x}_i \equiv (x_{i1} \ x_{i2} \ \cdots \ x_{ip})'$，比如不同词汇出现的频率。

最简单的建模方法为“线性回归模型”
$$
y_i = \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} + \varepsilon_i = \boldsymbol{x}_i' \boldsymbol{\beta} + \varepsilon_i$$
其中，参数向量$\boldsymbol{\beta} \equiv (\beta_1 \beta_2 \cdots \beta_p)'$

线性概率模型的优点在于，计算方便(就是 OLS 估计)，且容易得到边际效应(即回归系数)。
但线性概率模型一般并不适合作预测。

明知 y的取值非 0 即 1，但根据线性概率模型所作的预测值却可能出现$\hat y >1$
或者$\hat y<0$的不现实情形。
![fix](//images/images/Pasted%20/images/image%2020250707083510.png)
可以理解为
$$\hat{y} =
\begin{cases}
1, & f(x) > 0.5 \\
0, & f(x) < 0.5
\end{cases}$$

为使 y的预测值总是介于[0,1]之间，在给定 x的情况下，考虑 y的两点分布概率：
将$X\beta$限制在[0,1]之间。
$$\begin{cases}
\mathrm{P}(y=1 \mid \boldsymbol{x}) = F(\boldsymbol{x}, \boldsymbol{\beta}) \\
\mathrm{P}(y=0 \mid \boldsymbol{x}) = 1 - F(\boldsymbol{x}, \boldsymbol{\beta})
\end{cases}
$$
其中，函数$F(X,\beta)$称为连接函数，将特征向量$X$与响应变量$y$连接起来。

连接函数的选择具有一定的灵活性。用过选择合适的连接函数$F(x,\beta)$（例如，某随机变量的累次分布函数）可以保证$0 \leq \hat{y} \leq 1$。

由两点分布的期望公式可得，在给定X的情况下，y 的条件期望为
$$E(y|x) = p(y=1|x)$$
可将模型的拟合值(预测值)理解为事件“$y = 1$"的发生概率。如果$F(X,\beta)$为标准正态的累积分布函数，则
$$\mathrm{P}(y=1 \mid \boldsymbol{x}) = F(\boldsymbol{x}, \boldsymbol{\beta}) = \Phi(\boldsymbol{x}'\boldsymbol{\beta}) \equiv \int_{-\infty}^{\boldsymbol{x}'\boldsymbol{\beta}} \phi(t) \, dt$$
其中$\Phi(\boldsymbol{x}'\boldsymbol{\beta})$和$\phi(t)$分别为标准正态的密度函数与累积分布函数。
此模型称为**概率单位模型(Probit)**。

如果连接函数$F(X,\beta)$为“逻辑分布”的累积分布函数
则$$\mathrm{P}(y=1 \mid \boldsymbol{x}) = F(\boldsymbol{x}, \boldsymbol{\beta}) = \Lambda(\boldsymbol{x}'\boldsymbol{\beta}) \equiv \dfrac{\exp(\boldsymbol{x}'\boldsymbol{\beta})}{1 + \exp(\boldsymbol{x}'\boldsymbol{\beta})} = \dfrac{1}{1 + \exp(-\boldsymbol{x}'\boldsymbol{\beta})}$$
其中，函数的定义为$\Lambda(~)$的定义为

$$\Lambda(z) \equiv \frac{e^z}{1 + e^z} = \frac{1}{1 + e^{-z}}$$

此模型称为逻辑回归(Logistic Regression)或“逻辑斯蒂回归”，简记 Logit。

对逻辑函数$\Lambda(~)$求导数，即可得到逻辑分布的密度函数：
$$\begin{align*} \frac{d\Lambda(z)}{dz} &= \frac{d\bigl(1 + e^{-z}\bigr)^{-1}}{dz} = (-1)\bigl(1 + e^{-z}\bigr)^{-2} e^{-z} (-1) \\ &= \frac{1}{1 + e^{-z}} \cdot \frac{e^{-z}}{1 + e^{-z}} = \Lambda(z)\bigl[1 - \Lambda(z)\bigr] = \frac{e^z}{\bigl(1 + e^z\bigr)^2} \end{align*}$$
在 Python 中，画逻辑分布的密度函数与累积分布函数，可输入命令

```python
import numpy as np

import matplotlib.pyplot as plt

from scipy.stats import logistic

fig, ax = plt.subplots(1,2,figsize=(10,5))

x = np.linspace(-5, 5, 100)

ax[0].plot(x, logistic.pdf(x), 'r-', lw=2)

ax[0].vlines(0,0, .255,lw = 1)

ax[0].hlines(0, -5, 5, lw = 1)

ax[0].set_title('Logistic Density Function')

ax[1].plot(x, logistic.cdf(x), 'b-', lw=2)

ax[1].vlines(0,0, 1,lw = 1)

ax[1].hlines(0, -5, 5, lw = 1)

ax[1].set_title('Logistic Cumulative Distribution Function')
```

其中，第 2 个命令从 SciPy 的 stats 子模块导入 logistic 类(class)；logistic.pdf()与 logistic.cdf()分别为逻辑分布的密度函数与累积分布函数。第四个命令将画布分为$1\times 2$的画轴，而整个画布尺寸为10英寸宽与5英寸高，结果如下：
![fix](//images/images/Pasted%20/images/image%2020250707101620.png)
逻辑分布的密度函数关于原点对称，期望为 0。

由于逻辑分布的累积分布函数之形状类似于(拉长的)大写英文字母 S，故在机器学习中常称为 S 型函数(sigmoid function)，记为$\sigma(z)$，广泛用于神经网络模型。
有时，S 型函数(sigmoid function)也泛指所有形如 S 的函数。

在统计学中，将 Probit 与 Logit 模型统称为广义线性模型(Generalized Linear Model，简记 GLM)，因为二者的模型均可写为如下形式

$$\mathrm{P}(y=1 \mid \boldsymbol{x}) = F(\boldsymbol{x}'\boldsymbol{\beta})$$
在上式中，非线性的连接函数$F(~)$作用于线性函数$X'\beta$，由此决定“$y = 1$”的条件概率$P(y=1|x)$。完成从线性函数到非线性概率的转换之间的桥梁：连接函数。

如果使用某连续变量的累积分布函数作为连接函数$F(~)$，则$F(~)$为严格单调函数，故其逆函数$F^{-1}(~)$存在

以此逆函数$F^{-1}(~)$作用于方程的两边，则可得到一个线性模型：$$F^{-1}\bigl[ \mathrm{P}(y=1 \mid \boldsymbol{x}) \bigr] = \boldsymbol{x}'\boldsymbol{\beta}$$
对于Logit模型，此逆函数$F^{-1}(~)$为“对数几率”，也称为“逻辑变换”，他将概率变换为相应的对书几率。

### 最大似然估计

 Logit 模型本质上为非线性模型（连接函数是非线性模型），故一般使用最大似然估计(Maximum Likelihood Estimation，简记 MLE)，而不使用最小二乘法。

考虑第i个观测值，由于 $y_i = 0$或 1，故 $y_i$服从“两点分布”。这是“二项分布”（binomial distribution）的特例。

第i个观测数据的条件概率为
$$\mathrm{P}(y_i \mid \boldsymbol{x}_i) =
\begin{cases}
\Lambda(\boldsymbol{x}_i'\boldsymbol{\beta}), & \text{if } y_i = 1 \\
1 - \Lambda(\boldsymbol{x}_i'\boldsymbol{\beta}), & \text{if } y_i = 0
\end{cases}$$
其中，$\Lambda(z) \equiv \frac{e^z}{1 + e^z} = \frac{1}{1 + e^{-z}}$为逻辑分布的累积分布函数。
将其更紧凑地写成
$$\ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left[ y_i \ln\Lambda(\boldsymbol{x}_i'\boldsymbol{\beta}) + (1-y_i) \ln\bigl(1-\Lambda(\boldsymbol{x}_i'\boldsymbol{\beta})\bigr) \right]$$
（分别带入$y_i = 0$或1即可验证）

给定相互独立的训练样本$\left\{ \mathbf{X}_i, y_i \right\}_{i=1}^n$，则整个样本的联合概率为其中，$y_i =0$或1
$$P(\mathbf{y} \mid \mathbf{X}) = \prod_{i=1}^n \left[ \Lambda(\boldsymbol{x}_i' \boldsymbol{\beta}) \right]^{y_i} \left[ 1 - \Lambda(\boldsymbol{x}_i' \boldsymbol{\beta}) \right]^{1 - y_i}$$
该样本的似然函数
$$L(\boldsymbol{\beta} \mid \mathbf{y}, \mathbf{X}) = \prod_{i=1}^n \left[ \Lambda(\boldsymbol{x}_i' \boldsymbol{\beta}) \right]^{y_i} \left[ 1 - \Lambda(\boldsymbol{x}_i' \boldsymbol{\beta}) \right]^{1 - y_i}
$$
将上式取对数，可得对数似然函数(loglikelihood function)，并选择参数$\beta$使其最大化：
$$\max_{\boldsymbol{\beta}} \ln L(\boldsymbol{\beta} \mid \mathbf{y}, \mathbf{X}) = \sum_{i=1}^n y_i \ln\left[ \Lambda(\boldsymbol{x}_i' \boldsymbol{\beta}) \right] + \sum_{i=1}^n (1 - y_i) \ln\left[ 1 - \Lambda(\boldsymbol{x}_i' \boldsymbol{\beta}) \right]
$$
所得最优解$\hat \beta$即为最大似然估计。由于目标函数为非线性函数，故不存在解析解。
则利用逻辑函数的导数公式，可得到对数似然函数的梯度向量。
利用$$\begin{align*} \frac{d\Lambda(z)}{dz} &= \frac{d\bigl(1 + e^{-z}\bigr)^{-1}}{dz} = (-1)\bigl(1 + e^{-z}\bigr)^{-2} e^{-z} (-1) \\ &= \frac{1}{1 + e^{-z}} \cdot \frac{e^{-z}}{1 + e^{-z}} = \Lambda(z)\bigl[1 - \Lambda(z)\bigr] = \frac{e^z}{\bigl(1 + e^z\bigr)^2} \end{align*}$$
则有
$$\begin{align*}
\frac{\partial \ln L(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}
&= \frac{\partial \sum_{i=1}^n y_i \ln\left[ \Lambda(\mathbf{x}_i' \boldsymbol{\beta}) \right]}{\partial \boldsymbol{\beta}}
+ \frac{\partial \sum_{i=1}^n (1-y_i) \ln\left[ 1 - \Lambda(\mathbf{x}_i' \boldsymbol{\beta}) \right]}{\partial \boldsymbol{\beta}} \\
&= \sum_{i=1}^n y_i \frac{1}{\Lambda_i} \Lambda_i (1-\Lambda_i) \mathbf{x}_i
- \sum_{i=1}^n (1-y_i) \frac{1}{1-\Lambda_i} \Lambda_i (1-\Lambda_i) \mathbf{x}_i \\
&= \sum_{i=1}^n y_i (1-\Lambda_i) \mathbf{x}_i
- \sum_{i=1}^n (1-y_i) \Lambda_i \mathbf{x}_i \\
&= \sum_{i=1}^n (y_i - \Lambda_i) \mathbf{x}_i
\end{align*}
$$
其中，记$\Lambda_i \equiv \Lambda(\mathbf{x}_i' \boldsymbol{\beta})$。
对梯度向量再次求偏导，可得黑塞矩阵：
$$\begin{align*} \frac{\partial^2 \ln L(\boldsymbol{\beta})}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}'} &= \frac{\partial \sum_{i=1}^n \left( y_i - \Lambda(\mathbf{x}_i' \boldsymbol{\beta}) \right) \mathbf{x}_i}{\partial \boldsymbol{\beta}'} = \underbrace{\frac{\partial \sum_{i=1}^n y_i \mathbf{x}_i}{\partial \boldsymbol{\beta}'}}_{=\mathbf{0}} - \frac{\partial \sum_{i=1}^n \Lambda(\mathbf{x}_i' \boldsymbol{\beta}) \mathbf{x}_i}{\partial \boldsymbol{\beta}'} \\ &= -\sum_{i=1}^n \Lambda_i (1 - \Lambda_i) \mathbf{x}_i \mathbf{x}_i' \end{align*}$$
在上式中，由于$0<\Lambda_i <1$，故黑塞矩阵为负定。则Logit 的对数似然函数为“凹函数”(concave function)，一定存在唯一的最大值。

则可对一阶导数求最值，同极点问题处理，可求最大值。

### Logit模型的解释（模型的进一步理解）

对于非线性模型，其估计量$\hat \beta$一般并非边际效应。它衡量的是 **自变量对 “对数优势比”（Log Odds）的影响**，而非对概率的直接影响

对于Logit模型，可使用微积分的链式法则，计算第K个特征变量$X_k$的边际效应。

$$\begin{align*}
\frac{\partial \mathrm{P}(y=1 \mid \boldsymbol{x})}{\partial x_k}
&= \frac{\partial \Lambda(\boldsymbol{x}' \boldsymbol{\beta})}{\partial x_k} \\
&= \frac{\partial \Lambda(\boldsymbol{x}' \boldsymbol{\beta})}{\partial (\boldsymbol{x}' \boldsymbol{\beta})} \cdot \frac{\partial (\boldsymbol{x}' \boldsymbol{\beta})}{\partial x_k} \\
&= \lambda(\boldsymbol{x}' \boldsymbol{\beta}) \cdot \beta_k
\end{align*}$$
其中，$\lambda(Z) = \frac{e^z}{(1+e^Z)^2}$为逻辑分布的密度函数
则由上可知，非线性模型的边际效应通常不是常数，随着特征向量X发生变化。

可根据样本数据，计算平均边际效应(Average Marginal Effects，简记AME)，即分别计算在每个样本点xi上的边际效应，然后针对整个样本进行平均。

记变量$x_k$对于个体i 的边际效应 $\widehat{AME}_{ik}$ （$\widehat {~~~~ ~~}$ 表示为样本估计值）
则向量$x_k$的平均边际效应为
$$\widehat{AME_k} = \frac{1}{n}\sum_{i =1 }^n \widehat{AME_{ik}}，(k = 1,\dots.p)$$
其中$\widehat {AME_{ik}} = \lambda(x_i'\hat \beta)\hat \beta_k$
$\hat \beta$为向量，$\hat \beta_k$为标量，即$x_k$前的系数。

既然$\hat \beta$并非边际效应，那么它究竟有什么含义？记事件“ $y = 1$”发生的条件概率为
$p = P(y =1|x)$，则该事件不发生概率为$1-p = P(y = 0|x)$

对于logit模型，由于$p = \frac{e^{x'\beta}}{1+e^{x'\beta}}$，而$1-p = \frac{1}{1+e^{x'\beta}}$
故事件发生与不发生的几率为
$$\text{几率} \equiv \frac{p}{1-p} = \exp(\boldsymbol{x}' \boldsymbol{\beta})$$

其中，$\frac{p}{1-p}$称为**几率或相对风险**

例如，在一个检验药物疗效的随机实验中，“$y = 1$“表示“生”，而“$y = 0$”表示“死”。如果几率为2
则意味着存活的概率是死亡概率的两倍，故存活概率为2/3，而死亡概率为1/3。

对几率取对数可得：

$$\ln\left( \frac{p}{1-p} \right) = \boldsymbol{x}' \boldsymbol{\beta} = \beta_1 x_1 + \cdots + \beta_p x_p
$$
其中 $\ln\left( \frac{p}{1-p} \right)$称为对数几率，而上试右边为线性函数，这正是上文提及的“逻辑变换”

回归系数$\beta_k$表示当变量$x_k$增加一个微小量时，引起对数几率的边际变化。

由$\ln\left( \frac{p}{1-p} \right)$的式子可知，对$x_k$求偏导即可得回归系数$\beta_k$

即（非瞬时变化率，所以是约等于）
$$\beta_k = \frac{\partial \ln\left( \frac{p}{1-p} \right)}{\partial x_k} \approx \frac{\Delta\left( \frac{p}{1-p} \right) / \frac{p}{1-p}}{\Delta x_k}$$
这意味着$\beta_k$解释为半弹性，即当$x_k$增加1单位时，可引起几率$\frac{p}{1-p}$变化的百分比。
$$
\frac{\Delta \text{odds}}{\text{odds}} = \frac{\Delta \left( \frac{p}{1-p} \right)}{\frac{p}{1-p}} \approx \beta_k \cdot \underbrace{\Delta x_k}_{=1} = \beta_k
$$
例如，$\beta_k= 0.12$，意味着$x_k$增加1单位可引起几率增加12%。
以上解释隐含假设$x_k$为连续变量，且可求导数。

如果$x_k$为离散变量（比如，性别、子女数）,则无法微分，可使用如下解释方法。
假设$x_k$增加1单位，从$x_k$变为$x_k+1$，记概率$p$的新值为$p^*$，则可根据新几率$\frac{p^*}{1-p^*}$
与原几率$\frac{p}{1-p}$的几率定义**几率比**
$$\text{几率比} \equiv \frac{\dfrac{p^*}{1-p^*}}{\dfrac{p}{1-p}} = \frac{\exp\left[ \beta_1 x_1 + \cdots + \beta_k (x_k + 1) + \cdots + \beta_p x_p \right]}{\exp\left( \beta_1 x_1 + \cdots + \beta_k x_k + \cdots + \beta_p x_p \right)} = \exp(\beta_k)$$
若$\beta_k = 0.12$，则几率比$exp(\beta_k) = e^{0.12} =1.13$
这意味着，当$x_k$增加1单位时，新几率变为原几率大1.13倍，或增加13%。

如果$\beta_k$较小，则$\exp(\beta_k) - 1 \approx \beta_k$（将$\exp(\beta_k)$泰勒展开）
则以上两种方法**基本等价**

若使用 Probit 模型，由于其连接函数较为复杂，故无法使用几率比对其系数$\hat\beta$进行类似的解释，这是 Probit 模型的劣势。

### 非线性模型的拟合优度

非线性模型不存在平方和分解公式，一般无法使用$R^2$度量拟合优度。

对于使用MLE（最大似然估计）进行评估的非线性模型，可使用准$R^2（PseudoR^2)$或伪$R^2$度量模型的拟合优度。
准$R^2$的定义为：
$$ \text{准}R^2 \equiv \frac{\ln L_0 - \ln L_1}{\ln L_0} $$
**零模型$(\boldsymbol{\ln L_0})$的计算：**
设样本量为n，其中$y = 1$的样本数为$n_1$，$y = 0$的样本数为$n-n_1$
零模型的预测概率：$\hat p = \frac{n_1}{n}$（所有样本的预测概率均为$\hat p$）
似然值：$L_0 = \prod_{i=1}^n \left[ \hat{p}^{y_i} \cdot (1 - \hat{p})^{1-y_i} \right] = \hat{p}^{n_1} \cdot (1 - \hat{p})^{n_0}$
对数似然：$\ln L_0 = n_1 \ln \hat{p} + n_0 \ln(1 - \hat{p}) \ (\text{因}\hat{p} \in (0,1),\ \text{故}\ln L_0 < 0)$

**当前模型($\ln L_1$)的计算：**
以logit模型为例
对每个样本$i$，预测概率为$\Lambda (x_i'\beta) = \frac{1}{1+e^{-x_i'\beta}}$
似然值：$L_1 = \prod_{i=1}^n \left[ \Lambda(\boldsymbol{x}_i' \boldsymbol{\beta})^{y_i} \cdot \left(1 - \Lambda(\boldsymbol{x}_i' \boldsymbol{\beta})\right)^{1-y_i} \right]$
对数似然：$\ln L_1 = \sum_{i=1}^n \left[ y_i \ln \Lambda(\boldsymbol{x}_i' \boldsymbol{\beta}) + (1 - y_i) \ln\left(1 - \Lambda(\boldsymbol{x}_i' \boldsymbol{\beta})\right) \right]$

$\ln L_1$为原模型的对数似然函数的最大值，而$\ln L_0$为以常数项为唯一变量的对数似然函数的最大值。

由于$y$为离散的两点分布，似然函数的最大可能值为1，故对数似然函数的最大可能值为0，记为$\ln L_{max}$

显然，$0 \geq \ln L_1 \geq \ln L_0$
则$$ \text{准}R^2 = \frac{\ln L_1 - \ln L_0}{\ln L_{\text{max}} - \ln L_0} $$
在统计学中，还常使用**偏离度**的概念。偏离度也称为**残差偏离度**。
其定义为
$$\text{(残差) 偏离度} \equiv -2\ln L_1$$
偏离度表达式中的 2，只是为了凑成统计学中“似然比检验”(likelihood ratio test)统计量而设。
偏离度表达式中的负号，则使得最大化$2\ln L_1$的问题，变为最小化$-2\ln L_1$

### logit模型的预测

得到 Logit 模型的估计系数（$\beta$）后，即可预测“$y_i = 1$”的条件概率

$$\hat{p}_i \equiv \widehat{\mathrm{P}(y_i = 1 \mid \boldsymbol{x}_i)} = \Lambda(\boldsymbol{x}_i' \hat{\boldsymbol{\beta}}) \equiv \frac{\exp(\boldsymbol{x}_i' \hat{\boldsymbol{\beta}})}{1 + \exp(\boldsymbol{x}_i' \hat{\boldsymbol{\beta}})}$$
如果预测概率$\hat p_i>0.5$，则可预测$\hat y_i =1$，反之，如果$\hat p_i<0.5$，则可预测$\hat y_i = 0$

如果$\hat p_i = 1-\hat p_i = 0.5$则可预测$y = 0$或$1$

对于二分类问题，在特征空间(feature space)中，所有满足“$\hat p_i =0.5$”的样本点之集合称为**决策边界**
$$\text{决策边界} \equiv \left\{ \boldsymbol{x}_i : \hat{p}_i(\boldsymbol{x}_i) = 0.5 \right\}$$
在决策边界，可以无差别地预测$\hat y_i =0$或$1$
对于logit模型，也可以使用对数几率来预测其响应变量的类别：
$$\ln (\frac{\hat p_i}{1-\hat p_i}) = x_i'\hat \beta$$
如果对数几率大于0（$\hat p_i>1-\hat p_i$），则可预测$\hat y_i = 1$
反之则$\hat y_i = 0$，且当对数几率等于0时，类似于决策边界。

线性的决策边界将特征空间分割为两部分。其中，在决策边界的一侧
$\left\{ \boldsymbol{x}_i : \boldsymbol{x}_i' \hat{\boldsymbol{\beta}} > 0 \right\}$，则可预测$\hat y_i =1$
$\left\{ \boldsymbol{x}_i : \boldsymbol{x}_i' \hat{\boldsymbol{\beta}} < 0 \right\}$，则可预测$\hat y_i = 0$

### 二分类模型的评估

对于监督学习问题，一般用预测效果来评估其模型的性能。

具体到分类问题，一个常用指标为**准确率**，也称为“正确预测的百分比”(percent correctly predicted)只要将降本数据的预测值$\hat y_i$与实际值$y_i$进行比较，即可计算正确预测的百分比。
$$ \text{准确率} \equiv \frac{\sum_{i=1}^n I(\hat{y}_i = y_i)}{n} $$
其中$I(~)$为示性函数

有时我们更专注于错误预测的百分比，即**错误率**或**错分率**
$$\text{错分率} \equiv \frac{\sum_{i=1}^n I(\hat{y}_i \neq y_i)}{n}$$
如果所考虑样本为训练集，则为“训练误差”(training error)。如果所考虑样本为测试集，则为“测试误差”(test error)。准确率与错误率之和为 1。

准确率或错分率并不适用于“类别不平衡”(class imbalance)的数据。

将样本数据分为以下四类，并用一个矩阵来表示，即所谓混淆矩阵(confusion matrix)。
![fix](//images/images/Pasted%20/images/image%2020250708141900.png)
混淆矩阵的左上角为真阳性或“真正例”(True Positive)，简记 TP，即预测正例（$\hat y = 1$），而实际也是正例（$y = 1$）的情形

右上角为假阳性或“假正例”(False Positive)，简记 FP，类似于假警报(false alarm)，即预测正例（$\hat y =1$），实际反例的情形（$y =0$）

混淆矩阵的左下角为假阴性或“假反例”(False Negative)，简记 FN，即预测反例（$\hat y =0$），实际正例的情形（$y =1$）

右下角为真阴性或“真反例”(True Negative)，简记 TN，即预测反例（$\hat y =0$），实际反例的情形（$y =0$）

定义灵敏度，查准率，反映正确预测正例在实际为正例的字样本中的比例。
$$
\text{灵敏度} = \text{真阳率} \equiv \frac{\text{TP}}{\text{TP} + \text{FN}}
$$

定义特异度，反映正确预测反例在实际为反例的子样本中的占比。
$$
\text{特异度}=\text{真阴率} \equiv \frac{\text{TN}}{\text{FP}+\text{TN}}
$$

定义假阳率，反映错误预测反例的子样本在实际为反例的子样本中的占比

$$ \text{假阳率}=1-\text{特异度} \equiv \frac{\text{FP}}{\text{FP}+\text{TN}} $$

定义查全率或召回率，在预测正确的比例在预测为正例的子样本中的占比
$$
\text{查全率} =\text{召回率}\equiv \frac{\text{TP}}{\text{TP} + \text{FP}}
$$

### ROC与AUC

从决策理论的角度，默认用于分类的“门槛值为”这$\hat p =0.5$，未必是最佳选择。

从混淆矩阵可知，在作预测时，可能犯两类不同的错误，即“假阳性”与“假阴性”。在具体的业务中，这两类错误的成本可能差别很大。比如：“假阳性”将健康者误判为病人，其成本可能只是多做些医疗检查；而“假阴性”将病人视为健康者，则会耽误病情，后果更为严重。又比如：“假阳性”将正常客户视为劣质客户而拒绝贷款，其成本只是少赚些利润；而“假阴性”将劣质客户视为正常客户而放贷，则会面临因断供而损失本金的巨大成本。

作预测的两类错误，其成本可能并不对称。此时，应根据具体的业务需要，考虑使用合适的门槛值$\hat p = c$进行分类。

比如，为了降低错误放贷的损失，银行可将分类为劣质客户的门槛值降低到$\hat p =0.2$，这意味着，如有20%或以上的概率客户会断供，则判断为断供并拒绝贷款。

**使用更低的门槛值，将预测更多的正例，而预测更少的反例**
改变门槛值直接影响灵敏度和假阳率：由于原本样本的实际正例与实际反例的数量不变，而改变门槛值之后会导致预测的正例与反例的值发生变化。（$p>\hat p\text{时，}\hat y =1$）
此时，在实际为正例的子样本中，预测准确率将上升，即灵敏度上升。而在实际为反例的子样本中，预测准确率将下降，即特异度下降，故“$1-\text{特异度}$”上升。

![fix](//images/images/Pasted%20/images/image%2020250708163044.png)
ROC曲线示意图：以假阳率为x轴，以灵敏度为y轴，让门槛值$\hat p = c$的取值从0连续地变为1，则可得到一条曲线，即所谓接收器工作特征曲线，简记ROC曲线。

当门槛值为0，则所有样例都被预测为正例，此时混淆矩阵为
![fix](//images/images/Pasted%20/images/image%2020250708163306.png)
此时$\text{灵敏度}=\frac{TP}{TP+FN} = 1$
$\text{假阳率} = 1-特异度=\frac{FP}{FP+TN} = 1$

当门槛值为1，则所有样例都被预测为反例，此时的混淆矩阵为
![fix](//images/images/Pasted%20/images/image%2020250708163513.png)
则灵敏度= 假阳率= 0

由于纵轴为实际正例中的准确率(灵敏度)，而横轴为实际反例中的错误率(1-特异度)，故我们希望模型的 ROC 曲线越靠近左上角越好。

因此，为衡量 ROC 曲线的优良程度，可使用 ROC曲线下面积（简记AUC）
![fix](//images/images/Pasted%20/images/image%2020250708163613.png)

AUC 小于 0.5 的情形十分罕见，这意味着模型的预测结果还不如随机猜测。
对于二分类问题，在比较不同模型的预测效果时，常使用 AUC。由于AUC 为衡量预测效果的综合性指标，可使用此单一指标比较不同的算法

### 案例

泰坦尼克号的生存人员预测概率

```python
import numpy as np

import pandas as pd

titanic = pd.read_csv('titanic.csv')

titanic.shape
```

输出结果

```python
(32, 5)
```

输出前五项

```python
titanic.head()
```

```python|Class|Sex|Age|Survived|Freq|
|---|---|---|---|---|
|0|1st|Male|Child|No|0|
|1|2nd|Male|Child|No|0|
|2|3rd|Male|Child|No|35|
|3|Crew|Male|Child|No|0|
|4|1st|Female|Child|No|0|
```

按照频次复制数据

```python
freq = titanic.Freq.to_numpy()
#transform the frequency column to a numpy array
index = np.repeat(np.arange(32), freq)
titanic = titanic.iloc[index,:]
# 按照索引顺序和次数复制行
titanic = titanic.drop(columns=['Freq'])
# drop the frequency column
titanic.head()
```

先将Freq单独提取并转换成numpy数组
再进行重复 注意arange(32)是从0开始
最后去掉频数这一列。
结果如下

```python
|Class|Sex|Age|Survived|
|---|---|---|---|
|2|3rd|Male|Child|No|
|2|3rd|Male|Child|No|
|2|3rd|Male|Child|No|
|2|3rd|Male|Child|No|
|2|3rd|Male|Child|No|
```

交叉统计表

```python
pd.crosstab(titanic.Sex,titanic.Survived, margins=True)

# 交叉表统计性别和生存情况并添加合计 margins

# margins=True 添加合计行和列
```

```python
|Survived|No|Yes|All|
|---|---|---|---|
|Sex||||
|---|---|---|---|
|Female|126|344|470|
|Male|1364|367|1731|
|All|1490|711|2201|

```

查看比例

```python
pd.crosstab(titanic.Sex,titanic.Survived, normalize='index')

# normalize='index' 计算每行的比例
|Survived|No|Yes|
|---|---|---|
|Sex|||
|---|---|---|
|Female|0.268085|0.731915|
|Male|0.787984|0.212016|
```

逻辑对数似然估计

```python
#logit estimation 逻辑对数似然估计

from sklearn.model_selection import train_test_split

import statsmodels.api as sm #提供统计模型和计量经济学工具

from patsy import dmatrices #用于创建设计矩阵

train, test = train_test_split(titanic, test_size=0.3, stratify=titanic.Survived,random_state=0)

# 对数据集进行分割，70%用于训练，30%用于测试

# stratify=titanic.Survived 对数据进行分层抽样，确保训练和测试集中的生存情况比例相同
```

stratify=titanic.Survived ：按照Survived的比例进行分层抽样 确保训练和测试集中的生存情况比例相同

```python
y_train, X_train = dmatrices('Survived~Class + Sex + Age', data=train, return_type='dataframe')

# 使用dmatrices函数创建设计矩阵，包含生存情况和特征变量

# X中添加默认的截距项 用于线性模型中的常数项

# 返回类型='dataframe' 返回pandas DataFrame格式

pd.options.display.max_columns = 15 #最多显示15个数据

```

设计自变量与因变量矩阵，返回表格形式。

```python
y_train = y_train.iloc[:,1]

# 只保留生存情况列 当生存判断为1时表示生存，为0时表示未生存
```

对测试集做同样处理

```python
y_test, X_test = dmatrices('Survived ~ Class + Sex + Age', data=test, return_type='dataframe')

y_test = y_test.iloc[:,1]

# 对测试集进行同样的处理
```

创建逻辑回归模型

```python
model = sm.Logit(y_train, X_train)

# 创建逻辑回归模型

results = model.fit()

results.params

# 通过最大似然估计求

# 拟合模型并输出模型参数
```

通过最大似然估计求模型参数
也可理解为 对于连续变量（如 `Age`），系数表示该特征每增加 1 单位时，**对数优势（log-odds）** 的变化量。
其中，**截距项（Intercept）**：当所有特征为 0 时的对数优势。

由上文：$$\beta_k = \frac{\partial \ln\left( \frac{p}{1-p} \right)}{\partial x_k} \approx \frac{\Delta\left( \frac{p}{1-p} \right) / \frac{p}{1-p}}{\Delta x_k}$$
最大似然估计求出的模型参数也是对数优势比
又有：
$$\text{几率比} \equiv \frac{\dfrac{p^*}{1-p^*}}{\dfrac{p}{1-p}} = \frac{\exp\left[ \beta_1 x_1 + \cdots + \beta_k (x_k + 1) + \cdots + \beta_p x_p \right]}{\exp\left( \beta_1 x_1 + \cdots + \beta_k x_k + \cdots + \beta_p x_p \right)} = \exp(\beta_k)$$
则需要对求出的参数做指数化处理，得出几率比

```python
np.exp(results.params)
results.summary()
# 输出模型的详细统计信息，包括系数、标准误差、z值、p值等

# 通过指数化系数可以得到每个特征对生存概率的影响

# 例如，Class为2时生存概率是Class为1时的exp(0.63)倍，Sex为女性时生存概率是男性的exp(0.96)倍，Age每增加1岁生存概率是exp(-0.04)倍

# 通过p值可以判断每个特征是否显著影响生存
```

结果如下

```python
|   |   |   |   |
|---|---|---|---|
Logit Regression Results
|Dep. Variable:|Survived[Yes]|No. Observations:|1540|
|Model:|Logit|Df Residuals:|1534|
|Method:|MLE|Df Model:|5|
|Date:|Wed, 09 Jul 2025|Pseudo R-squ.:|0.2019|
|Time:|10:36:45|Log-Likelihood:|-772.98|
|converged:|True|LL-Null:|-968.52|
|Covariance Type:|nonrobust|LLR p-value:|2.482e-82|

|   |   |   |   |   |   |   |
|---|---|---|---|---|---|---|
||coef|std err|z|P>\|z\||[0.025|0.975]|
|Intercept|2.0235|0.200|10.141|0.000|1.632|2.415|
|Class[T.2nd]|-1.0459|0.232|-4.509|0.000|-1.501|-0.591|
|Class[T.3rd]|-1.8691|0.206|-9.059|0.000|-2.273|-1.465|
|Class[T.Crew]|-0.8826|0.192|-4.598|0.000|-1.259|-0.506|
|Sex[T.Male]|-2.3558|0.167|-14.079|0.000|-2.684|-2.028|
|Age[T.Child]|1.1269|0.287|3.927|0.000|0.565|1.689|
```

计算边际效应：方法如下（平均边际效应AM E）
考虑单一变量对整体预测的影响，负表示下降，正表示上升。

———————————————回顾上文———————————————————————
对于Logit模型，可使用微积分的链式法则，计算第K个特征变量$X_k$的边际效应。

$$\begin{align*}
\frac{\partial \mathrm{P}(y=1 \mid \boldsymbol{x})}{\partial x_k}
&= \frac{\partial \Lambda(\boldsymbol{x}' \boldsymbol{\beta})}{\partial x_k} \\
&= \frac{\partial \Lambda(\boldsymbol{x}' \boldsymbol{\beta})}{\partial (\boldsymbol{x}' \boldsymbol{\beta})} \cdot \frac{\partial (\boldsymbol{x}' \boldsymbol{\beta})}{\partial x_k} \\
&= \lambda(\boldsymbol{x}' \boldsymbol{\beta}) \cdot \beta_k
\end{align*}$$
其中，$\lambda(Z) = \frac{e^z}{(1+e^Z)^2}$为逻辑分布的密度函数
则由上可知，非线性模型的边际效应通常不是常数，随着特征向量X发生变化。

可根据样本数据，计算平均边际效应(Average Marginal Effects，简记AME)，即分别计算在每个样本点xi上的边际效应，然后针对整个样本进行平均。

记变量$x_k$对于个体i 的边际效应 $\widehat{AME}_{ik}$ （$\widehat {~~~~ ~~}$ 表示为样本估计值）
则向量$x_k$的平均边际效应为
$$\widehat{AME_k} = \frac{1}{n}\sum_{i =1 }^n \widehat{AME_{ik}}，(k = 1,\dots.p)$$
其中$\widehat {AME_{ik}} = \lambda(x_i'\hat \beta)\hat \beta_k$
$\hat \beta$为向量，$\hat \beta_k$为标量，即$x_k$前的系数。
—————————————————————————————————————————
```python
margeff = results.get_margeff()
margeff.summary()
```
结果如下
```python
|   |   |
|---|---|
Logit Marginal Effects
|Dep. Variable:|Survived[Yes]|
|Method:|dydx|
|At:|overall|

||dy/dx|std err|z|P>\|z\||[0.025|0.975]|
|---|---|---|---|---|---|---|
|Class[T.2nd]|-0.1708|0.037|-4.595|0.000|-0.244|-0.098|
|Class[T.3rd]|-0.3053|0.031|-9.788|0.000|-0.366|-0.244|
|Class[T.Crew]|-0.1442|0.031|-4.711|0.000|-0.204|-0.084|
|Sex[T.Male]|-0.3848|0.021|-18.409|0.000|-0.426|-0.344|
|Age[T.Child]|0.1841|0.046|3.980|0.000|0.093|0.275|
```

模型的评估：混淆矩阵

理论：默认的阈值为0.5
得到 Logit 模型的估计系数（$\beta$）后，即可预测“$y_i = 1$”的条件概率

$$\hat{p}_i \equiv \widehat{\mathrm{P}(y_i = 1 \mid \boldsymbol{x}_i)} = \Lambda(\boldsymbol{x}_i' \hat{\boldsymbol{\beta}}) \equiv \frac{\exp(\boldsymbol{x}_i' \hat{\boldsymbol{\beta}})}{1 + \exp(\boldsymbol{x}_i' \hat{\boldsymbol{\beta}})}$$
如果预测概率$\hat p_i>0.5$，则可预测$\hat y_i =1$，反之，如果$\hat p_i<0.5$，则可预测$\hat y_i = 0$
将预测的与实际的进行比较，即可得到混淆矩阵

```python
table = results.pred_table()
teble
```
```python
array([[949., 94.], [252., 245.]])
```
![fix](//images/images/Pasted%20/images/image%2020250709151747.png)
示例输出结果

**准确率**，也称为“正确预测的百分比”(percent correctly predicted)只要将降本数据的预测值$\hat y_i$与实际值$y_i$进行比较，即可计算正确预测的百分比。
$$ \text{准确率} \equiv \frac{\sum_{i=1}^n I(\hat{y}_i = y_i)}{n} $$
```python
Accuracy = (table[0,0] + table[1,1]) / table.sum()

Accuracy
0.7753246753246753
```
**错分率**
```python
Error_rate = 1-Accuracy

Error_rate
0.2246753246753247
```

定义特异度，反映正确预测反例在实际为反例的子样本中的占比。
$$
\text{特异度}=\text{真阴率} \equiv \frac{\text{TN}}{\text{FP}+\text{TN}}
$$
```python
Specificity = table[0,0] / (table[0,0] + table[0,1])

# 输出特异性

Sensitivity = table[1,1] / (table[1,0] + table[1,1])

# 输出灵敏率

print(f'Specificity: {Specificity}, Sensitivity: {Sensitivity}')
# 输出
```

预测值（将参数$\beta$带入，计算p）
并按照阈值0.5将预测概率转换为0或1；0代表未生存，1代表生存
```python
prob = results.predict(X_test)# 输出测试集的预测概率
pred = (prob >=0.5).astype(int)
pred.head()
```
使用 `pandas` 的 `crosstab` 函数生成一个 **混淆矩阵**，用于评估分类模型的预测性能。并转换为数组。
```python
table = pd.crosstab(y_test,pred,colnames = ['predicted'])
table = np.array(table)
table
```
结果如下
```python
array([[415, 32], [110, 104]])
```