---
title: 决策树
date: 2025-07-25
updated: 2025-07-25
description:
---

## 决策树

决策树本质上也是一种近邻方法，可视为“自适应近邻法”

决策树在进行节点分裂时考虑了 $y$ 的信息，故更有智慧，不受噪音变量的影响，且适用于高维数据。

如果将决策树用于分类问题，则称为“分类树”。
如果将决策树用于回归问题，则称为“回归树”。

### 分类树的启发案例

Breiman et al. (1984)研究了 UCSD 医学中心的一个案例。当心梗病人进入 UCSD 医学中心后 24 小时内，测量 19 个变量，包括血
压、年龄以及 17 个排序或虚拟变量。数据集中包含 215 个病人，其中 37
人为高危病人。

研究目的是为了快速预测哪些心梗病人为“高危病人”(High risk，记为H，无法活过30天)，而哪些是“低危病人” (Low risk，记为L，可活过30天)，从而更好地配置医疗资源。

Breiman et al. (1984)建立了如下分类树

![fix](/images/imageDT.png)

在图中，首先从顶部的根节点(root node)出发，考察病人“收缩压是否低于或等于 91”。（根节点指第一个节点

如果答案为“是”，则向左，到达终节点(terminal node)或叶节点(leaf node)，
归类为 H(高危)；反之，则向右，到下一个内节点(internal node)。

此时，需回答的问题为“年龄是否小于或等于 62.5 岁”。如何答案为“是”，则向左，到达终节点，归类为 L(低危)；反之，则继续向右，到再下一个内节点。

此时，需回答的问题为“是否窦性心动过速”。如何答案为“是”，则向左，到达终节点，归类为 H(高危)；反之，则继续向右，到达终节点，归
类为 L(低危)。

在图中，终节点为方形，为决策树的最底端，不再分裂。

在分裂之前，**所有样本点都在最顶端的根节点，而根节点与终节点之间的节点的节点均称为“内节点”。**

建立分类树模型之后，要进行预测十分简单。只要将观测值从决策树放下(drop an observation down the tree)，回答一系列的是或否问题(是则向左，否即向右)，看它落入哪片叶节点。

然后使用“多票数规则”进行预测，即看落入该叶节点的训练数据最多为哪类。

由于决策树不对函数形式作任何假设，故比较稳健，其预测效果可能优于参数方法(比如判别分析、逻辑回归)。

在以上案例中，虽然数据集共有 19 个特征变量，但所估计的分类树只用到 3 个变量。

通过以上模型可清晰地知道高危与低危病人的类型：比如，模型所识别的高危病人可分为两种类型，即收缩压低于或等于 91 者(血压过低)；或收缩压虽高于 91，但年龄大于 62.5 岁，且窦性心动过速者。

### 二叉树的数学本质

上述案例所用的决策树为二叉树：每次总将“母节点”一分为二，分裂为两个“子节点”，直到达终节点。

二叉树将“特征空间”进行递归分割，每次总是沿着某个特征向量 $X_j$ 轴平行的方向进行切割，切成“矩形”、“超矩形”区域，即所谓“箱体”。

二叉树是通过分割特征空间进行分类的分类器：假设只有两个特征向量 $(x_1,x_2)$，则递归分裂的一种可能结果如下

![fix](/images/imageDT-1.png)

首先根据是否 $x_1 \leq t_1$ 进行分裂。

然后根据是否 $x_2 \leq t_2$ 进行分割，得到终节点 $R_1$ 和 $R_2$。

接着，根据是否 $x_1 \leq t_3$ 进行分裂，得到终节点 $R_3$。

最后，根据是否 $x_2 \leq t_4$ 进行分割，得到终节点 $R_4 \leq R_5$。

![fix](/images/imageDT-2.png)

对于三维以上的特征空间，无法使用类似于上图的方法来展示递推分割；但依然可用图中的树状结构来表示，因为决策树每次仅使用一个变量进行分割。

决策树模型将特征空间分割为若干个矩形终节点。

在进行预测时，每个终节点只有一个共同的预测值。

对于分类问题，次预测值为该终节点所有训练样本的最常见类别。

对于回归问题，测预测为该终节点所有训练样本的平均值。

在数学上，决策树为“分段场值函数”，这意味着估计的函数 $\hat f(X)$ 不是连续函数。

但这并不妨碍决策树成为一种灵活而有用的算法，特别是作为“基学习器”(base learner)广泛用于随机森林与提升法。

![fix](/images/imageDT-3.png)

因为这些超矩形的维度太高，同时估计在计算上不可行。决策树采取了一种“自上而下”(top-down)，每次仅分裂一个变量的方法。这是一种“贪心算法”(greedy algorithm)，每次仅选择一个最优的分裂变量，而未通盘考虑全局的最优分区。

### 分类树的分裂准则

确定根节点的选择，怎么选择，如何划分？

对于 CART 算法的二叉树，在每个节点进行分裂时，需要确定“分裂变量”进行分裂，以及在该变量的什么临界值进行分裂。

对于分类树，我们希望在节点分裂之后，其两个字节点内部的纯度最高。

关于决策树的纯度，在决策树算法中，纯度（Purity） 是衡量数据集中样本 "一致性" 的指标。直观地说，纯度越高，数据集中的样本越可能属于同一类别；纯度越低，则样本越混杂。决策树通过递归划分数据来提高纯度，从而构建分类或回归模型。

分裂之后数据的“不纯度”应下降最低。

假设响应变量 $y$ 共分 $K$ 类，取值为 $y \in {1,\dots , K}$ ，在节点 $t$，记不同 $y$ 的取值相应概率为 $p_1$ ,$\dots$ , $p_k$，其中 $p_k \geq 0$，且 $\sum _{k = 1}^K p_k = 1$。

作为分裂准则(splitting criterion)，希望定义一个节点不纯度函数(node impurity function）$\varphi(p_1,\cdots,p_K) \geq 0$。

该函数应具备以下性质：

(1) 当$p_1 = \cdots = p_K = \dfrac{1}{K}$时，不纯度最高，即$\varphi(p_1,\cdots,p_K)$达到最大值。

(2) 当且仅当$(p_1,p_2 \cdots,p_K) = (1,0,\cdots,0)$，$(0,1,\cdots,0)$，$\cdots$，或$(0,0,\cdots,1)$时，不纯度为 0，即$\varphi(p_1,\cdots,p_K)$达到最小值 0。

(3) $\varphi(p_1,\cdots,p_K)$关于自变量$(p_1,\cdots,p_K)$是对称的。

满足这些性质的函数并不唯一。一个自然的选择是使用错分率(misclassification rate)作为不纯度函数：

$$
\text{Err}(p_1,\cdots,p_K) \equiv 1 - \max\left\{p_1,\cdots,p_K\right\}
$$

其中， $\max \{ p_1,p_2,\dots ,p_K\}$ 为最多类别的发生频率，而 $1 - \max \{ p_1,p_2,\dots ,p_K\}$ 则为以最多类别预测时的错分率。

对于二分类问题，错分率简化为

$$
\text{Err}(p_1, p_2) \equiv 1 - \max\left\{ p_1, 1 - p_1 \right\} =
\begin{cases}
p_1 & \text{if } 0 \leq p_1 < 0.5 \\
1 - p_1 & \text{if } 1 \geq p_1 \geq 0.5
\end{cases}
$$

将错分率视为 $p_1$ 的函数，则错分率在 $p_1 = 0.5$ 处达到最大值0.5，然后以 $p_1 = 0.5$ 为中心，向两边线性递减，呈三角形状。

![fix](/images/imageDT-4.png)

如图，错分率为“分段线性函数”，对于“不纯度”的度量并不敏感，故实际效果不太好。

实际中常用的两个不纯度函数分别为**基尼指数**与**信息差**

**基尼指数度量的是，从概率分布 $(p_1,\dots , p_K)$ 中随机抽取两个观测值，则这两个观测值的类别不一致的概率为：**

$$
\text{Gini}(p_1, \cdots, p_K) \equiv \sum_{k=1}^{K} p_k (1 - p_k) = \underbrace{\sum_{k=1}^{K} p_k}_{=1} - \sum_{k=1}^{K} p_k^2 = 1 - \sum_{k=1}^{K} p_k^2
$$

其中，$\sum_{k =1 } ^K P_K^2$ 可视为随机抽取的两个观测值的**类别一致**的概率。

对于二分类问题，基尼指数可写为

$$
\text{Gini}(p_1, p_2) = 1 - p_1^2 - (1 - p_1)^2 = 2p_1(1 - p_1)
$$

其中，$p_1(1-p_1)$ 可视为两点分布的方差。

在多分类的情况下，基尼指数也可结束为单独取出其中的某类，而将其他类别归并为一类，计算此两点分布的方差，然后重复此过程，将所有的方差加总。

基尼指数为概率分布的二次函数

对于二分类问题，基尼指数为抛物线，在 $p_1 = 0.5$ 处达到最大值0.5，然后以二次曲线向两边下降。

![fix](/images/imageDT-5.png)

### 信息理论

另一场用的节点不纯度函数为信息熵。信息熵起源于信息理论。

例如：有人告诉你：“明天太阳会升起”，则此信息毫无价值，因为它是必然事件。如果有人告诉你：“明天能看到太阳”，这个信息的意义也不打，因为大多数日子都能看见太阳，你自己也很可能猜对。
而假如有人告诉你，“明天会下雨”，这一消息的信息量就比较大。因为下雨的概率一般较小。而如果有人告诉你，“明天会地震”，这条信息的信息量就很大，因为地震是稀有事件，发生概率低。

熵表示随机变量不确定性的度量，即混乱程度。这里表示进行训练之后，归类到的类别中的混乱程度。由此可得，熵表示数据训练之后归类的类别中数据的混乱程度

一个随机事件的发生，其**信息量**似乎与其发生概率成反比。直观上，信息度量的是”吃惊程度“。

记随机事件“$y = k$”的发生概率为 $p_k$，**初步猜想该事件发生的信息量为** $\frac{1}{p_k}$。希望当 $p_k = 1$（必然事件）时，此事件的信息量为0。一个自然的选择，即 $\log_2(\frac{1}{p_k})= -\log_2 p_k$。**（要使概率越高，数据越纯，熵越低则单调低价。并且要求当概率为1时的熵值为0，对数函数是很好的选择）**

**将 $y$ 的每个可能取值的信息量，以相应概率 $p_k$ 为权重，加权求和即可求得期望信息量，即“信息熵”。**

$$
\text{Entropy}(p_1, \cdots, p_K) \equiv \mathrm{E}\left(-\log_2 p_k\right) = -\sum_{k=1}^{K} p_k \log_2 p_k \quad (p_k \geq 0)
$$

其中，$\log_2(\cdot)$ 为以 2 为底的对数，其单位称为“比特”(bit，表示 binary digits)。  

如果 $p_k = 0$（发生概率为 0），则定义 $0 \cdot \log_2(0) \equiv 0$（因为根据洛必达法则，$\lim\limits_{p \to 0} p \cdot \log_2 p = 0$)。

**确定节点时，先求出原始响应变量的熵，再求各个特征作为节点的信息熵，其差值表示熵降低的多少，称为信息增益，通常用信息增益来判断是否作为节点。**

在定义式中，也可使用以 $e$ 为底的自然对数 $\ln(\cdot)$，其单位称为“奈特” (nat，表示 natural units)。无论使用什么底数，二者并无实质区别。  

对于二分类问题，信息熵可写为  

$$
\text{Entropy}(p_1, p_2) = -p_1 \log_2 p_1 - (1 - p_1) \log_2 (1 - p_1)
$$  

二分类问题的信息熵函数，其几何图形参考如下。

![fix](/images/imageDT-6.png)

信息熵与基尼指数在函数形状上很接近，但信息熵的最大值为1，而非0.5。

![fix](/images/imageDT-7.png)

将信息熵函数除以二，将处理后的信息熵、基尼指数与错分率这三个节点不纯度函数画在一起如上所示。

在实践中，使用基尼指数或信息熵的预测效果一般很接近。

选定分裂准则(比如基尼指数)之后，在进行节点分裂时，针对每个特征变量，首先寻找其最优临界值(cut)，比如 $x_j \leq t$，并计算以该变量为分裂变量可带来的节点不纯度函数的下降幅度。选择下降程度最大的作为节点。同上文的信息增益。

**然后选择可使节点不纯度下降最多的变量作为分裂变量。**

信息增益有缺陷：当对于特征变量很多时，效果不好：

![fix](/images/imageDT-9.png)
这时引入**信息增益率**：

信息增益存在偏向取值多的特征的问题（如 “身份证号” 这类特征，每个取值对应极少样本，划分后熵接近 0，导致增益很大，但无实际意义）。**信息增益率通过归一化处理修正这一偏向：用信息增益除以特征自身的熵（分裂信息）。**

**例**：若特征A的信息增益 $G(D, A)=0.56$，其分裂信息 $H_A(D)=1.2$，则增益率为 $0.56 / 1.2 \approx 0.47$。若特征A取值极多（如 100 个），$H_A(D)$ 会很大（接近 $\log_2 100 \approx 6.6$），即使增益仍为 0.56，增益率也会降至 $0.56 / 6.6 \approx 0.085$，避免被优先选择。

此时又遇到问题：对于连续值该选择哪个值作为分界点？**贪婪算法**

例：对于体重在60、70、75、90、95、100、120、125、130中的人，哪些适合打篮球哪些不适合打篮球？

先按顺序进行排序，进行二分，则可能有9个分界点，有9种熵值比较，求出熵值最小的点，将这个点当作当前连续值的分界点。**即连续值离散化**

### 成本复杂性修枝

在估计决策树模型时，面临一个选择，即何时停止分裂。

如果不停地进行分裂，将使得每个叶节点最终只有一个观测值（或多个相同观测值），此时训练误差为0，导致过拟合，泛化预测能力下降。

需要选择一个合适的决策树模型，停止节点分裂。

如果不让决策树长到最大，则很难知道应在何时停止分裂。

因为在某个节点进行分裂时，即使节点不纯度函数下降很多，但依然可能在以后的节点分裂中，不纯度函数大幅下降。

预剪枝：限制深度，叶子节点个数，叶子节点样本数，信息增益量等。

后剪枝：先建立完决策树后再进行剪枝。

先让决策树尽情生长，即最大的树为 $T_{max}$，再进行“修枝”，以得到一个“子树” $T$。

对于任意子树 $T \subseteq T_{max}$，定义其“复杂性”为子树 $T$ 的终节点数目，记为 $\vert T \vert$。

为避免过拟合，不希望决策树过于复杂，故惩罚其规模 $\vert T \vert$

$$
\min_{T} \underbrace{R(T)}_{\text{cost}} + \lambda \cdot \underbrace{|T|}_{\text{complexity}}
$$

其中，$R(T)$ 为原来的损失函数，比如 $0-1$ 损失函数，即在训练样本中，如果预测正确，即损失为0，若预测错误则损失为1.

$\lambda \geq 0$ 为调节参数，也称为“复杂性参数”。

$\lambda$ 控制对决策树规模 $\vert T \vert$ 的惩罚力度，可通过交叉验证确定，这种修枝方法称为**成本复杂性修枝**，即在成本与复杂性之间进行最优的权衡。

将 $0-1$ 损失函数代入表达式可得：

$$
\min_{T} \underbrace{\sum_{m=1}^{|T|} \sum_{\mathbf{x}_i \in R_m} I(y_i \neq \hat{y}_{R_m})}_{\text{cost}} + \lambda \cdot \underbrace{|T|}_{\text{complexity}}
$$

其中，$R_m$ 为第 $m$ 个终节点，$\hat y_{R_m}$ 为该终节点的预测值（该终节点观测值的众树），而 $I(y_i \neq \hat{y}_{R_m})$ 为示性函数。

$\sum _{X_i \in R_m} I(y_i \neq \hat{y}_{R_m})$ 表示在第 $m$ 个终节点的损失，然后对所有终节点 $m = 1, \dots ,\vert T \vert$ 进行加总。

如果 $\lambda = 0$，则没有复杂性惩罚项，一定选择最大的树 $T_{max}$，导致过拟合。

当 $\lambda$ 增大时，则会得到一个相互嵌套的子树序列，然后从中选择最优的子树。

### 回归树

将决策树应用于回归问题，则为回归树。

对于回归树，可使用“最小化残差平方和”作为节点的分裂准则。

在进行节点分裂时，希望分裂后，残差平方和下降最多，即两个字节点的残差平方和之总和最小。

为避免过拟合，对于回归树，也要使用惩罚项进行修枝，即最小化如下目标函数。

$$
\min_{T} \underbrace{\sum_{m=1}^{|T|} \sum_{\mathbf{x}_i \in R_m} (y_i - \hat{y}_{R_m})^2}_{\text{cost}} + \lambda \cdot \underbrace{|T|}_{\text{complexity}}
$$

其中，$R_m$ 为第 $m$ 个终节点，而 $\hat y_{R_m}$为该终节点的预测值（此终节点的样本均值）。 $\sum_{X_i \in R_m}(y_i-\hat y_{R_m})^2$为第 $m$ 个终节点的残差平方和。

### 决策树的优缺点

决策树与 KNN 的共同点为，二者都采取“分而治之”(divide and conquer)的策略，将特征空间分割为若干区域，利用近邻进行预测。

KNN 在分区域时，不考虑响应变量 $y$ 的信息，故在高维空间容易遇到维度灾难，且易受噪音变量的影响。

决策树在分区域时，考虑特征向量 $X$ 对 $y$ 的影响，且每次仅使用一个分裂变量，这使得决策树很容易应用于高维空间，且不受噪音变量的影响。

如果特征向量 $X$ 包含噪音变量（对 $y$ 无作用的变量），也不会被选为分裂变量，故不影响决策树的模型。决策树的分区预测更具智慧，可视为“自适应近邻法”。

决策树在进行递归分裂时，仅考虑“超矩形”区域。如果真实的决策边界与此相差较远或不规则，则可能导致较大误差。

幸运的是，基于决策树的集成学习(随机森林、提升法)，可得到比较光滑的决策边界，大幅提高预测准确率，

如果真实模型或决策边界为线性，则线性模型的预测效果一般优于决策树。反之，如果决策边界非线性，或接近于矩形，则决策树的预测效果可能更好。

### 回归树案例

![fix](/images/imageDT-8.png)

以下为决策树模型源码，无sklearn包
代码函数环环相扣，且有注解

```python
import math

def createDataSet():
    # 数据
    dataSet = [
        [0, 0, 0, 0, 'no'],
        [0, 0, 0, 1, 'no'],
        [0, 1, 0, 1, 'yes'],
        [0, 1, 1, 0, 'yes'],
        [0, 0, 0, 0, 'no'],
        [1, 0, 0, 0, 'no'],
        [1, 0, 0, 1, 'no'],
        [1, 1, 1, 1, 'yes'],
        [1, 0, 1, 2, 'yes'],
        [1, 0, 1, 2, 'yes'],
        [2, 0, 1, 2, 'yes'],
        [2, 0, 1, 1, 'yes'],
        [2, 1, 0, 1, 'yes'],
        [2, 1, 0, 2, 'yes'],
        [2, 0, 0, 0, 'no'],
    ]
    # 列名
    labels = ['F1-AGE', 'F2-WORK', 'F3-HOME', 'F4-LOAN']
    return dataSet, labels

# 获取出现次数最多的标签
def getMaxLabelByDataSet(curLabelList):
    classCount = {}
    maxKey,maxValue = None,None
    for label in curLabelList:
        if label in classCount.keys(): #classCount中是否有该label的键
            classCount[label] += 1 #如果有则计数加1
            # 如果当前label的计数大于maxValue，则更新maxKey和maxValue
            # maxKey为当前label，maxValue为当前label的计数
            if maxValue < classCount[label]:
                maxKey,maxValue = label , classCount[label]
        else: # 如果没有该label的键，则将其添加到classCount中
            classCount[label] = 1
            if maxKey is None:
                maxKey,maxValue = label ,1
    return maxKey # 返回出现次数最多的标签

# 计算标签的熵值
def calcEntropy(dataSet):
    exampleCount = len(dataSet) # 获取数据集的样本数量
    labelCount = {} # 用于存储每个标签的计数
    for featVec in dataSet:
        curLabel = featVec[-1] # 获取当前遍历样本的标签，即featVec的最后一个元素，y值
        if curLabel in labelCount.keys(): # 如果标签已经存在于labelCount中
            labelCount[curLabel] += 1 # 则计数加1
        else: # 如果标签不存在于labelCount中
            labelCount[curLabel] = 1 # 则将其添加到labelCount中
        # 计算熵的公式：-p*log2(p) 现在计算熵值。
    entropy = 0.0
    for key ,value in labelCount.items():
        p = labelCount[key] / exampleCount # 计算当前标签的概率
        curEntropy = -p * math.log(p, 2) # 计算当前标签的熵值，这里有两个值，对应no 和 yes 的熵。
        entropy += curEntropy # 将当前标签的熵值累加到总熵
    return entropy # 返回总熵值

# 选择最好的特征进行分割，返回最好特征索引
def chooseBestFeatureToSplit(dataSet):
    featureNum = len(dataSet[0]) -1 # 获取特征数量，减去标签
    curEntropy = calcEntropy(dataSet) # 计算当前数据集的熵值，对应上个函数
    bestInfoGain = 0 #初始化信息增益
    bestFeatureIndex = -1 # 初始化最好的特征索引
    for i in range(featureNum):
        featList = [example[i] for example in dataSet] # 获取当前遍历索引特征的所有值
        uniqueFeat = set(featList) # 获取当前特征包含的所有值（唯一值）
        newEntropy = 0
        # 计算分支（不同特征划分）后的熵值
        for value in uniqueFeat:
            subDataSet = splitDataSet(dataSet, i, value) # 根据当前特征和特征值划分数据集
            weight = len(subDataSet) / len(dataSet) # 计算当前子数据集的权重
            newEntropy += weight * calcEntropy(subDataSet) # 计算当前子数据集的熵值，并累加到newEntropy
        infoGain = curEntropy - newEntropy # 计算信息增益
        if infoGain > bestInfoGain: # 如果当前信息增益大于最好信息增益
            bestInfoGain = infoGain # 则更新最好信息增益
            bestFeatureIndex = i # 更新最好特征索引
    return bestFeatureIndex # 返回最好特征索引

# 根据特征和特征值划分数据集并且除去该特征
def splitDataSet(dataSet,featureIndex,value):
    returnDataSet = [] # 用于存储划分后的子数据集
    for featVec in dataSet:
        if featVec[featureIndex] ==value:
            deleteFeatVec = featVec[:featureIndex]
            deleteFeatVec.extend(featVec[featureIndex+1:]) # 删除这里用于划分的特征，组成新的样本。后续不需要该特征参与划分
            # 将删除后的样本追加到新的dataSet中
            returnDataSet.append(deleteFeatVec)
    return returnDataSet

# 递归生成决策树节点
def createTreeNode(dataSet,labels,featLabels):
    # 定义终止条件
    curLabelList = [example[-1] for example in dataSet]
    if len(curLabelList) == curLabelList.count(curLabelList[0]):
        return curLabelList[0]
    if len(labels) == 1:
        return getMaxLabelByDataSet(curLabelList)
    
    # 若不能终止，则进行决策树分类
    bestFeatIndex = chooseBestFeatureToSplit(dataSet)
    bestFeatLabel = labels[bestFeatIndex] # 获取最好特征的标签
    featLabels.append(bestFeatLabel) # 将最好特征标签添加到featLabels中
    myTree = {bestFeatLabel:{}} 
    del labels[bestFeatIndex]
    featValues = [example[bestFeatIndex] for example in dataSet]
    uniqueFeatValues = set(featValues)
    for value in uniqueFeatValues:
        myTree[bestFeatLabel][value] = createTreeNode(splitDataSet(dataSet,bestFeatIndex,value),labels.copy(),
                                                      featLabels.copy())

    return myTree

dataSet,labels = createDataSet()
myDecisionTree = createTreeNode(dataSet,labels,[])
print(myDecisionTree)

```

结果如下

```python
{'F3-HOME': {0: {'F2-WORK': {0: 'no', 1: 'yes'}}, 1: 'yes'}}
```

意思表示：根节点为特征3是否有房，内节点为是否有工作。
代码中的决策树后续节点生成是根据根节点特征的类别进行遍历生成，当`value = 1`时，满足终止条件1，故不继续生成节点。后续没有节点也是由于满足终止条件。

接下来使用SkLearn库实现决策树并可视化。

预剪枝参数说明：
`min_samples_split`：节点在分割之前必须具有的最小样本数
`min_samples_leaf`：叶子节点必须具有的最小样本数
`max_leaf_nodes`：叶子节点的最大数量
`max_features`：在每个节点处评估用于拆分的最大特征数（除非特征非常多，否则不建议限制最大特征数）
`max_depth`：树最大的深度

生成决策树示意图：

这里以鸢尾花数据为例，使用`sklearn.tree`进行训练。

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz

# 加载数据集 鸢尾花
iris = load_iris()
x = iris.data[:,2:]
y = iris.target
tree_clf = DecisionTreeClassifier(max_depth = 2)
tree_clf.fit(x,y)
```

输出决策树的dot文件

```python
export_graphviz(
    tree_clf,
    out_file='iris_tree.dot', # 输出决策树的dot文件
    feature_names = iris.feature_names[2:], # 特征名称
    class_names = iris.target_names, # 标签名称
    rounded = True, # 是否圆角
    filled = True, # 是否填充颜色
)
```

使用终端`brew` 命令转化为png文件：

```shell
dot -Tpng iris_tree.dot -o iris_tree.png
```

结果如下

![fix](/images/imageDT-10.png)

下面使用决策边界看一下预剪枝的作用：

```python
from matplotlib.colors import ListedColormap

def plot_decision_boundary(clf,X,y,axes = [0,7.5,0,3],iris = True,legend = False,plot_training = True):
    #定义函数 clf表示训练好的分类器；X表示特征数据；y表示标签数据；axes表示默认坐标轴范围；
    x1s = np.linspace(axes[0],axes[1],100)
    x2s = np.linspace(axes[2],axes[3],100)
    x1,x2 = np.meshgrid(x1s,x2s)
    X_new = np.c_[x1.ravel(),x2.ravel()]
    y_pred = clf.predict(X_new).reshape(x1.shape)
    custom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])
    plt.contourf(x1,x2,y_pred,alpha = 0.3,cmap = custom_cmap)
    if not iris:
        custom_cmap2 = ListedColormap(['#7d7d58', '#4c4c7f', '#507d50'])
        plt.contour(x1,x2,y_pred,alpha = 0.8,cmap = custom_cmap2)
    if plot_training:
        plt.plot(X[:,0][y==0],X[:,1][y==0],'yo',label = 'Iris Setosa')
        plt.plot(X[:,0][y==1],X[:,1][y==1],'bs',label = 'Iris Versicolor')
        plt.plot(X[:,0][y==2],X[:,1][y==2],'g^',label = 'Iris Virginica')
    if iris:
        plt.xlabel('Petal length',fontsize=14)
        plt.ylabel('Petal width',fontsize=14)
    else:
        plt.xlabel('X1',fontsize=18)
        plt.ylabel('X2',fontsize=18,rotation=0)
    if legend:
        plt.legend(loc='lower right',fontsize=14)
```

先定义函数`plot_decision_boundary`
再根据已知的数据来训练模型，接着将网格中的所有点都进行预测，根据网格中所有点的预测来画决策边界。

加入几种判断：是否为鸢尾花数据？如果不是则换一种颜色绘画决策边界；如果是则改变原来的x轴y轴坐标名称为`Petal length`和`Petal width`；这里默认不在图中显示类别标签，如果需要显示则显示在右下角。

需要验证决策树的决策边界效果，则需要将X，y模型训练过程加入限制对照。

接着我们用`make_moons`来生成数据查看效果。

```python
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt
import numpy as np

X, y = make_moons(n_samples=100, noise=0.25, random_state=53)
tree_clf1 = DecisionTreeClassifier(random_state=42)
tree_clf2 = DecisionTreeClassifier(min_samples_leaf=4, random_state=42)

tree_clf1.fit(X, y)
tree_clf2.fit(X, y)

plt.figure(figsize=(12, 4))
plt.subplot(121)
plot_decision_boundary(tree_clf1, X, y, axes=[-1.5, 2.5, -1, 1.5], iris=False)
plt.title('No Restrictions')

plt.subplot(122)
plot_decision_boundary(tree_clf2, X, y, axes=[-1.5, 2.5, -1, 1.5], iris=False)
plt.title('min_samples_leaf = 4')
```

这里定义两个模型，一个没有限制，另一个加了最小样本叶为4.

结果如下。

![fix](/images/imageDT-11.png)

左边是没有增加预剪枝参数的决策边界，明显可以看出，它将一些离群点也考虑进去了，模型过为复杂，存在过拟合现象

而右边限制了 `min_samples_leaf = 4` 的决策树就没有存在明显的过拟合现象。

```python
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(6)
Xs = np.random.rand(100, 2) - 0.5 # 生成[-0.5,0.5]的100行2列的数组
ys = (Xs[:, 0] > 0).astype(np.int32)*2 # 将第一列数据大于0的变为2，否则为0 （布尔数组*2）
angle = np.pi / 4
rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)],
                            [np.sin(angle), np.cos(angle)]])
Xsr = Xs.dot(rotation_matrix) # 数组旋转 \pi/4角度
tree_clf_s = DecisionTreeClassifier(random_state=42)
tree_clf_s.fit(Xs, ys)

tree_clf_sr = DecisionTreeClassifier(random_state=42)
tree_clf_sr.fit(Xsr, ys)

plt.figure(figsize=(11, 4))
plt.subplot(121)
plot_decision_boundary(tree_clf_s,
                       Xs,
                       ys,
                       axes=[-0.7, 0.7, -0.7, 0.7],
                       iris=False)
plt.title('Sensitivity to training set rotation')
plt.subplot(122)
plot_decision_boundary(tree_clf_sr,
                       Xsr,
                       ys,
                       axes=[-0.7, 0.7, -0.7, 0.7],
                       iris=False)
plt.title('Sensitivity to training set rotation')
plt.show()
```

![fix](/images/imageDT-12.png)

先看左图，决策树很轻松的用一根垂直线将样本分成了两份，但如果我们对数据做一点小小的改动，将原本的数据进行90度旋转，如右图所示，决策边界就会复杂很多。

主要原因：决策树进行决策边界划分时只能沿着与坐标轴垂直的方向划分，所以对数据很敏感

再看回归任务：

```python
# 构建数据
np.random.seed(42)
m = 200
X = np.random.rand(m,1) # [0,1]的正态分布数据 100行1列
y = 4 * (X - 0.5)**2 
y = y + np.random.randn(m,1) / 10 #加入噪音 
# 数据可视化
plt.plot(X,y,'go') # 'go' 绿色
plt.show()
```

训练模型

```python
from sklearn.tree import DecisionTreeRegressor
# 创建回归树
tree_reg = DecisionTreeRegressor(max_depth=2)
# 训练回归树
tree_reg.fit(X, y)
```

参考上述可视化作图

```python
export_graphviz(
    tree_reg,
    out_file='RegressionTree.dot', # 输出决策树的dot文件
    feature_names=['x1'], # 特征名称
    class_names=None, # 回归树不需要class_names
    rounded=True, # 是否圆角
    filled=True, # 是否填充颜色
)

```

```shell
dot -Tpng RegressionTree.dot -o RegressionTree.png
```

![fix](/images/imageDT-13.png)
